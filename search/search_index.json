{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ResearchOS (pre-alpha version)","text":"<p>This is the documentation for ResearchOS, a Python package for scientific computing.</p>"},{"location":"#install-with-pip","title":"Install with pip","text":"<pre><code>pip install researchos\n</code></pre>"},{"location":"#project-description","title":"Project Description","text":"<p>Scientific computing is currently fractured, with many competing data standards (or lack thereof) and data processing tools that do not have a common way to communicate. ResearchOS provides a generalized framework to perform scientific computing of any kind, in a modular, easily shareable format.</p> <p>The primary innovation behind ResearchOS is to treat every single piece of the scientific data analysis workflow as an object, complete with ID and metadata. While this incurs some code overhead, the ability to have a standardized way to communicate between different parts of a pipeline and to share and integrate others' pipelines is invaluable, and sorely needed in the scientific computing community.</p>"},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"#minimum-todo-to-process-data","title":"Minimum TODO to Process Data","text":"<ul> <li>[ ] Implement Logsheet<ul> <li>[ ] Implement read logsheet.<ul> <li>[ ] Populate the database with the logsheet data.</li> </ul> </li> </ul> </li> <li>[ ] Implement saving participant data to disk/the database.<ul> <li>[ ] Implement data schema for participant data</li> </ul> </li> <li>[ ] Implement Process<ul> <li>[ ] Level</li> <li>[ ] How to structure the Process methods?</li> </ul> </li> <li>[ ] Implement Subset</li> </ul>"},{"location":"#version-01","title":"Version 0.1","text":"<ul> <li>[x] Do multiple things with one Action.</li> <li>[x] Create research objects, save and load them with attributes</li> <li>[x] Create edges between research objects and allow the edges to have their own attributes.</li> <li>[ ] Load and save even complex attributes (e.g. list of dicts) with JSON. Right now I'm just using json.loads()/dumps() but I may need something more sophisticated.</li> <li>[ ] Implement Logsheet<ul> <li>[ ] Implement read logsheet.<ul> <li>[ ] Populate the database with the logsheet data.</li> </ul> </li> </ul> </li> <li>[ ] Implement saving participant data to disk/the database.<ul> <li>[ ] Implement data schema for participant data</li> </ul> </li> <li>[ ] Implement subsets.</li> <li>[ ] Publish my proof of concept to JOSS.</li> </ul>"},{"location":"#version-02","title":"Version 0.2","text":"<ul> <li>[ ] Implement Plots</li> <li>[ ] Implement Stats</li> <li>[ ] Create a graph of research objects and edges</li> <li>[ ] Implement rollback-able version history for research objects</li> <li>[ ] Enhance multi-user support on the same machine.</li> <li>[ ] Look into CI/CD best practices, improve test coverage.</li> <li>[ ] Import/export a ResearchObject for sharing with other users.</li> <li>[ ] Export stats results to LaTeX tables.</li> <li>[ ] Export images to LaTeX figures.<ul> <li>[ ] For images with transparent backgrounds, allow them to be stacked so that multiple can be compared at once.</li> </ul> </li> </ul>"},{"location":"#version-03-and-beyond","title":"Version 0.3 and beyond","text":"<ul> <li>[ ] Implement a MariaDB-based backend for ResearchOS so that it can be used in a multi-user environment.</li> <li>[ ] Implement password-based authentication for the MariaDB backend.</li> <li>[ ] Implement a web-based frontend for ResearchOS.</li> <li>[ ] Get journals on board with ResearchOS so that they can accept ResearchObjects with submissions.</li> <li>[ ] Integrate ResearchOS with participant management systems like RedHat so that people &amp; data are linked.</li> </ul>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#example-1","title":"Example 1","text":"<p>Let's use ResearchOS to create a simple one step pipeline that reads a single number from a text file, squares it, and stores that value.</p> <p>First, after creating a new project directory and activating a virtual environment in that directory, install ResearchOS:</p> <pre><code>pip install researchos\n</code></pre> <p>Next, in the command line, run the following command:</p> <pre><code>python -m researchos quick-start\n</code></pre> <p>This will perform the following actions:</p> <ol> <li> <p>Create a new directory called 'researchos_db' in the current directory.</p> </li> <li> <p>Create a .db file in the 'researchos_db' directory with the proper schema.</p> </li> <li> <p>Create a new Project object in the .db file, and sets it to be the current Project.</p> </li> </ol> <p>Then, create a file called <code>example1.py</code> with the following contents:</p> <pre><code>from researchos.pipeline_objects.project import Project\n</code></pre> <p>This will create a new project.</p>"},{"location":"DiGraph/digraph/","title":"DiGraph","text":""},{"location":"DiGraph/digraph/#introduction","title":"Introduction","text":"<p>The NetworkX MultiDiGraph (directional graph that can have multiple parallel edges) is the data structure that organizes the relationships between all of the different research objects. Just like the objects themselves, the Research Object DiGraph models the relationships between all research objects across all projects. This is especially useful when there are objects that are common to multiple projects - they can be reused! And updates to the object in one project can propagate to other projects, if desired.</p> <p>The Research Object DiGraph consists of both Data Objects and Pipeline Objects - therefore it can become quite large and cumbersome to work with. For example if there are 10 Trial objects (DataObject) each referencing 10 Variable objects (DataObject &amp; PipelineObject), this can quickly become quite large (100 connections in this small example). Often, it is not necessary to have both DataObjects and PipelineObjects in the same graph. Therefore, Data Object DiGraphs and Pipeline Object DiGraphs can be created separately by using <code>data_objects = True</code> and <code>pipeline_objects = True</code> keyword arguments.</p> <p>Subgraphs can also be created by specifying the top level node. For example, to work with just one project's DiGraph, use the <code>source_node = {research_object_id}</code> keyword argument in the constructor, where <code>{research_object_id}</code> is the Project object's ID.</p>"},{"location":"DiGraph/digraph/#note-about-adding-objects-to-the-digraph","title":"Note About Adding Objects to the DiGraph","text":"<p>When adding objects to the DiGraph, they must exist before being added to the DiGraph! In the future the ability to create objects by adding them to the DiGraph may be added, but for now object creation and addition to the DiGraph are two entirely separate steps.</p>"},{"location":"Quick%20Start/quickstart/","title":"Quick Start","text":""},{"location":"Quick%20Start/quickstart/#to-run-quick-start-after-installing","title":"To run Quick Start after installing:","text":"<pre><code>&gt; researchos-quickstart\n</code></pre> <p>That will do the following:</p> <ol> <li> <p>Create a new directory called <code>researchos-quickstart</code>.</p> </li> <li> <p>Create a new quickstart.py file in that directory containing the basic steps for a new Project:</p> </li> </ol>"},{"location":"Quick%20Start/quickstart_py/","title":"quickstart.py","text":""},{"location":"Research%20Object%20Types/research_object/","title":"Research Objects","text":""},{"location":"Research%20Object%20Types/research_object/#overview","title":"Overview","text":"<p>Everything within the ResearchOS framework is a Research Object - at the highest level are User objects, and at the lowest are Variable objects. All Research Objects are stored in the database, and are accessible by using the methods provided in the ResearchOS API.</p> <p>All Research Objects are one or both of the following:</p> <ul> <li> <p>Data Objects - objects that are involved in storing data, such as a Subject or Trial. These objects are typically created by a Process, and are used to store data that is generated by a Process. It is perhaps helpful to recognize that these objects are similar in nature to the \"factors\" of a statistical analysis.</p> </li> <li> <p>Pipeline Objects - objects that are involved in performing data analysis, such as a Process or Project.</p> </li> </ul> <p></p> <p>We will now go over the different methods within the ResearchObject class</p> <p>One research object. Parent class of Data Objects &amp; Pipeline Objects.</p> Source code in <code>src/ResearchOS/research_object.py</code> <pre><code>class ResearchObject():\n    \"\"\"One research object. Parent class of Data Objects &amp; Pipeline Objects.\"\"\"\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __eq__(self, other):\n        if isinstance(other, ResearchObject):\n            return self.id == other.id\n        return NotImplemented\n\n    def __new__(cls, **kwargs):\n        \"\"\"Create a new research object in memory. If the object already exists in memory with this ID, return the existing object.\"\"\"\n        kwargs = ResearchObjectHandler.check_inputs(kwargs)\n        id = kwargs[\"id\"]\n        if id in ResearchObjectHandler.instances:\n            ResearchObjectHandler.counts[id] += 1\n            return ResearchObjectHandler.instances[id]\n        ResearchObjectHandler.counts[id] = 1\n        instance = super(ResearchObject, cls).__new__(cls)\n        ResearchObjectHandler.instances[id] = instance\n        return instance\n\n    def __init__(self, orig_default_attrs: dict, **orig_kwargs):\n        \"\"\"Initialize the research object.\"\"\"\n        action = None # Initialize the action.\n        self.__dict__[\"id\"] = orig_kwargs[\"id\"] # Put the ID in the __dict__ so that it is not overwritten by the __setattr__ method.\n        del orig_kwargs[\"id\"]\n        default_attrs = all_default_attrs | orig_default_attrs # Merge the default attributes, with class-specific attributes taking precedence (if any conflict)\n        if ResearchObjectHandler.object_exists(self.id):\n            # Load the existing object's attributes from the database.\n            ResearchObjectHandler._load_ro(self, default_attrs) \n            action = Action(name = f\"set object attributes\")\n            kwargs = orig_kwargs\n        else:\n            # Create a new object.\n            action = Action(name = f\"created object\")\n            ResearchObjectHandler._create_ro(self, action = action) # Create the object in the database.\n            # Add the default attributes to the kwargs to be set, only if they're not being overwritten by a kwarg.\n            kwargs = default_attrs | orig_kwargs\n        for key in kwargs:\n            validate = True # Default is to validate any attribute.\n            # If the attribute value is a default value, don't validate it.\n            if key in default_attrs and kwargs[key] == default_attrs[key]:\n                validate = False\n            self.__setattr__(key, kwargs[key], action = action, validate = validate)\n        action.execute(commit = True)    \n</code></pre>"},{"location":"Research%20Object%20Types/research_object/#src.ResearchOS.research_object.ResearchObject.__init__","title":"<code>__init__(orig_default_attrs, **orig_kwargs)</code>","text":"<p>Initialize the research object.</p> Source code in <code>src/ResearchOS/research_object.py</code> <pre><code>def __init__(self, orig_default_attrs: dict, **orig_kwargs):\n    \"\"\"Initialize the research object.\"\"\"\n    action = None # Initialize the action.\n    self.__dict__[\"id\"] = orig_kwargs[\"id\"] # Put the ID in the __dict__ so that it is not overwritten by the __setattr__ method.\n    del orig_kwargs[\"id\"]\n    default_attrs = all_default_attrs | orig_default_attrs # Merge the default attributes, with class-specific attributes taking precedence (if any conflict)\n    if ResearchObjectHandler.object_exists(self.id):\n        # Load the existing object's attributes from the database.\n        ResearchObjectHandler._load_ro(self, default_attrs) \n        action = Action(name = f\"set object attributes\")\n        kwargs = orig_kwargs\n    else:\n        # Create a new object.\n        action = Action(name = f\"created object\")\n        ResearchObjectHandler._create_ro(self, action = action) # Create the object in the database.\n        # Add the default attributes to the kwargs to be set, only if they're not being overwritten by a kwarg.\n        kwargs = default_attrs | orig_kwargs\n    for key in kwargs:\n        validate = True # Default is to validate any attribute.\n        # If the attribute value is a default value, don't validate it.\n        if key in default_attrs and kwargs[key] == default_attrs[key]:\n            validate = False\n        self.__setattr__(key, kwargs[key], action = action, validate = validate)\n    action.execute(commit = True)    \n</code></pre>"},{"location":"Research%20Object%20Types/research_object/#src.ResearchOS.research_object.ResearchObject.__new__","title":"<code>__new__(**kwargs)</code>","text":"<p>Create a new research object in memory. If the object already exists in memory with this ID, return the existing object.</p> Source code in <code>src/ResearchOS/research_object.py</code> <pre><code>def __new__(cls, **kwargs):\n    \"\"\"Create a new research object in memory. If the object already exists in memory with this ID, return the existing object.\"\"\"\n    kwargs = ResearchObjectHandler.check_inputs(kwargs)\n    id = kwargs[\"id\"]\n    if id in ResearchObjectHandler.instances:\n        ResearchObjectHandler.counts[id] += 1\n        return ResearchObjectHandler.instances[id]\n    ResearchObjectHandler.counts[id] = 1\n    instance = super(ResearchObject, cls).__new__(cls)\n    ResearchObjectHandler.instances[id] = instance\n    return instance\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/","title":"Data Objects","text":"<p>             Bases: <code>ResearchObject</code></p> <p>The abstract base class for all data objects. Data objects are the ones not in the digraph, and represent some form of data storage.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>class DataObject(ResearchObject):\n    \"\"\"The abstract base class for all data objects. Data objects are the ones not in the digraph, and represent some form of data storage.\"\"\"    \n\n    def __init__(self, default_attrs: dict, **kwargs) -&gt; None:\n        \"\"\"Initialize the data object.\"\"\"\n        all_default_attrs_all = all_default_attrs | default_attrs # Class-specific default attributes take precedence.\n        super().__init__(all_default_attrs_all, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the value of an attribute in the DataObject and the database.\"\"\"\n        # TODO: HOW CAN I HANDLE VR VALUES BEING ADDED, MODIFIED, AND DELETED?\n        # In the format of self.vr = {vr_id: value}\n\n        # Addition: There are new fields.\n        # Deletion: There are fields that are no longer present.\n        # Modification: The fields have changed.\n        # NOTE: All/any combination of these three operations could happen at once.\n\n        if action is None:\n            action = Action(name = \"vr changed\")\n\n        conn = DBConnectionFactory.create_db_connection().conn\n        if validate and not all([IDCreator(conn).is_ro_id(vr_id) for vr_id in value]):\n            raise ValueError(\"The keys of the dictionary must be valid VR ID's.\")\n\n        # 1. Get the ID's of all the VR's being added.\n        new_vr_ids = []\n        for vr_id in value:\n            if vr_id not in self.vr:\n                new_vr_ids.append(vr_id)\n\n        # 2. Get the ID's of all the VR's being modified.\n        modified_vr_ids = []\n        for vr_id in value:\n            if vr_id in self.vr and self.vr[vr_id] != value[vr_id]:\n                modified_vr_ids.append(vr_id)\n\n        sqlquery = f\"SELECT action_id, schema_id FROM data_addresses WHERE address_id = '{self.id}'\"\n        cursor = conn.cursor()\n        result = cursor.execute(sqlquery).fetchall()\n        ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num=0)\n        latest_result = ResearchObjectHandler._get_most_recent_attrs(self, ordered_result)\n        if \"schema_id\" not in latest_result:\n            # raise ValueError(\"This object does not have an address_id in the database.\")\n            return\n\n        schema_id = latest_result[\"schema_id\"]\n\n        new_row_vr_ids = new_vr_ids + modified_vr_ids\n        for vr_id in new_row_vr_ids:\n            self.add_vr_row(vr_id, value[vr_id], schema_id, action)\n\n        action.execute()\n\n        # 3. Get the ID's of all the VR's being deleted.\n        deleted_vr_ids = []\n        for vr_id in self.vr:\n            if vr_id not in value:\n                deleted_vr_ids.append(vr_id)        \n\n    def add_vr_row(self, vr_id: str, value: Any, schema_id: str, action: Action) -&gt; None:\n        \"\"\"Add a VR to the DataObject. Also serves to modify existing objects.\"\"\"\n        json_value = json.dumps(None)\n        is_scalar = False\n        if isinstance(value, (int, float, bool, str, bytes)): # If is scalar.\n            json_value = json.dumps(value)\n            is_scalar = True\n        sqlquery = f\"INSERT INTO data_values (action_id, address_id, schema_id, vr_id, scalar_value) VALUES ({action.id}, {self.id}, {schema_id}, {vr_id}, {json_value})\"\n        action.add_sql_query(sqlquery)\n\n        dataset_id = self.get_dataset_id(schema_id)\n\n        # Save the data to the file system.\n        # Get the list of levels for this address and dataset schema.\n        if not is_scalar:\n            self.save_data_values(value, vr_id, dataset_id, schema_id)\n\n    def delete_vr(self, vr_id: str) -&gt; None:\n        \"\"\"Delete a VR from the DataObject.\"\"\"\n        pass\n\n    def save_data_values(self, value: Any, vr_id: str, dataset_id: str, schema_id: str) -&gt; None:\n        \"\"\"Save data values to the file system.\"\"\"\n        levels = self.get_levels(schema_id)\n        path = self.get_vr_file_path(vr_id, dataset_id, levels)\n        with open(path, \"w\") as f:\n            json.dump(value, f)\n\n    def load(self) -&gt; None:\n        \"\"\"Load data values from the database.\"\"\"        \n        self.load_data_values()     \n\n    def load_data_values(self) -&gt; None:\n        \"\"\"Load data values from the database OR file system.\"\"\"\n        # 1. Identify which rows in the database are associated with this data object and have not been overwritten.\n        sqlquery = f\"SELECT address_id, schema_id, action_id, vr_id, scalar_value FROM data_values WHERE address_id = '{self.id}'\"\n        conn = DBConnectionFactory.create_db_connection().conn\n        cursor = conn.cursor()\n        result = cursor.execute(sqlquery).fetchall()\n        ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num=1)\n        latest_result = ResearchObjectHandler._get_most_recent_attrs(self, ordered_result)\n\n        if not latest_result:\n            # raise ValueError(\"This object does not have an address_id in the database.\")\n            return\n\n        schema_id = latest_result[\"schema_id\"]\n\n        dataset_id = self.get_dataset_id(schema_id)\n\n        # Get the list of levels for this address and dataset schema.\n        levels = self.get_levels(schema_id)\n\n        # \"latest_result\" is a list containing each of the attrs. There should be no repetition of attr names between elements of the list.\n        vrs = {}\n        for row in latest_result:\n            vr_id = row[3]\n            scalar_value = json.loads(row[4])\n            if scalar_value is not None:\n                vrs[vr_id] = scalar_value\n                continue\n\n            # TODO: correct this, this is a placeholder.\n            path = self.get_vr_file_path(vr_id, dataset_id, levels)\n            with open(path, \"r\") as f:\n                # Is it json? Or a binary format?\n                vrs[vr_id] = json.load(f)\n\n        self.__dict__[\"vr\"] = vrs\n\n    def get_levels(self, schema_id: str, object_id: str = None) -&gt; list:\n        \"\"\"Get the levels of the data object.\"\"\"\n        if object_id is not None:\n            sqlquery = f\"SELECT level0_id, level1_id, level2_id, level3_id, level4_id, level5_id, level6_id, level7_id, level8_id, level9_id FROM data_addresses WHERE address_id = '{object_id}' AND schema_id = '{schema_id}' LIMIT 1\"\n        else:\n            sqlquery = f\"SELECT level0_id, level1_id, level2_id, level3_id, level4_id, level5_id, level6_id, level7_id, level8_id, level9_id FROM data_addresses WHERE schema_id = '{schema_id}'\"\n        conn = DBConnectionFactory.create_db_connection().conn\n        cursor = conn.cursor()\n        levels = cursor.execute(sqlquery).fetchall()\n        if len(levels) &lt; 1:\n            # raise ValueError(\"The address does not exist in the database.\")\n            return []\n        # if len(levels) &gt; 1:\n        #     raise ValueError(\"There are multiple addresses with the same ID in the database.\")\n        # Remove the None components from each row of the result.\n        trimmed_levels = []\n        for row in levels:\n            trimmed_levels.append([x for x in row if x is not None])\n        levels = ResearchObjectHandler.list_to_dict(trimmed_levels)\n        levels_dict = {}\n        for row_num in range(len(levels)):\n            for col_num in range(10):\n                object_id = levels[row_num][col_num]\n                if object_id is not None and object_id not in levels_dict:\n                    levels_dict[object_id] = {}\n        return levels_dict\n\n\n    def get_vr_file_path(self, vr_id: str, dataset_id: str, levels: list) -&gt; str:\n        \"\"\"Get the file path for a VR.\"\"\"\n        subfolder = \"\"\n        for level in levels:\n            if level is not None:\n                subfolder += level + os.sep\n        return root_data_path + os.sep + dataset_id + subfolder + vr_id + \".json\"\n\n    def get_dataset_id(self, schema_id: str) -&gt; str:\n        \"\"\"Get the current dataset ID.\"\"\"\n        sqlquery = f\"SELECT dataset_id FROM dataset_schemas WHERE schema_id = {schema_id} LIMIT 1\"\n        conn = DBConnectionFactory.create_db_connection().conn\n        cursor = conn.cursor()\n        dataset_id = cursor.execute(sqlquery).fetchall()\n        if len(dataset_id) &lt; 1:\n            raise ValueError(\"The schema does not exist in the database.\")\n        if len(dataset_id) &gt; 1:\n            raise ValueError(\"There are multiple schemas with the same ID in the database.\")\n        return dataset_id[0][0]\n\n    def get_address(self, attr_name: str) -&gt; list:\n        \"\"\"Get the address for a specific attribute.\"\"\"\n        conn = DBConnectionFactory.create_db_connection().conn\n        schema_id = self.get_schema_id()\n        sqlquery = f\"SELECT {attr_name} FROM data_addresses WHERE schema_id = '{schema_id}'\"\n        result = conn.execute(sqlquery).fetchone()\n        return result\n\n    def get_current_schema_id(self, dataset_id: str) -&gt; str:\n        conn = DBConnectionFactory.create_db_connection().conn\n        sqlquery = f\"SELECT action_id FROM data_address_schemas WHERE dataset_id = '{dataset_id}'\"\n        action_ids = conn.cursor().execute(sqlquery).fetchall()\n        action_ids = ResearchObjectHandler._get_time_ordered_result(action_ids, action_col_num=0)\n        action_id_schema = action_ids[0][0] if action_ids else None\n        if action_id_schema is None and not self.schema:\n            return # If the schema is empty and the addresses are empty, this is likely initialization so just return.\n\n        sqlquery = f\"SELECT schema_id FROM data_address_schemas WHERE dataset_id = '{dataset_id}' AND action_id = '{action_id_schema}'\"\n        schema_id = conn.execute(sqlquery).fetchone()\n        schema_id = schema_id[0] if schema_id else None\n        return schema_id\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.__init__","title":"<code>__init__(default_attrs, **kwargs)</code>","text":"<p>Initialize the data object.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def __init__(self, default_attrs: dict, **kwargs) -&gt; None:\n    \"\"\"Initialize the data object.\"\"\"\n    all_default_attrs_all = all_default_attrs | default_attrs # Class-specific default attributes take precedence.\n    super().__init__(all_default_attrs_all, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the value of an attribute in the DataObject and the database.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the value of an attribute in the DataObject and the database.\"\"\"\n    # TODO: HOW CAN I HANDLE VR VALUES BEING ADDED, MODIFIED, AND DELETED?\n    # In the format of self.vr = {vr_id: value}\n\n    # Addition: There are new fields.\n    # Deletion: There are fields that are no longer present.\n    # Modification: The fields have changed.\n    # NOTE: All/any combination of these three operations could happen at once.\n\n    if action is None:\n        action = Action(name = \"vr changed\")\n\n    conn = DBConnectionFactory.create_db_connection().conn\n    if validate and not all([IDCreator(conn).is_ro_id(vr_id) for vr_id in value]):\n        raise ValueError(\"The keys of the dictionary must be valid VR ID's.\")\n\n    # 1. Get the ID's of all the VR's being added.\n    new_vr_ids = []\n    for vr_id in value:\n        if vr_id not in self.vr:\n            new_vr_ids.append(vr_id)\n\n    # 2. Get the ID's of all the VR's being modified.\n    modified_vr_ids = []\n    for vr_id in value:\n        if vr_id in self.vr and self.vr[vr_id] != value[vr_id]:\n            modified_vr_ids.append(vr_id)\n\n    sqlquery = f\"SELECT action_id, schema_id FROM data_addresses WHERE address_id = '{self.id}'\"\n    cursor = conn.cursor()\n    result = cursor.execute(sqlquery).fetchall()\n    ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num=0)\n    latest_result = ResearchObjectHandler._get_most_recent_attrs(self, ordered_result)\n    if \"schema_id\" not in latest_result:\n        # raise ValueError(\"This object does not have an address_id in the database.\")\n        return\n\n    schema_id = latest_result[\"schema_id\"]\n\n    new_row_vr_ids = new_vr_ids + modified_vr_ids\n    for vr_id in new_row_vr_ids:\n        self.add_vr_row(vr_id, value[vr_id], schema_id, action)\n\n    action.execute()\n\n    # 3. Get the ID's of all the VR's being deleted.\n    deleted_vr_ids = []\n    for vr_id in self.vr:\n        if vr_id not in value:\n            deleted_vr_ids.append(vr_id)        \n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.add_vr_row","title":"<code>add_vr_row(vr_id, value, schema_id, action)</code>","text":"<p>Add a VR to the DataObject. Also serves to modify existing objects.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def add_vr_row(self, vr_id: str, value: Any, schema_id: str, action: Action) -&gt; None:\n    \"\"\"Add a VR to the DataObject. Also serves to modify existing objects.\"\"\"\n    json_value = json.dumps(None)\n    is_scalar = False\n    if isinstance(value, (int, float, bool, str, bytes)): # If is scalar.\n        json_value = json.dumps(value)\n        is_scalar = True\n    sqlquery = f\"INSERT INTO data_values (action_id, address_id, schema_id, vr_id, scalar_value) VALUES ({action.id}, {self.id}, {schema_id}, {vr_id}, {json_value})\"\n    action.add_sql_query(sqlquery)\n\n    dataset_id = self.get_dataset_id(schema_id)\n\n    # Save the data to the file system.\n    # Get the list of levels for this address and dataset schema.\n    if not is_scalar:\n        self.save_data_values(value, vr_id, dataset_id, schema_id)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.delete_vr","title":"<code>delete_vr(vr_id)</code>","text":"<p>Delete a VR from the DataObject.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def delete_vr(self, vr_id: str) -&gt; None:\n    \"\"\"Delete a VR from the DataObject.\"\"\"\n    pass\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.get_address","title":"<code>get_address(attr_name)</code>","text":"<p>Get the address for a specific attribute.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def get_address(self, attr_name: str) -&gt; list:\n    \"\"\"Get the address for a specific attribute.\"\"\"\n    conn = DBConnectionFactory.create_db_connection().conn\n    schema_id = self.get_schema_id()\n    sqlquery = f\"SELECT {attr_name} FROM data_addresses WHERE schema_id = '{schema_id}'\"\n    result = conn.execute(sqlquery).fetchone()\n    return result\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.get_dataset_id","title":"<code>get_dataset_id(schema_id)</code>","text":"<p>Get the current dataset ID.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def get_dataset_id(self, schema_id: str) -&gt; str:\n    \"\"\"Get the current dataset ID.\"\"\"\n    sqlquery = f\"SELECT dataset_id FROM dataset_schemas WHERE schema_id = {schema_id} LIMIT 1\"\n    conn = DBConnectionFactory.create_db_connection().conn\n    cursor = conn.cursor()\n    dataset_id = cursor.execute(sqlquery).fetchall()\n    if len(dataset_id) &lt; 1:\n        raise ValueError(\"The schema does not exist in the database.\")\n    if len(dataset_id) &gt; 1:\n        raise ValueError(\"There are multiple schemas with the same ID in the database.\")\n    return dataset_id[0][0]\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.get_levels","title":"<code>get_levels(schema_id, object_id=None)</code>","text":"<p>Get the levels of the data object.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def get_levels(self, schema_id: str, object_id: str = None) -&gt; list:\n    \"\"\"Get the levels of the data object.\"\"\"\n    if object_id is not None:\n        sqlquery = f\"SELECT level0_id, level1_id, level2_id, level3_id, level4_id, level5_id, level6_id, level7_id, level8_id, level9_id FROM data_addresses WHERE address_id = '{object_id}' AND schema_id = '{schema_id}' LIMIT 1\"\n    else:\n        sqlquery = f\"SELECT level0_id, level1_id, level2_id, level3_id, level4_id, level5_id, level6_id, level7_id, level8_id, level9_id FROM data_addresses WHERE schema_id = '{schema_id}'\"\n    conn = DBConnectionFactory.create_db_connection().conn\n    cursor = conn.cursor()\n    levels = cursor.execute(sqlquery).fetchall()\n    if len(levels) &lt; 1:\n        # raise ValueError(\"The address does not exist in the database.\")\n        return []\n    # if len(levels) &gt; 1:\n    #     raise ValueError(\"There are multiple addresses with the same ID in the database.\")\n    # Remove the None components from each row of the result.\n    trimmed_levels = []\n    for row in levels:\n        trimmed_levels.append([x for x in row if x is not None])\n    levels = ResearchObjectHandler.list_to_dict(trimmed_levels)\n    levels_dict = {}\n    for row_num in range(len(levels)):\n        for col_num in range(10):\n            object_id = levels[row_num][col_num]\n            if object_id is not None and object_id not in levels_dict:\n                levels_dict[object_id] = {}\n    return levels_dict\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.get_vr_file_path","title":"<code>get_vr_file_path(vr_id, dataset_id, levels)</code>","text":"<p>Get the file path for a VR.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def get_vr_file_path(self, vr_id: str, dataset_id: str, levels: list) -&gt; str:\n    \"\"\"Get the file path for a VR.\"\"\"\n    subfolder = \"\"\n    for level in levels:\n        if level is not None:\n            subfolder += level + os.sep\n    return root_data_path + os.sep + dataset_id + subfolder + vr_id + \".json\"\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.load","title":"<code>load()</code>","text":"<p>Load data values from the database.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load data values from the database.\"\"\"        \n    self.load_data_values()     \n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.load_data_values","title":"<code>load_data_values()</code>","text":"<p>Load data values from the database OR file system.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def load_data_values(self) -&gt; None:\n    \"\"\"Load data values from the database OR file system.\"\"\"\n    # 1. Identify which rows in the database are associated with this data object and have not been overwritten.\n    sqlquery = f\"SELECT address_id, schema_id, action_id, vr_id, scalar_value FROM data_values WHERE address_id = '{self.id}'\"\n    conn = DBConnectionFactory.create_db_connection().conn\n    cursor = conn.cursor()\n    result = cursor.execute(sqlquery).fetchall()\n    ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num=1)\n    latest_result = ResearchObjectHandler._get_most_recent_attrs(self, ordered_result)\n\n    if not latest_result:\n        # raise ValueError(\"This object does not have an address_id in the database.\")\n        return\n\n    schema_id = latest_result[\"schema_id\"]\n\n    dataset_id = self.get_dataset_id(schema_id)\n\n    # Get the list of levels for this address and dataset schema.\n    levels = self.get_levels(schema_id)\n\n    # \"latest_result\" is a list containing each of the attrs. There should be no repetition of attr names between elements of the list.\n    vrs = {}\n    for row in latest_result:\n        vr_id = row[3]\n        scalar_value = json.loads(row[4])\n        if scalar_value is not None:\n            vrs[vr_id] = scalar_value\n            continue\n\n        # TODO: correct this, this is a placeholder.\n        path = self.get_vr_file_path(vr_id, dataset_id, levels)\n        with open(path, \"r\") as f:\n            # Is it json? Or a binary format?\n            vrs[vr_id] = json.load(f)\n\n    self.__dict__[\"vr\"] = vrs\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.save_data_values","title":"<code>save_data_values(value, vr_id, dataset_id, schema_id)</code>","text":"<p>Save data values to the file system.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>def save_data_values(self, value: Any, vr_id: str, dataset_id: str, schema_id: str) -&gt; None:\n    \"\"\"Save data values to the file system.\"\"\"\n    levels = self.get_levels(schema_id)\n    path = self.get_vr_file_path(vr_id, dataset_id, levels)\n    with open(path, \"w\") as f:\n        json.dump(value, f)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/","title":"Dataset","text":"<p>Inherits from DataObject</p> <p>             Bases: <code>DataObject</code></p> <p>A dataset is one set of data. Class-specific Attributes: 1. data path: The root folder location of the dataset. 2. data schema: The schema of the dataset (specified as a list of classes)</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>class Dataset(DataObject):\n    \"\"\"A dataset is one set of data.\n    Class-specific Attributes:\n    1. data path: The root folder location of the dataset.\n    2. data schema: The schema of the dataset (specified as a list of classes)\"\"\"\n\n    prefix: str = \"DS\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"        \n        is_new = False\n        if not ResearchObjectHandler.object_exists(kwargs.get(\"id\")):\n            is_new = True\n        super().__init__(all_default_attrs, **kwargs)   \n        if is_new:     \n            schema = {\n                Dataset: {}\n            }\n            self.schema = schema\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        self.load_schema() # Load the dataset schema.\n        self.load_addresses() # Load the dataset addresses.\n        DataObject.load(self) # Load the attributes specific to it being a DataObject.\n\n    ### Schema Methods\n\n    def validate_schema(self, schema: dict[dict]) -&gt; None:\n        \"\"\"Validate that the data schema follows the proper format.\n        Must be a dict of dicts, where all keys are Python types matching a DataObject subclass, and the lowest levels are empty.\"\"\"\n        subclasses = DataObject.__subclasses__()\n        # Check if the object is a dictionary\n        if not isinstance(schema, dict):\n            raise ValueError(\"The schema must be a dictionary!\")\n\n        for key, value in schema.items():\n            # Check if the key is an instance of a DataObject subclass Python type\n            if not key in subclasses:\n                raise ValueError(\"The key must be an instance of a DataObject subclass Python type!\")\n\n            self.validate_schema(value)\n\n    def save_schema(self, schema: list, action: Action) -&gt; None:\n        \"\"\"Save the schema to the database.\"\"\"\n        # 1. Convert the list of types to a list of str.\n        str_schema = class_to_prefix(schema)\n\n        # 2. Convert the list of str to a json string.\n        json_schema = json.dumps(str_schema)\n\n        # 3. Save the schema to the database.\n        conn = DBConnectionFactory.create_db_connection().conn\n        schema_id = IDCreator(conn).create_action_id()\n        sqlquery = f\"INSERT INTO data_address_schemas (schema_id, levels_edge_list, dataset_id, action_id) VALUES ('{schema_id}', '{json_schema}', '{self.id}', '{action.id}')\"\n        action.add_sql_query(sqlquery)\n\n    def load_schema(self) -&gt; None:\n        \"\"\"Load the schema from the database and convert it via json.\"\"\"\n        prefix_schema = all_default_attrs[\"schema\"] # Initialize the schema\n        # 1. Get the dataset ID\n        id = self.id\n        # 2. Get the most recent action ID for the dataset in the data_address_schemas table.\n        sqlquery = f\"SELECT action_id FROM data_address_schemas WHERE dataset_id = '{id}'\"\n        conn = DBConnectionFactory.create_db_connection().conn\n        result = conn.execute(sqlquery).fetchall()\n        # result = [_[0] for _ in result]\n        ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num = 0)\n        if len(ordered_result) &gt; 0:\n            most_recent_action_id = ordered_result[0][0]\n            # 3. Get the schema from the levels_ordered column in the data_address_schemas table.\n            sqlquery = f\"SELECT levels_edge_list FROM data_address_schemas WHERE action_id = '{most_recent_action_id}'\"\n            result = conn.execute(sqlquery).fetchall()\n            prefix_schema = json.loads(result[0][0])\n\n        # 5. If the schema is not None, convert the string to a list of types.\n        schema = prefix_to_class(prefix_schema)    \n\n        # 6. Store the schema as an attribute of the dataset.\n        self.__dict__[\"schema\"] = schema\n\n    ### Dataset path methods\n\n    def validate_dataset_path(self, path: str) -&gt; None:\n        \"\"\"Validate the dataset path.\"\"\"\n        import os\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path is not a path or does not currently exist!\")\n\n    ### Address Methods\n\n    def validate_addresses(self, addresses: dict, count: int = 0) -&gt; None:\n        \"\"\"Validate that the addresses are in the correct format.\"\"\"\n        conn = DBConnectionFactory.create_db_connection().conn\n        if count==0: # Only do this once.\n            self.validate_schema(self.schema) # Ensure that the schema is valid before doing the addresses.\n        if count &gt; 9:\n            raise ValueError(\"Address dict must be of depth 10 or less!\")\n\n        if not isinstance(addresses, dict):\n            raise ValueError(\"Addresses must be provided as a dict!\")\n\n        for key, value in addresses.items():\n            if not isinstance(key, str):\n                raise ValueError(\"addresses must be str!\")\n            if not IDCreator(conn).is_ro_id(key):\n                raise ValueError(\"Addresses must have the proper prefixes to be considered valid!\")\n            if not ResearchObjectHandler.object_exists(key):\n                raise ValueError(\"The object at this address does not exist in the database!\")\n            self.validate_addresses(value, count + 1)\n\n    def save_addresses(self, addresses: dict, action: Action) -&gt; None:\n        \"\"\"Save the addresses to the data_addresses table in the database.\"\"\"        \n        # 1. Get the schema_id for the current dataset_id that has not been overwritten by an Action.        \n        dataset_id = self.id\n        schema_id = self.get_current_schema_id(dataset_id)\n\n        # 2. Put the addresses into the data_addresses table.\n        def save_addresses_recurse(address_dict: dict, schema_id: str, action: Action, level_num: int = 0, prev_addresses_list: list = []) -&gt; None:\n            \"\"\"Recursively save the addresses to the database.\"\"\"            \n            for address in address_dict:\n                # 1. Save the current address.\n                full_address_list = copy.deepcopy(prev_addresses_list)\n                full_address_list.append(address)\n                level_names = \"\"\n                for idx, level in enumerate(full_address_list):\n                    level_names += f\"level{idx}_id, \"\n                level_names = level_names[:-2] # Remove final comma and space.\n                sqlquery = f\"INSERT INTO data_addresses (schema_id, action_id, address_id, {level_names}) VALUES ('{schema_id}', '{action.id}', '{address}'\"\n                for level in full_address_list:\n                    sqlquery += f\", '{level}'\"\n                sqlquery += \")\"\n                action.add_sql_query(sqlquery)\n                # 2. Recurse.\n                save_addresses_recurse(address_dict[address], schema_id, action, level_num + 1, full_address_list)\n\n        save_addresses_recurse(addresses, schema_id, action)\n\n            # level_names = \"\"\n            # for level in address:\n            #     level_names += f\"level{address.index(level)}_id, \"\n            # level_names = level_names[:-2] # Remove final comma and space.\n\n            # for level in address:\n            #     sqlquery += f\", '{level}'\"\n            # sqlquery += \")\"\n        #     for address_id, value in addresses.items():\n        #         sqlquery = f\"INSERT INTO data_addresses (schema_id, action_id, address_id, level{level_num}_id) VALUES ('{schema_id}', '{action.id}', '{address_id}', '{address_id}')\"\n        #         action.add_sql_query(sqlquery)\n        #         save_addresses_recurse(value, schema_id, action)\n\n        # for address in addresses:\n        #     # address_id = IDCreator(conn).create_action_id()\n        #     level_names = \"\"\n        #     for level in address:\n        #         level_names += f\"level{address.index(level)}_id, \"\n        #     level_names = level_names[:-2] # Remove final comma and space.\n        #     sqlquery = f\"INSERT INTO data_addresses (schema_id, action_id, address_id, {level_names}) VALUES ('{schema_id}', '{action.id}', '{address_id}'\"\n        #     for level in address:\n        #         sqlquery += f\", '{level}'\"\n        #     sqlquery += \")\"\n        #     action.add_sql_query(sqlquery)\n\n    def load_addresses(self) -&gt; list[list]:\n        \"\"\"Load the addresses from the database.\"\"\"\n        # conn = DBConnectionFactory.create_db_connection().conn\n\n        schema_id = self.get_current_schema_id(self.id)\n\n        # 2. Get the addresses for the current schema_id.\n        # sqlquery = f\"SELECT action_id FROM data_addresses WHERE schema_id = '{schema_id}'\"\n        # action_ids = conn.execute(sqlquery).fetchall()\n        # action_ids = ResearchObjectHandler._get_time_ordered_result(action_ids, action_col_num=0)\n        # action_id = action_ids[0][0] if action_ids else None\n\n        levels = self.get_levels(schema_id)\n        self.__dict__[\"addresses\"] = levels\n\n    def load_address_objects(self, address_objects: dict = {}, address_dict: dict = {}) -&gt; list:\n        \"\"\"Using the self.addresses property, load the addressed objects from the database.\"\"\"\n        dataobject_subclasses = DataObject.__subclasses__()\n        if not address_dict:\n            address_dict = self.addresses\n        for address in address_dict:\n            for cls in dataobject_subclasses:\n                if not address.startswith(cls.prefix):\n                    continue                \n                break\n            cls_object = cls(id = address)\n            if cls_object in address_objects:\n                continue\n            address_objects[cls_object] = {}\n            self.load_address_objects(address_objects = address_objects[cls_object], address_dict = address_dict[address])\n\n        return address_objects\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"        \n    is_new = False\n    if not ResearchObjectHandler.object_exists(kwargs.get(\"id\")):\n        is_new = True\n    super().__init__(all_default_attrs, **kwargs)   \n    if is_new:     \n        schema = {\n            Dataset: {}\n        }\n        self.schema = schema\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    self.load_schema() # Load the dataset schema.\n    self.load_addresses() # Load the dataset addresses.\n    DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.load_address_objects","title":"<code>load_address_objects(address_objects={}, address_dict={})</code>","text":"<p>Using the self.addresses property, load the addressed objects from the database.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def load_address_objects(self, address_objects: dict = {}, address_dict: dict = {}) -&gt; list:\n    \"\"\"Using the self.addresses property, load the addressed objects from the database.\"\"\"\n    dataobject_subclasses = DataObject.__subclasses__()\n    if not address_dict:\n        address_dict = self.addresses\n    for address in address_dict:\n        for cls in dataobject_subclasses:\n            if not address.startswith(cls.prefix):\n                continue                \n            break\n        cls_object = cls(id = address)\n        if cls_object in address_objects:\n            continue\n        address_objects[cls_object] = {}\n        self.load_address_objects(address_objects = address_objects[cls_object], address_dict = address_dict[address])\n\n    return address_objects\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.load_addresses","title":"<code>load_addresses()</code>","text":"<p>Load the addresses from the database.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def load_addresses(self) -&gt; list[list]:\n    \"\"\"Load the addresses from the database.\"\"\"\n    # conn = DBConnectionFactory.create_db_connection().conn\n\n    schema_id = self.get_current_schema_id(self.id)\n\n    # 2. Get the addresses for the current schema_id.\n    # sqlquery = f\"SELECT action_id FROM data_addresses WHERE schema_id = '{schema_id}'\"\n    # action_ids = conn.execute(sqlquery).fetchall()\n    # action_ids = ResearchObjectHandler._get_time_ordered_result(action_ids, action_col_num=0)\n    # action_id = action_ids[0][0] if action_ids else None\n\n    levels = self.get_levels(schema_id)\n    self.__dict__[\"addresses\"] = levels\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.load_schema","title":"<code>load_schema()</code>","text":"<p>Load the schema from the database and convert it via json.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def load_schema(self) -&gt; None:\n    \"\"\"Load the schema from the database and convert it via json.\"\"\"\n    prefix_schema = all_default_attrs[\"schema\"] # Initialize the schema\n    # 1. Get the dataset ID\n    id = self.id\n    # 2. Get the most recent action ID for the dataset in the data_address_schemas table.\n    sqlquery = f\"SELECT action_id FROM data_address_schemas WHERE dataset_id = '{id}'\"\n    conn = DBConnectionFactory.create_db_connection().conn\n    result = conn.execute(sqlquery).fetchall()\n    # result = [_[0] for _ in result]\n    ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num = 0)\n    if len(ordered_result) &gt; 0:\n        most_recent_action_id = ordered_result[0][0]\n        # 3. Get the schema from the levels_ordered column in the data_address_schemas table.\n        sqlquery = f\"SELECT levels_edge_list FROM data_address_schemas WHERE action_id = '{most_recent_action_id}'\"\n        result = conn.execute(sqlquery).fetchall()\n        prefix_schema = json.loads(result[0][0])\n\n    # 5. If the schema is not None, convert the string to a list of types.\n    schema = prefix_to_class(prefix_schema)    \n\n    # 6. Store the schema as an attribute of the dataset.\n    self.__dict__[\"schema\"] = schema\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.save_addresses","title":"<code>save_addresses(addresses, action)</code>","text":"<p>Save the addresses to the data_addresses table in the database.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def save_addresses(self, addresses: dict, action: Action) -&gt; None:\n    \"\"\"Save the addresses to the data_addresses table in the database.\"\"\"        \n    # 1. Get the schema_id for the current dataset_id that has not been overwritten by an Action.        \n    dataset_id = self.id\n    schema_id = self.get_current_schema_id(dataset_id)\n\n    # 2. Put the addresses into the data_addresses table.\n    def save_addresses_recurse(address_dict: dict, schema_id: str, action: Action, level_num: int = 0, prev_addresses_list: list = []) -&gt; None:\n        \"\"\"Recursively save the addresses to the database.\"\"\"            \n        for address in address_dict:\n            # 1. Save the current address.\n            full_address_list = copy.deepcopy(prev_addresses_list)\n            full_address_list.append(address)\n            level_names = \"\"\n            for idx, level in enumerate(full_address_list):\n                level_names += f\"level{idx}_id, \"\n            level_names = level_names[:-2] # Remove final comma and space.\n            sqlquery = f\"INSERT INTO data_addresses (schema_id, action_id, address_id, {level_names}) VALUES ('{schema_id}', '{action.id}', '{address}'\"\n            for level in full_address_list:\n                sqlquery += f\", '{level}'\"\n            sqlquery += \")\"\n            action.add_sql_query(sqlquery)\n            # 2. Recurse.\n            save_addresses_recurse(address_dict[address], schema_id, action, level_num + 1, full_address_list)\n\n    save_addresses_recurse(addresses, schema_id, action)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.save_schema","title":"<code>save_schema(schema, action)</code>","text":"<p>Save the schema to the database.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def save_schema(self, schema: list, action: Action) -&gt; None:\n    \"\"\"Save the schema to the database.\"\"\"\n    # 1. Convert the list of types to a list of str.\n    str_schema = class_to_prefix(schema)\n\n    # 2. Convert the list of str to a json string.\n    json_schema = json.dumps(str_schema)\n\n    # 3. Save the schema to the database.\n    conn = DBConnectionFactory.create_db_connection().conn\n    schema_id = IDCreator(conn).create_action_id()\n    sqlquery = f\"INSERT INTO data_address_schemas (schema_id, levels_edge_list, dataset_id, action_id) VALUES ('{schema_id}', '{json_schema}', '{self.id}', '{action.id}')\"\n    action.add_sql_query(sqlquery)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.validate_addresses","title":"<code>validate_addresses(addresses, count=0)</code>","text":"<p>Validate that the addresses are in the correct format.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def validate_addresses(self, addresses: dict, count: int = 0) -&gt; None:\n    \"\"\"Validate that the addresses are in the correct format.\"\"\"\n    conn = DBConnectionFactory.create_db_connection().conn\n    if count==0: # Only do this once.\n        self.validate_schema(self.schema) # Ensure that the schema is valid before doing the addresses.\n    if count &gt; 9:\n        raise ValueError(\"Address dict must be of depth 10 or less!\")\n\n    if not isinstance(addresses, dict):\n        raise ValueError(\"Addresses must be provided as a dict!\")\n\n    for key, value in addresses.items():\n        if not isinstance(key, str):\n            raise ValueError(\"addresses must be str!\")\n        if not IDCreator(conn).is_ro_id(key):\n            raise ValueError(\"Addresses must have the proper prefixes to be considered valid!\")\n        if not ResearchObjectHandler.object_exists(key):\n            raise ValueError(\"The object at this address does not exist in the database!\")\n        self.validate_addresses(value, count + 1)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.validate_dataset_path","title":"<code>validate_dataset_path(path)</code>","text":"<p>Validate the dataset path.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def validate_dataset_path(self, path: str) -&gt; None:\n    \"\"\"Validate the dataset path.\"\"\"\n    import os\n    if not os.path.exists(path):\n        raise ValueError(\"Specified path is not a path or does not currently exist!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.validate_schema","title":"<code>validate_schema(schema)</code>","text":"<p>Validate that the data schema follows the proper format. Must be a dict of dicts, where all keys are Python types matching a DataObject subclass, and the lowest levels are empty.</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def validate_schema(self, schema: dict[dict]) -&gt; None:\n    \"\"\"Validate that the data schema follows the proper format.\n    Must be a dict of dicts, where all keys are Python types matching a DataObject subclass, and the lowest levels are empty.\"\"\"\n    subclasses = DataObject.__subclasses__()\n    # Check if the object is a dictionary\n    if not isinstance(schema, dict):\n        raise ValueError(\"The schema must be a dictionary!\")\n\n    for key, value in schema.items():\n        # Check if the key is an instance of a DataObject subclass Python type\n        if not key in subclasses:\n            raise ValueError(\"The key must be an instance of a DataObject subclass Python type!\")\n\n        self.validate_schema(value)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/phase/","title":"Phase","text":"<p>             Bases: <code>DataObject</code></p> <p>Phase class.</p> Source code in <code>src/ResearchOS/DataObjects/phase.py</code> <pre><code>class Phase(DataObject):\n    \"\"\"Phase class.\"\"\"\n\n    prefix = \"PH\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/phase/#src.ResearchOS.DataObjects.phase.Phase.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/DataObjects/phase.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/phase/#src.ResearchOS.DataObjects.phase.Phase.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/DataObjects/phase.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/phase/#src.ResearchOS.DataObjects.phase.Phase.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/DataObjects/phase.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/subject/","title":"Subject","text":"<p>             Bases: <code>DataObject</code></p> <p>Subject class.</p> Source code in <code>src/ResearchOS/DataObjects/subject.py</code> <pre><code>class Subject(DataObject):\n    \"\"\"Subject class.\"\"\"\n\n    prefix: str = \"SJ\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/subject/#src.ResearchOS.DataObjects.subject.Subject.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/DataObjects/subject.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/subject/#src.ResearchOS.DataObjects.subject.Subject.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/DataObjects/subject.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/subject/#src.ResearchOS.DataObjects.subject.Subject.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/DataObjects/subject.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/trial/","title":"Trial","text":"<p>             Bases: <code>DataObject</code></p> Source code in <code>src/ResearchOS/DataObjects/trial.py</code> <pre><code>class Trial(DataObject):\n\n    prefix = \"TR\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/trial/#src.ResearchOS.DataObjects.trial.Trial.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/DataObjects/trial.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/trial/#src.ResearchOS.DataObjects.trial.Trial.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/DataObjects/trial.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/trial/#src.ResearchOS.DataObjects.trial.Trial.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/DataObjects/trial.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/visit/","title":"Visit","text":"<p>             Bases: <code>DataObject</code></p> Source code in <code>src/ResearchOS/DataObjects/visit.py</code> <pre><code>class Visit(DataObject):\n\n    prefix: str = \"VS\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/visit/#src.ResearchOS.DataObjects.visit.Visit.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/DataObjects/visit.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/visit/#src.ResearchOS.DataObjects.visit.Visit.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/DataObjects/visit.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/visit/#src.ResearchOS.DataObjects.visit.Visit.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/DataObjects/visit.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    DataObject.load(self) # Load the attributes specific to it being a DataObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20%26%20Data%20Object%20Types/user/","title":"User","text":"Source code in <code>src/ResearchOS/user.py</code> <pre><code>class User():\n\n    pass\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20%26%20Data%20Object%20Types/variable/","title":"Variable","text":"<p>             Bases: <code>DataObject</code>, <code>PipelineObject</code></p> <p>Variable class.</p> Source code in <code>src/ResearchOS/variable.py</code> <pre><code>class Variable(DataObject,  PipelineObject):\n    \"\"\"Variable class.\"\"\"\n\n    prefix: str = \"VR\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        if name == \"vr\":\n            return\n            # raise ValueError(\"Cannot set 'vr' attribute for a Variable.\")\n        if name != \"value\":\n            ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n        # Set variable value.\n\n\n\n    def load(self) -&gt; None:\n        \"\"\"Load the variable-specific attributes from the database in an attribute-specific way.\"\"\"\n        pass\n\n    #################### Start class-specific attributes ###################\n    def validate_level(self, level: type) -&gt; None:\n        \"\"\"Check that the level is of a valid type.\"\"\"\n        if not isinstance(level, type):\n            raise ValueError(\"Level must be a type.\")\n        if not isinstance(level, DataObject):\n            raise ValueError(\"Level must be a DataObject.\")\n        us = User(id = User.get_current_user_object_id())\n        us.validate_current_project_id(id = us.current_project_id)\n        pj = Project(id = us.current_project_id)\n        pj.validate_current_dataset_id(id = pj.current_dataset_id)\n        ds = Dataset(id = pj.current_dataset_id)\n        ds.validate_schema(schema = ds.schema)\n        if level not in ds.schema:\n            raise ValueError(\"Level must be in the dataset schema.\")\n\n    #################### Start Source objects ####################\n    def get_source_object_ids(self, cls: type) -&gt; list:\n        \"\"\"Return a list of all source objects for the Variable.\"\"\"\n        ids = self._get_all_source_object_ids(cls = cls)\n        return [cls(id = id) for id in ids]\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20%26%20Data%20Object%20Types/variable/#src.ResearchOS.variable.Variable.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/variable.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20%26%20Data%20Object%20Types/variable/#src.ResearchOS.variable.Variable.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/variable.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    if name == \"vr\":\n        return\n        # raise ValueError(\"Cannot set 'vr' attribute for a Variable.\")\n    if name != \"value\":\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20%26%20Data%20Object%20Types/variable/#src.ResearchOS.variable.Variable.get_source_object_ids","title":"<code>get_source_object_ids(cls)</code>","text":"<p>Return a list of all source objects for the Variable.</p> Source code in <code>src/ResearchOS/variable.py</code> <pre><code>def get_source_object_ids(self, cls: type) -&gt; list:\n    \"\"\"Return a list of all source objects for the Variable.\"\"\"\n    ids = self._get_all_source_object_ids(cls = cls)\n    return [cls(id = id) for id in ids]\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20%26%20Data%20Object%20Types/variable/#src.ResearchOS.variable.Variable.load","title":"<code>load()</code>","text":"<p>Load the variable-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/variable.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the variable-specific attributes from the database in an attribute-specific way.\"\"\"\n    pass\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20%26%20Data%20Object%20Types/variable/#src.ResearchOS.variable.Variable.validate_level","title":"<code>validate_level(level)</code>","text":"<p>Check that the level is of a valid type.</p> Source code in <code>src/ResearchOS/variable.py</code> <pre><code>def validate_level(self, level: type) -&gt; None:\n    \"\"\"Check that the level is of a valid type.\"\"\"\n    if not isinstance(level, type):\n        raise ValueError(\"Level must be a type.\")\n    if not isinstance(level, DataObject):\n        raise ValueError(\"Level must be a DataObject.\")\n    us = User(id = User.get_current_user_object_id())\n    us.validate_current_project_id(id = us.current_project_id)\n    pj = Project(id = us.current_project_id)\n    pj.validate_current_dataset_id(id = pj.current_dataset_id)\n    ds = Dataset(id = pj.current_dataset_id)\n    ds.validate_schema(schema = ds.schema)\n    if level not in ds.schema:\n        raise ValueError(\"Level must be in the dataset schema.\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/analysis/","title":"Analysis","text":"<p>             Bases: <code>PipelineObject</code></p> Source code in <code>src/ResearchOS/PipelineObjects/analysis.py</code> <pre><code>class Analysis(PipelineObject):\n\n    prefix = \"AN\"        \n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/analysis/#src.ResearchOS.PipelineObjects.analysis.Analysis.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/PipelineObjects/analysis.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/analysis/#src.ResearchOS.PipelineObjects.analysis.Analysis.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/PipelineObjects/analysis.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/analysis/#src.ResearchOS.PipelineObjects.analysis.Analysis.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/PipelineObjects/analysis.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/","title":"Logsheet","text":"<p>             Bases: <code>PipelineObject</code></p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>class Logsheet(PipelineObject):\n\n    prefix = \"LG\"\n\n    #################### Start class-specific attributes ###################\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n\n    def read_and_clean_logsheet(self, nrows: int = None) -&gt; None:\n        \"\"\"Read the logsheet (CSV only) and clean it.\"\"\"\n        logsheet = []\n        first_elem_prefix = '\\ufeff'\n        with open(self.path, \"r\") as f:\n            reader = csv.reader(f, delimiter=',', quotechar='\"')            \n\n            for row_num, row in enumerate(reader):                                    \n                logsheet.append(row)\n                if nrows is not None and row_num == nrows-1:\n                    break\n\n        # 7. Check that the headers all match the logsheet.\n        logsheet[0][0] = logsheet[0][0].replace(first_elem_prefix, \"\")\n        return logsheet\n\n    ### Logsheet path\n\n    def validate_path(self, path: str) -&gt; None:\n        \"\"\"Validate the logsheet path.\"\"\"\n        # 1. Check that the path exists in the file system.\n        import os\n        if not isinstance(path, str):\n            raise ValueError(\"Path must be a string!\")\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path does not exist!\")\n        # 2. Check that the path is a file.\n        if not os.path.isfile(path):\n            raise ValueError(\"Specified path is not a file!\")\n        # 3. Check that the file is a CSV.\n        if not path.endswith((\"csv\", \"xlsx\", \"xls\")):\n            raise ValueError(\"Specified file is not a CSV!\")\n        # 4. Check that the file is not empty.\n        # if os.stat(path).st_size == 0:\n        #     raise ValueError(\"Specified file is empty!\")\n\n    ### Logsheet headers\n\n    def validate_headers(self, headers: list) -&gt; None:\n        \"\"\"Validate the logsheet headers. These are the headers that are in the logsheet file.\n        The headers must be a list of tuples, where each tuple has 3 elements:\n        1. A string (the name of the header)\n        2. A type (the type of the header)\n        3. A valid variable ID (the ID of the Variable that the header corresponds to)\"\"\"\n        self.validate_path(self.path)\n\n        # 1. Check that the headers are a list.\n        if not isinstance(headers, list):\n            raise ValueError(\"Headers must be a list!\")\n\n        # 2. Check that the headers are a list of tuples and meet the other requirements.        \n        for header in headers:\n            if not isinstance(header, tuple):\n                raise ValueError(\"Headers must be a list of tuples!\")\n            # 3. Check that each header tuple has 3 elements.        \n            if len(header) != 3:\n                raise ValueError(\"Each header tuple must have 3 elements!\")\n            # 4. Check that the first element of each header tuple is a string.        \n            if not isinstance(header[0], str):\n                raise ValueError(\"First element of each header tuple must be a string!\")\n            # 5. Check that the second element of each header tuple is a type.        \n            # if not isinstance(header[1], type):\n            #     raise ValueError(\"Second element of each header tuple must be a Python type!\")\n            if header[1] not in [str, int, float]:\n                raise ValueError(\"Second element of each header tuple must be a Python type!\")\n            # 6. Check that the third element of each header tuple is a valid variable ID.                \n            if not header[2].startswith(Variable.prefix) or not ResearchObjectHandler.object_exists(header[2]):\n                raise ValueError(\"Third element of each header tuple must be a valid pre-existing variable ID!\")\n\n        logsheet = self.read_and_clean_logsheet(nrows = 1)\n        headers_in_logsheet = logsheet[0]\n        header_names = [header[0] for header in headers]\n        missing = [header for header in headers_in_logsheet if header not in header_names]\n\n        if len(missing) &gt; 0:\n            raise ValueError(f\"The headers {missing} do not match between logsheet and code!\")\n\n    def to_json_headers(self, headers: list) -&gt; str:\n        \"\"\"Convert the headers to a JSON string.\"\"\"\n        str_headers = []\n        for header in headers:\n            str_headers.append((header[0], str(header[1])[8:-2], header[2]))\n        return json.dumps(str_headers)\n\n    def from_json_headers(self, json_var: str) -&gt; list:\n        \"\"\"Convert the JSON string to a list of headers.\"\"\"\n        str_var = json.loads(json_var)\n        headers = []\n        mapping = {\n            \"str\": str,\n            \"int\": int,\n            \"float\": float\n        }\n        for header in str_var:\n            headers.append((header[0], mapping[header[1]], header[2]))                \n        return headers\n\n    ### Num header rows\n\n    def validate_num_header_rows(self, num_header_rows: int) -&gt; None:\n        \"\"\"Validate the number of header rows. If it is not valid, the value is rejected.\"\"\"                \n        if not isinstance(num_header_rows, int | float):\n            raise ValueError(\"Num header rows must be numeric!\")\n        if num_header_rows&lt;0:\n            raise ValueError(\"Num header rows must be positive!\")\n        if num_header_rows % 1 != 0:\n            raise ValueError(\"Num header rows must be an integer!\")  \n\n    ### Class column names\n\n    def validate_class_column_names(self, class_column_names: dict) -&gt; None:\n        \"\"\"Validate the class column names.\"\"\"\n        # 1. Check that the class column names are a dict.\n        if not isinstance(class_column_names, dict):\n            raise ValueError(\"Class column names must be a dict!\")\n        # 2. Check that the class column names are a dict of str to type.        \n        for key, value in class_column_names.items():\n            if not isinstance(key, str):\n                raise ValueError(\"Keys of class column names must be strings!\")\n            if not issubclass(value, DataObject):\n                raise ValueError(\"Values of class column names must be Python types that subclass DataObject!\")\n\n        headers = self.read_and_clean_logsheet(nrows = 1)[0]\n        if not all([header in headers for header in class_column_names.keys()]):\n            raise ValueError(\"The class column names must be in the logsheet headers!\")\n\n    def from_json_class_column_names(self, json_var: dict) -&gt; dict:\n        \"\"\"Convert the dict from JSON string where values are class prefixes to a dict where keys are column names and values are DataObject subclasses.\"\"\"     \n        prefix_var = json.loads(json_var)\n        class_column_names = {}\n        all_classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n        for key, prefix in prefix_var.items():\n            for cls in all_classes:\n                if hasattr(cls, \"prefix\") and cls.prefix == prefix:\n                    class_column_names[key] = cls\n                    break\n        return class_column_names\n\n    def to_json_class_column_names(self, var: dict) -&gt; dict:\n        \"\"\"Convert the dict from a dict where keys are column names and values are DataObject subclasses to a JSON string where values are class prefixes.\"\"\"        \n        prefix_var = {}\n        for key in var:\n            prefix_var[key] = var[key].prefix\n        return json.dumps(prefix_var)\n\n    ### Subset ID\n\n    def validate_subset_id(self, subset_id: str) -&gt; None:\n        \"\"\"Validate the subset ID.\"\"\"\n        if not ResearchObjectHandler.object_exists(subset_id):\n            raise ValueError(\"Subset ID must be a valid ID!\")\n        if not subset_id.startswith(Subset.prefix):\n            raise ValueError(\"Subset ID must start with the correct prefix!\")\n\n    #################### Start class-specific methods ####################\n    def load_xlsx(self) -&gt; list[list]:\n        \"\"\"Load the logsheet as a list of lists using Pandas.\"\"\"        \n        df = pd.read_excel(self.path, header = None)\n        return df.values.tolist()\n\n    def read_logsheet(self) -&gt; None:\n        \"\"\"Run the logsheet import process.\"\"\"\n        self.validate_class_column_names(self.class_column_names)\n        self.validate_headers(self.headers)\n        self.validate_num_header_rows(self.num_header_rows)\n        self.validate_path(self.path)\n        self.validate_subset_id(self.subset_id)\n\n        conn = DBConnectionFactory.create_db_connection().conn\n\n        # 1. Load the logsheet (using built-in Python libraries)\n        if self.path.endswith((\"xlsx\", \"xls\")):\n            full_logsheet = self.load_xlsx()\n        else:\n            full_logsheet = self.read_and_clean_logsheet()\n\n        if len(full_logsheet) &lt; self.num_header_rows:\n            raise ValueError(\"The number of header rows is greater than the number of rows in the logsheet!\")\n\n        # Run the logsheet import.\n        # headers = full_logsheet[0:self.num_header_rows]\n        if len(full_logsheet) == self.num_header_rows:\n            logsheet = []\n        else:\n            logsheet = full_logsheet[self.num_header_rows:]\n\n        # For each row, connect instances of the appropriate DataObject subclass to all other instances of appropriate DataObject subclasses.\n        headers_in_logsheet = full_logsheet[0]\n        header_names = [header[0] for header in self.headers]\n        header_types = [header[1] for header in self.headers]\n        header_vrids = [header[2] for header in self.headers]\n        # Load/create all of the Variables\n        vr_list = []\n        for idx, vrid in enumerate(header_vrids):\n            vr = Variable(id = vrid, name = header_names[idx])\n            vr_list.append(vr.id)\n        # Order the class column names by precedence in the schema so that higher level objects always exist before lower level, so they can be attached.\n        # schema_ordered_col_names = self.order_class_column_names()\n        idcreator = IDCreator(conn)\n        for row in logsheet:            \n            # Create a new instance of the appropriate DataObject subclass(es) and store it in the database.\n            # TODO: How to order the data objects?\n            for header, cls in self.class_column_names.items():\n                col_idx = headers_in_logsheet.index(header)\n                raw_value = row[col_idx]\n                value = header_types[col_idx](raw_value)\n\n                # Create the DataObject instance.\n                new_dobj = cls(id = idcreator.create_ro_id(cls))\n\n                new_dobj.vr[vr_list[col_idx]] = value\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.from_json_class_column_names","title":"<code>from_json_class_column_names(json_var)</code>","text":"<p>Convert the dict from JSON string where values are class prefixes to a dict where keys are column names and values are DataObject subclasses.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def from_json_class_column_names(self, json_var: dict) -&gt; dict:\n    \"\"\"Convert the dict from JSON string where values are class prefixes to a dict where keys are column names and values are DataObject subclasses.\"\"\"     \n    prefix_var = json.loads(json_var)\n    class_column_names = {}\n    all_classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n    for key, prefix in prefix_var.items():\n        for cls in all_classes:\n            if hasattr(cls, \"prefix\") and cls.prefix == prefix:\n                class_column_names[key] = cls\n                break\n    return class_column_names\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.from_json_headers","title":"<code>from_json_headers(json_var)</code>","text":"<p>Convert the JSON string to a list of headers.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def from_json_headers(self, json_var: str) -&gt; list:\n    \"\"\"Convert the JSON string to a list of headers.\"\"\"\n    str_var = json.loads(json_var)\n    headers = []\n    mapping = {\n        \"str\": str,\n        \"int\": int,\n        \"float\": float\n    }\n    for header in str_var:\n        headers.append((header[0], mapping[header[1]], header[2]))                \n    return headers\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.load_xlsx","title":"<code>load_xlsx()</code>","text":"<p>Load the logsheet as a list of lists using Pandas.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def load_xlsx(self) -&gt; list[list]:\n    \"\"\"Load the logsheet as a list of lists using Pandas.\"\"\"        \n    df = pd.read_excel(self.path, header = None)\n    return df.values.tolist()\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.read_and_clean_logsheet","title":"<code>read_and_clean_logsheet(nrows=None)</code>","text":"<p>Read the logsheet (CSV only) and clean it.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def read_and_clean_logsheet(self, nrows: int = None) -&gt; None:\n    \"\"\"Read the logsheet (CSV only) and clean it.\"\"\"\n    logsheet = []\n    first_elem_prefix = '\\ufeff'\n    with open(self.path, \"r\") as f:\n        reader = csv.reader(f, delimiter=',', quotechar='\"')            \n\n        for row_num, row in enumerate(reader):                                    \n            logsheet.append(row)\n            if nrows is not None and row_num == nrows-1:\n                break\n\n    # 7. Check that the headers all match the logsheet.\n    logsheet[0][0] = logsheet[0][0].replace(first_elem_prefix, \"\")\n    return logsheet\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.read_logsheet","title":"<code>read_logsheet()</code>","text":"<p>Run the logsheet import process.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def read_logsheet(self) -&gt; None:\n    \"\"\"Run the logsheet import process.\"\"\"\n    self.validate_class_column_names(self.class_column_names)\n    self.validate_headers(self.headers)\n    self.validate_num_header_rows(self.num_header_rows)\n    self.validate_path(self.path)\n    self.validate_subset_id(self.subset_id)\n\n    conn = DBConnectionFactory.create_db_connection().conn\n\n    # 1. Load the logsheet (using built-in Python libraries)\n    if self.path.endswith((\"xlsx\", \"xls\")):\n        full_logsheet = self.load_xlsx()\n    else:\n        full_logsheet = self.read_and_clean_logsheet()\n\n    if len(full_logsheet) &lt; self.num_header_rows:\n        raise ValueError(\"The number of header rows is greater than the number of rows in the logsheet!\")\n\n    # Run the logsheet import.\n    # headers = full_logsheet[0:self.num_header_rows]\n    if len(full_logsheet) == self.num_header_rows:\n        logsheet = []\n    else:\n        logsheet = full_logsheet[self.num_header_rows:]\n\n    # For each row, connect instances of the appropriate DataObject subclass to all other instances of appropriate DataObject subclasses.\n    headers_in_logsheet = full_logsheet[0]\n    header_names = [header[0] for header in self.headers]\n    header_types = [header[1] for header in self.headers]\n    header_vrids = [header[2] for header in self.headers]\n    # Load/create all of the Variables\n    vr_list = []\n    for idx, vrid in enumerate(header_vrids):\n        vr = Variable(id = vrid, name = header_names[idx])\n        vr_list.append(vr.id)\n    # Order the class column names by precedence in the schema so that higher level objects always exist before lower level, so they can be attached.\n    # schema_ordered_col_names = self.order_class_column_names()\n    idcreator = IDCreator(conn)\n    for row in logsheet:            \n        # Create a new instance of the appropriate DataObject subclass(es) and store it in the database.\n        # TODO: How to order the data objects?\n        for header, cls in self.class_column_names.items():\n            col_idx = headers_in_logsheet.index(header)\n            raw_value = row[col_idx]\n            value = header_types[col_idx](raw_value)\n\n            # Create the DataObject instance.\n            new_dobj = cls(id = idcreator.create_ro_id(cls))\n\n            new_dobj.vr[vr_list[col_idx]] = value\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.to_json_class_column_names","title":"<code>to_json_class_column_names(var)</code>","text":"<p>Convert the dict from a dict where keys are column names and values are DataObject subclasses to a JSON string where values are class prefixes.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def to_json_class_column_names(self, var: dict) -&gt; dict:\n    \"\"\"Convert the dict from a dict where keys are column names and values are DataObject subclasses to a JSON string where values are class prefixes.\"\"\"        \n    prefix_var = {}\n    for key in var:\n        prefix_var[key] = var[key].prefix\n    return json.dumps(prefix_var)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.to_json_headers","title":"<code>to_json_headers(headers)</code>","text":"<p>Convert the headers to a JSON string.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def to_json_headers(self, headers: list) -&gt; str:\n    \"\"\"Convert the headers to a JSON string.\"\"\"\n    str_headers = []\n    for header in headers:\n        str_headers.append((header[0], str(header[1])[8:-2], header[2]))\n    return json.dumps(str_headers)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_class_column_names","title":"<code>validate_class_column_names(class_column_names)</code>","text":"<p>Validate the class column names.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def validate_class_column_names(self, class_column_names: dict) -&gt; None:\n    \"\"\"Validate the class column names.\"\"\"\n    # 1. Check that the class column names are a dict.\n    if not isinstance(class_column_names, dict):\n        raise ValueError(\"Class column names must be a dict!\")\n    # 2. Check that the class column names are a dict of str to type.        \n    for key, value in class_column_names.items():\n        if not isinstance(key, str):\n            raise ValueError(\"Keys of class column names must be strings!\")\n        if not issubclass(value, DataObject):\n            raise ValueError(\"Values of class column names must be Python types that subclass DataObject!\")\n\n    headers = self.read_and_clean_logsheet(nrows = 1)[0]\n    if not all([header in headers for header in class_column_names.keys()]):\n        raise ValueError(\"The class column names must be in the logsheet headers!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_headers","title":"<code>validate_headers(headers)</code>","text":"<p>Validate the logsheet headers. These are the headers that are in the logsheet file. The headers must be a list of tuples, where each tuple has 3 elements: 1. A string (the name of the header) 2. A type (the type of the header) 3. A valid variable ID (the ID of the Variable that the header corresponds to)</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def validate_headers(self, headers: list) -&gt; None:\n    \"\"\"Validate the logsheet headers. These are the headers that are in the logsheet file.\n    The headers must be a list of tuples, where each tuple has 3 elements:\n    1. A string (the name of the header)\n    2. A type (the type of the header)\n    3. A valid variable ID (the ID of the Variable that the header corresponds to)\"\"\"\n    self.validate_path(self.path)\n\n    # 1. Check that the headers are a list.\n    if not isinstance(headers, list):\n        raise ValueError(\"Headers must be a list!\")\n\n    # 2. Check that the headers are a list of tuples and meet the other requirements.        \n    for header in headers:\n        if not isinstance(header, tuple):\n            raise ValueError(\"Headers must be a list of tuples!\")\n        # 3. Check that each header tuple has 3 elements.        \n        if len(header) != 3:\n            raise ValueError(\"Each header tuple must have 3 elements!\")\n        # 4. Check that the first element of each header tuple is a string.        \n        if not isinstance(header[0], str):\n            raise ValueError(\"First element of each header tuple must be a string!\")\n        # 5. Check that the second element of each header tuple is a type.        \n        # if not isinstance(header[1], type):\n        #     raise ValueError(\"Second element of each header tuple must be a Python type!\")\n        if header[1] not in [str, int, float]:\n            raise ValueError(\"Second element of each header tuple must be a Python type!\")\n        # 6. Check that the third element of each header tuple is a valid variable ID.                \n        if not header[2].startswith(Variable.prefix) or not ResearchObjectHandler.object_exists(header[2]):\n            raise ValueError(\"Third element of each header tuple must be a valid pre-existing variable ID!\")\n\n    logsheet = self.read_and_clean_logsheet(nrows = 1)\n    headers_in_logsheet = logsheet[0]\n    header_names = [header[0] for header in headers]\n    missing = [header for header in headers_in_logsheet if header not in header_names]\n\n    if len(missing) &gt; 0:\n        raise ValueError(f\"The headers {missing} do not match between logsheet and code!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_num_header_rows","title":"<code>validate_num_header_rows(num_header_rows)</code>","text":"<p>Validate the number of header rows. If it is not valid, the value is rejected.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def validate_num_header_rows(self, num_header_rows: int) -&gt; None:\n    \"\"\"Validate the number of header rows. If it is not valid, the value is rejected.\"\"\"                \n    if not isinstance(num_header_rows, int | float):\n        raise ValueError(\"Num header rows must be numeric!\")\n    if num_header_rows&lt;0:\n        raise ValueError(\"Num header rows must be positive!\")\n    if num_header_rows % 1 != 0:\n        raise ValueError(\"Num header rows must be an integer!\")  \n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_path","title":"<code>validate_path(path)</code>","text":"<p>Validate the logsheet path.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def validate_path(self, path: str) -&gt; None:\n    \"\"\"Validate the logsheet path.\"\"\"\n    # 1. Check that the path exists in the file system.\n    import os\n    if not isinstance(path, str):\n        raise ValueError(\"Path must be a string!\")\n    if not os.path.exists(path):\n        raise ValueError(\"Specified path does not exist!\")\n    # 2. Check that the path is a file.\n    if not os.path.isfile(path):\n        raise ValueError(\"Specified path is not a file!\")\n    # 3. Check that the file is a CSV.\n    if not path.endswith((\"csv\", \"xlsx\", \"xls\")):\n        raise ValueError(\"Specified file is not a CSV!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_subset_id","title":"<code>validate_subset_id(subset_id)</code>","text":"<p>Validate the subset ID.</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def validate_subset_id(self, subset_id: str) -&gt; None:\n    \"\"\"Validate the subset ID.\"\"\"\n    if not ResearchObjectHandler.object_exists(subset_id):\n        raise ValueError(\"Subset ID must be a valid ID!\")\n    if not subset_id.startswith(Subset.prefix):\n        raise ValueError(\"Subset ID must start with the correct prefix!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/pipeline_object/","title":"Pipeline Objects","text":"<p>             Bases: <code>ResearchObject</code></p> <p>Parent class of all pipeline objects: Projects, Analyses, Logsheets, Process Groups, Processes, Variables, SpecifyTrials, Views</p> Source code in <code>src/ResearchOS/PipelineObjects/pipeline_object.py</code> <pre><code>class PipelineObject(ResearchObject):\n    \"\"\"Parent class of all pipeline objects: Projects, Analyses, Logsheets, Process Groups, Processes, Variables, SpecifyTrials, Views\"\"\"\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        pass\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/pipeline_object/#src.ResearchOS.PipelineObjects.pipeline_object.PipelineObject.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/PipelineObjects/pipeline_object.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    pass\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/","title":"Process","text":"<p>             Bases: <code>PipelineObject</code></p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>class Process(PipelineObject):\n\n    prefix = \"PR\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n\n    #################### Start class-specific attributes ###################\n    def validate_method(self, method: Callable) -&gt; None:\n        if not isinstance(method, Callable):\n            raise ValueError(\"Method must be a callable function!\")\n        if method not in globals().values():\n            raise ValueError(\"Method must be a global function!\")\n\n    def validate_level(self, level: type) -&gt; None:\n        if not isinstance(level, type):\n            raise ValueError(\"Level must be a type!\")\n\n    def validate_input_vr(self, inputs: dict) -&gt; None:\n        \"\"\"Validate that the input variables are correct.\"\"\"\n        self._validate_vr(self, inputs)\n\n    def validate_output_vr(self, outputs: dict) -&gt; None:\n        \"\"\"Validate that the output variables are correct.\"\"\"\n        self._validate_vr(self, outputs)\n\n    def _validate_vr(self, vr: dict) -&gt; None:\n        \"\"\"Validate that the input and output variables are correct. They should follow the same format.\n        The format is a dictionary with the variable name as the key and the variable ID as the value.\"\"\"\n        if not isinstance(vr, dict):\n            raise ValueError(\"Variables must be a dictionary.\")\n        for key, value in vr.items():\n            if not isinstance(key, str):\n                raise ValueError(\"Variable names in code must be strings.\")\n            if not str(key).isidentifier():\n                raise ValueError(\"Variable names in code must be valid variable names.\")\n            if not ResearchObjectHandler.object_exists(value):\n                raise ValueError(\"Variable ID's must reference existing Variables.\")\n\n    def from_json_method(self, json_method: str) -&gt; Callable:\n        \"\"\"Convert a JSON string to a method.\n        Returns None if the method name is not found (e.g. if code changed locations or something)\"\"\"\n        method_name = json.loads(json_method)\n        if method_name in globals():\n            method = globals()[method_name]\n        else:\n           print(f\"Method {method_name} not found in globals.\")\n           method = None\n        return method\n\n    def to_json_method(self, method: Callable) -&gt; str:\n        \"\"\"Convert a method to a JSON string.\"\"\"\n        return json.dumps(method.__name__)\n\n    def from_json_level(self, level: str) -&gt; type:\n        \"\"\"Convert a JSON string to a Process level.\"\"\"\n        classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n        for cls in classes:\n            if hasattr(cls, \"prefix\") and cls.prefix == level:\n                return cls\n\n    def to_json_level(self, level: type) -&gt; str:\n        \"\"\"Convert a Process level to a JSON string.\"\"\"\n        return json.dumps(level.prefix)\n\n    #################### Start class-specific methods ###################\n    def get_input_variables(self) -&gt; list:\n        \"\"\"Return a list of variable IDs that belong to this process.\"\"\"        \n        vr_ids = self._get_all_source_object_ids(cls = Variable)\n        return self._gen_obj_or_none(vr_ids, Variable)\n\n    def get_output_variables(self) -&gt; list:\n        \"\"\"Return a list of variable IDs that belong to this process.\"\"\"        \n        vr_ids = self._get_all_target_object_ids(cls = Variable)\n        return self._gen_obj_or_none(vr_ids, Variable)\n\n    def get_subsets(self) -&gt; list:\n        \"\"\"Return a list of subset IDs that belong to this process.\"\"\"        \n        ss_ids = self._get_all_target_object_ids(cls = Subset)\n        return self._gen_obj_or_none(ss_ids, Subset)\n\n    def add_input_variable_id(self, var_id):\n        \"\"\"Add an input variable to the process.\"\"\"\n        # TODO: Need to add a mapping between variable ID and name in code.        \n        self._add_source_object_id(var_id)\n\n    def add_output_variable_id(self, var_id):\n        \"\"\"Add an output variable to the process.\"\"\"\n        # TODO: Need to add a mapping between variable ID and name in code.        \n        self._add_target_object_id(var_id)\n\n    def remove_input_variable_id(self, var_id):\n        \"\"\"Remove an input variable from the process.\"\"\"        \n        self._remove_source_object_id(var_id)\n\n    def remove_output_variable_id(self, var_id):\n        \"\"\"Remove an output variable from the process.\"\"\"        \n        self._remove_target_object_id(var_id)\n\n    def add_subset_id(self, ss_id):\n        \"\"\"Add a subset to the process.\"\"\"        \n        self._add_target_object_id(ss_id)\n\n    def remove_subset_id(self, ss_id):\n        \"\"\"Remove a subset from the process.\"\"\"        \n        self._remove_target_object_id(ss_id)\n\n    def run_method(self) -&gt; None:\n        \"\"\"Execute the attached method.\"\"\"\n        # 1. Validate that the level &amp; method have been properly set.\n        self.validate_method(self.method)\n        self.validate_level(self.level)\n\n        # 2. Validate that the input &amp; output variables have been properly set.\n        self.validate_input_vr()\n        self.validate_output_vr()\n\n        # 3. Validate that the subsets have been properly set.\n        self.validate_subset()\n\n        # 4. Run the method.\n        # Get the subset of the data.\n        subset_dict = {} # Comes from Subset object.\n        # Get data schema\n        conn = DBConnectionFactory().create_db_connection().conn\n        us = CurrentUser(conn).get_current_user_id()\n        pj = Project(id = us.current_project_id)\n        ds = Dataset(id = pj.current_dataset_id)\n        # Preserves the hierarchy order, but only includes levels needed for this method.\n        curr_schema = [sch for sch in ds.schema if sch in self.level]\n        self._run_index = -1 # Tells the run method which index we are on.\n        self._current_schema = [None for _ in range(len(curr_schema))] # Initialize with None.\n        self.run_recurse(curr_schema)\n        self.output_vr_names_in_code = get_returned_variable_names(self.method)\n\n    def run_recurse(self, full_schema: list[type]) -&gt; None:\n        \"\"\"Run the method, looping recursively.\"\"\"        \n        self._run_index +=1\n        for sch in full_schema:\n            # If index is not at the lowest level, recurse.\n            self._current_schema[self._run_index] = sch\n            if self._run_index &lt; len(full_schema) - 1:                \n                self.run_recurse(full_schema)\n                continue\n            # If index is at the lowest level, run the method.\n            # 1. Get the input variables.\n            input_vars_name_value = {} # Dict: var_name: var_value\n            for vr in self.input_vr:\n                input_vars_name_value[vr] = vr.get_value(sch)\n            outputs = self.method(**input_vars_name_value)\n            if not isinstance(outputs, tuple):\n                raise ValueError(\"Method must return a tuple of outputs.\")\n            if len(outputs) != len(self.output_vr_names_in_code):\n                raise ValueError(\"Method must return the same number of outputs as output variables.\")\n            for idx, output in enumerate(outputs):\n                name_in_code = self.output_vr_names_in_code[idx]\n                vr = Variable(id = name_in_code, address = sch)\n                vr.value = output\n        self._current_schema[self._run_index] = None # Reset\n        self._run_index -=1\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.add_input_variable_id","title":"<code>add_input_variable_id(var_id)</code>","text":"<p>Add an input variable to the process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def add_input_variable_id(self, var_id):\n    \"\"\"Add an input variable to the process.\"\"\"\n    # TODO: Need to add a mapping between variable ID and name in code.        \n    self._add_source_object_id(var_id)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.add_output_variable_id","title":"<code>add_output_variable_id(var_id)</code>","text":"<p>Add an output variable to the process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def add_output_variable_id(self, var_id):\n    \"\"\"Add an output variable to the process.\"\"\"\n    # TODO: Need to add a mapping between variable ID and name in code.        \n    self._add_target_object_id(var_id)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.add_subset_id","title":"<code>add_subset_id(ss_id)</code>","text":"<p>Add a subset to the process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def add_subset_id(self, ss_id):\n    \"\"\"Add a subset to the process.\"\"\"        \n    self._add_target_object_id(ss_id)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.from_json_level","title":"<code>from_json_level(level)</code>","text":"<p>Convert a JSON string to a Process level.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def from_json_level(self, level: str) -&gt; type:\n    \"\"\"Convert a JSON string to a Process level.\"\"\"\n    classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n    for cls in classes:\n        if hasattr(cls, \"prefix\") and cls.prefix == level:\n            return cls\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.from_json_method","title":"<code>from_json_method(json_method)</code>","text":"<p>Convert a JSON string to a method. Returns None if the method name is not found (e.g. if code changed locations or something)</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def from_json_method(self, json_method: str) -&gt; Callable:\n    \"\"\"Convert a JSON string to a method.\n    Returns None if the method name is not found (e.g. if code changed locations or something)\"\"\"\n    method_name = json.loads(json_method)\n    if method_name in globals():\n        method = globals()[method_name]\n    else:\n       print(f\"Method {method_name} not found in globals.\")\n       method = None\n    return method\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.get_input_variables","title":"<code>get_input_variables()</code>","text":"<p>Return a list of variable IDs that belong to this process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def get_input_variables(self) -&gt; list:\n    \"\"\"Return a list of variable IDs that belong to this process.\"\"\"        \n    vr_ids = self._get_all_source_object_ids(cls = Variable)\n    return self._gen_obj_or_none(vr_ids, Variable)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.get_output_variables","title":"<code>get_output_variables()</code>","text":"<p>Return a list of variable IDs that belong to this process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def get_output_variables(self) -&gt; list:\n    \"\"\"Return a list of variable IDs that belong to this process.\"\"\"        \n    vr_ids = self._get_all_target_object_ids(cls = Variable)\n    return self._gen_obj_or_none(vr_ids, Variable)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.get_subsets","title":"<code>get_subsets()</code>","text":"<p>Return a list of subset IDs that belong to this process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def get_subsets(self) -&gt; list:\n    \"\"\"Return a list of subset IDs that belong to this process.\"\"\"        \n    ss_ids = self._get_all_target_object_ids(cls = Subset)\n    return self._gen_obj_or_none(ss_ids, Subset)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.remove_input_variable_id","title":"<code>remove_input_variable_id(var_id)</code>","text":"<p>Remove an input variable from the process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def remove_input_variable_id(self, var_id):\n    \"\"\"Remove an input variable from the process.\"\"\"        \n    self._remove_source_object_id(var_id)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.remove_output_variable_id","title":"<code>remove_output_variable_id(var_id)</code>","text":"<p>Remove an output variable from the process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def remove_output_variable_id(self, var_id):\n    \"\"\"Remove an output variable from the process.\"\"\"        \n    self._remove_target_object_id(var_id)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.remove_subset_id","title":"<code>remove_subset_id(ss_id)</code>","text":"<p>Remove a subset from the process.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def remove_subset_id(self, ss_id):\n    \"\"\"Remove a subset from the process.\"\"\"        \n    self._remove_target_object_id(ss_id)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.run_method","title":"<code>run_method()</code>","text":"<p>Execute the attached method.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def run_method(self) -&gt; None:\n    \"\"\"Execute the attached method.\"\"\"\n    # 1. Validate that the level &amp; method have been properly set.\n    self.validate_method(self.method)\n    self.validate_level(self.level)\n\n    # 2. Validate that the input &amp; output variables have been properly set.\n    self.validate_input_vr()\n    self.validate_output_vr()\n\n    # 3. Validate that the subsets have been properly set.\n    self.validate_subset()\n\n    # 4. Run the method.\n    # Get the subset of the data.\n    subset_dict = {} # Comes from Subset object.\n    # Get data schema\n    conn = DBConnectionFactory().create_db_connection().conn\n    us = CurrentUser(conn).get_current_user_id()\n    pj = Project(id = us.current_project_id)\n    ds = Dataset(id = pj.current_dataset_id)\n    # Preserves the hierarchy order, but only includes levels needed for this method.\n    curr_schema = [sch for sch in ds.schema if sch in self.level]\n    self._run_index = -1 # Tells the run method which index we are on.\n    self._current_schema = [None for _ in range(len(curr_schema))] # Initialize with None.\n    self.run_recurse(curr_schema)\n    self.output_vr_names_in_code = get_returned_variable_names(self.method)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.run_recurse","title":"<code>run_recurse(full_schema)</code>","text":"<p>Run the method, looping recursively.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def run_recurse(self, full_schema: list[type]) -&gt; None:\n    \"\"\"Run the method, looping recursively.\"\"\"        \n    self._run_index +=1\n    for sch in full_schema:\n        # If index is not at the lowest level, recurse.\n        self._current_schema[self._run_index] = sch\n        if self._run_index &lt; len(full_schema) - 1:                \n            self.run_recurse(full_schema)\n            continue\n        # If index is at the lowest level, run the method.\n        # 1. Get the input variables.\n        input_vars_name_value = {} # Dict: var_name: var_value\n        for vr in self.input_vr:\n            input_vars_name_value[vr] = vr.get_value(sch)\n        outputs = self.method(**input_vars_name_value)\n        if not isinstance(outputs, tuple):\n            raise ValueError(\"Method must return a tuple of outputs.\")\n        if len(outputs) != len(self.output_vr_names_in_code):\n            raise ValueError(\"Method must return the same number of outputs as output variables.\")\n        for idx, output in enumerate(outputs):\n            name_in_code = self.output_vr_names_in_code[idx]\n            vr = Variable(id = name_in_code, address = sch)\n            vr.value = output\n    self._current_schema[self._run_index] = None # Reset\n    self._run_index -=1\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.to_json_level","title":"<code>to_json_level(level)</code>","text":"<p>Convert a Process level to a JSON string.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def to_json_level(self, level: type) -&gt; str:\n    \"\"\"Convert a Process level to a JSON string.\"\"\"\n    return json.dumps(level.prefix)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.to_json_method","title":"<code>to_json_method(method)</code>","text":"<p>Convert a method to a JSON string.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def to_json_method(self, method: Callable) -&gt; str:\n    \"\"\"Convert a method to a JSON string.\"\"\"\n    return json.dumps(method.__name__)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.validate_input_vr","title":"<code>validate_input_vr(inputs)</code>","text":"<p>Validate that the input variables are correct.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def validate_input_vr(self, inputs: dict) -&gt; None:\n    \"\"\"Validate that the input variables are correct.\"\"\"\n    self._validate_vr(self, inputs)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.validate_output_vr","title":"<code>validate_output_vr(outputs)</code>","text":"<p>Validate that the output variables are correct.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def validate_output_vr(self, outputs: dict) -&gt; None:\n    \"\"\"Validate that the output variables are correct.\"\"\"\n    self._validate_vr(self, outputs)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/","title":"Project","text":"<p>Inherits from PipelineObject</p> <p>             Bases: <code>PipelineObject</code></p> <p>A project is a collection of analyses. Class-specific Attributes: 1. current_analysis_id: The ID of the current analysis for this project. 2. current_dataset_id: The ID of the current dataset for this project. 3. project path: The root folder location of the project.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>class Project(PipelineObject):\n    \"\"\"A project is a collection of analyses.\n    Class-specific Attributes:\n    1. current_analysis_id: The ID of the current analysis for this project.\n    2. current_dataset_id: The ID of the current dataset for this project.\n    3. project path: The root folder location of the project.\"\"\"\n\n    prefix: str = \"PJ\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n\n    def validate_current_analysis_id(self, id: str) -&gt; None:\n        \"\"\"Validate the current analysis ID. If it is not valid, the value is rejected.\"\"\"        \n        if not isinstance(id, str):\n            raise ValueError(\"Specified value is not a string!\")\n        if not self.is_id(id):\n            raise ValueError(\"Specified value is not an ID!\")\n        parsed_id = self.parse_id(id)\n        if parsed_id[0] != \"AN\":\n            raise ValueError(\"Specified ID is not an Analysis!\")\n        if not self.object_exists(id):\n            raise ValueError(\"Analysis does not exist!\")\n\n    def validate_current_dataset_id(self, id: str) -&gt; None:\n        \"\"\"Validate the current dataset ID. If it is not valid, the value is rejected.\"\"\"\n        if not self.is_id(id):\n            raise ValueError(\"Specified value is not an ID!\")\n        parsed_id = self.parse_id(id)\n        if parsed_id[0] != \"DS\":\n            raise ValueError(\"Specified ID is not a Dataset!\")\n        if not self.object_exists(id):\n            raise ValueError(\"Dataset does not exist!\")\n\n    def validate_project_path(self, path: str) -&gt; None:\n        \"\"\"Validate the project path. If it is not valid, the value is rejected.\"\"\"\n        # 1. Check that the path exists in the file system.\n        import os\n        if not isinstance(path, str):\n            raise ValueError(\"Specified path is not a string!\")\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path is not a path or does not currently exist!\")        \n\n    #################### Start Source objects ####################\n    def get_users(self) -&gt; list:\n        \"\"\"Return a list of user objects that belong to this project. Identical to Dataset.get_users()\"\"\"\n        from ResearchOS.user import User\n        us_ids = self._get_all_source_object_ids(cls = User)\n        return self._gen_obj_or_none(us_ids, User)\n\n    #################### Start Target objects ####################\n    def get_analyses(self) -&gt; list[\"Analysis\"]:        \n        \"\"\"Return a list of analysis objects in the project.\"\"\"\n        from ResearchOS.PipelineObjects.analysis import Analysis\n        an_ids = self._get_all_target_object_ids(cls = Analysis)\n        return self._gen_obj_or_none(an_ids, Analysis)\n\n    def add_analysis_id(self, analysis_id: str):\n        \"\"\"Add an analysis to the project.\"\"\"\n        from ResearchOS.PipelineObjects.analysis import Analysis        \n        self._add_target_object_id(analysis_id, cls = Analysis)\n\n    def remove_analysis_id(self, analysis_id: str):\n        \"\"\"Remove an analysis from the project.\"\"\"\n        from ResearchOS.PipelineObjects.analysis import Analysis        \n        self._remove_target_object_id(analysis_id, cls = Analysis)\n\n    #################### Start class-specific methods ####################\n    def open_project_path(self) -&gt; None:\n        \"\"\"Open the project's path in the Finder/File Explorer.\"\"\"\n        path = self.project_path\n        self._open_path(path)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.add_analysis_id","title":"<code>add_analysis_id(analysis_id)</code>","text":"<p>Add an analysis to the project.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def add_analysis_id(self, analysis_id: str):\n    \"\"\"Add an analysis to the project.\"\"\"\n    from ResearchOS.PipelineObjects.analysis import Analysis        \n    self._add_target_object_id(analysis_id, cls = Analysis)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.get_analyses","title":"<code>get_analyses()</code>","text":"<p>Return a list of analysis objects in the project.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def get_analyses(self) -&gt; list[\"Analysis\"]:        \n    \"\"\"Return a list of analysis objects in the project.\"\"\"\n    from ResearchOS.PipelineObjects.analysis import Analysis\n    an_ids = self._get_all_target_object_ids(cls = Analysis)\n    return self._gen_obj_or_none(an_ids, Analysis)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.get_users","title":"<code>get_users()</code>","text":"<p>Return a list of user objects that belong to this project. Identical to Dataset.get_users()</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def get_users(self) -&gt; list:\n    \"\"\"Return a list of user objects that belong to this project. Identical to Dataset.get_users()\"\"\"\n    from ResearchOS.user import User\n    us_ids = self._get_all_source_object_ids(cls = User)\n    return self._gen_obj_or_none(us_ids, User)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.open_project_path","title":"<code>open_project_path()</code>","text":"<p>Open the project's path in the Finder/File Explorer.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def open_project_path(self) -&gt; None:\n    \"\"\"Open the project's path in the Finder/File Explorer.\"\"\"\n    path = self.project_path\n    self._open_path(path)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.remove_analysis_id","title":"<code>remove_analysis_id(analysis_id)</code>","text":"<p>Remove an analysis from the project.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def remove_analysis_id(self, analysis_id: str):\n    \"\"\"Remove an analysis from the project.\"\"\"\n    from ResearchOS.PipelineObjects.analysis import Analysis        \n    self._remove_target_object_id(analysis_id, cls = Analysis)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.validate_current_analysis_id","title":"<code>validate_current_analysis_id(id)</code>","text":"<p>Validate the current analysis ID. If it is not valid, the value is rejected.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def validate_current_analysis_id(self, id: str) -&gt; None:\n    \"\"\"Validate the current analysis ID. If it is not valid, the value is rejected.\"\"\"        \n    if not isinstance(id, str):\n        raise ValueError(\"Specified value is not a string!\")\n    if not self.is_id(id):\n        raise ValueError(\"Specified value is not an ID!\")\n    parsed_id = self.parse_id(id)\n    if parsed_id[0] != \"AN\":\n        raise ValueError(\"Specified ID is not an Analysis!\")\n    if not self.object_exists(id):\n        raise ValueError(\"Analysis does not exist!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.validate_current_dataset_id","title":"<code>validate_current_dataset_id(id)</code>","text":"<p>Validate the current dataset ID. If it is not valid, the value is rejected.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def validate_current_dataset_id(self, id: str) -&gt; None:\n    \"\"\"Validate the current dataset ID. If it is not valid, the value is rejected.\"\"\"\n    if not self.is_id(id):\n        raise ValueError(\"Specified value is not an ID!\")\n    parsed_id = self.parse_id(id)\n    if parsed_id[0] != \"DS\":\n        raise ValueError(\"Specified ID is not a Dataset!\")\n    if not self.object_exists(id):\n        raise ValueError(\"Dataset does not exist!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.validate_project_path","title":"<code>validate_project_path(path)</code>","text":"<p>Validate the project path. If it is not valid, the value is rejected.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>def validate_project_path(self, path: str) -&gt; None:\n    \"\"\"Validate the project path. If it is not valid, the value is rejected.\"\"\"\n    # 1. Check that the path exists in the file system.\n    import os\n    if not isinstance(path, str):\n        raise ValueError(\"Specified path is not a string!\")\n    if not os.path.exists(path):\n        raise ValueError(\"Specified path is not a path or does not currently exist!\")        \n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/","title":"Subset","text":"<p>Inherits from PipelineObject</p> <p>             Bases: <code>PipelineObject</code></p> <p>Provides rules to select a subset of data from a dataset.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>class Subset(PipelineObject):\n    \"\"\"Provides rules to select a subset of data from a dataset.\"\"\"\n\n    prefix = \"SS\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the attributes that are required by ResearchOS.\n        Other attributes can be added &amp; modified later.\"\"\"\n        super().__init__(all_default_attrs, **kwargs)\n\n    def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n\n    def load(self) -&gt; None:\n        \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n        PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n\n    def validate_conditions(self, conditions: dict) -&gt; None:\n        \"\"\"Validate the condition recursively.\n        Example usage:\n        conditions = {\n            \"and\": [\n                [vr1.id, \"&lt;\", 4],\n                {\n                    \"or\": [\n                        [vr1.id, \"&gt;\", 2],\n                        [vr1.id, \"=\", 7]\n                    ]\n                }\n            ]\n        }\n        \"\"\"        \n        # Validate a single condition.\n        if isinstance(conditions, list):\n            conn = DBConnectionFactory.create_db_connection().conn\n            if len(conditions) != 3:\n                raise ValueError(\"Condition must be a list of length 3.\")\n            if not IDCreator(conn).is_ro_id(conditions[0]):\n                raise ValueError(\"Variable ID must be a valid Variable ID.\")\n            if conditions[1] not in logic_options:\n                raise ValueError(\"Invalid logic.\")\n            if conditions[1] in numeric_logic_options and not isinstance(conditions[2], int):\n                raise ValueError(\"Numeric logical symbols must have an int value.\")\n            try:\n                a = json.dumps(conditions[2])\n            except:\n                raise ValueError(\"Value must be JSON serializable.\")\n            return\n\n        # Validate the \"and\"/\"or\" keys.\n        if not isinstance(conditions, dict):\n            raise ValueError(\"Condition must be a dict.\")\n        if \"and\" not in conditions and \"or\" not in conditions:\n            raise ValueError(\"Condition must contain an 'and' or 'or' key.\")\n        if \"and\" in conditions and \"or\" in conditions:\n            raise ValueError(\"Condition cannot contain both 'and' and 'or' keys.\")\n\n        for key, value in conditions.items():\n            if key not in (\"and\", \"or\"):\n                raise ValueError(\"Invalid key in condition.\")\n            if not isinstance(value, list):\n                raise ValueError(\"Value must be a list.\")\n            if not isinstance(value, (list, dict)):\n                raise ValueError(\"Value must be a list of lists or dicts.\")\n            a = [self.validate_conditions(cond) for cond in value] # Assigned to a just to make interpreter happy.\n\n\n        # # Check format for \"and\"/\"or\" keys.\n        # if isinstance(conditions, dict):\n        #     if not (\"and\" in conditions or \"or\" in conditions):\n        #         validate_single_condition(conditions)\n        #     if \"and\" in conditions and \"or\" in conditions:\n        #         raise ValueError(\"Condition cannot contain both 'and' and 'or' keys.\")\n        #     if \"and\" in conditions:\n        #         if not isinstance(conditions[\"and\"], list) or not all(isinstance(cond, dict) for cond in conditions[\"and\"]):\n        #             raise ValueError(\"Condition must be a list of dicts.\")\n        #         return all(self.validate_conditions(cond) for cond in conditions[\"and\"])                    \n        #     if \"or\" in conditions:\n        #         if not isinstance(conditions[\"or\"], list) or not all(isinstance(cond, dict) for cond in conditions[\"or\"]):\n        #             raise ValueError(\"Condition must be a list of dicts.\")\n        #         return all(self.validate_conditions(cond) for cond in conditions[\"or\"])\n        # elif isinstance(conditions, list):\n        #     return all(self.validate_conditions(cond) for cond in conditions)\n        # else:\n        #     raise ValueError(\"Condition must be a dict or a list.\")\n\n    # def validate_single_condition(condition: dict) -&gt; None:\n    #     \"\"\"Validate a single condition, inside of an and/or list.\"\"\"\n    #     if not isinstance(condition, dict):\n    #         raise ValueError(\"Condition must be a dict.\")\n    #     if \"VR\" not in condition:\n    #         raise ValueError(\"Condition must contain a 'VR' key.\")\n    #     if not self.is_id(condition[\"VR\"]):\n    #         raise ValueError(\"Variable ID must be a valid Variable ID.\")\n    #     if \"logic\" not in condition.keys():            \n    #         raise ValueError(\"Condition must contain a 'symbol' key.\")\n    #     if condition[\"logic\"] not in logic_options:\n    #         raise ValueError(\"Invalid logic.\")\n    #     if \"value\" not in condition.keys():\n    #         raise ValueError(\"Condition must contain a 'value' key.\")\n    #     if condition[\"logic\"] in numeric_logic_options and not isinstance(condition[\"value\"], int):\n    #         raise ValueError(\"Numeric logical symbols must have an int value.\")\n    #     try:\n    #         a = json.dumps(condition[\"value\"])\n    #     except:\n    #         raise ValueError(\"Value must be JSON serializable.\")\n\n    def get_subset(self) -&gt; dict[dict]:\n        \"\"\"Resolve the conditions to the actual subset of data. Returns a dict of lists of dicts [of lists...].\n        Example usage:\n        subset = {\n            \"DS000000_000\": {\n                \"SJ000000_000\": {\n                    \"VS000000_000\": {\n                        \"PH000000_000\": {},\n                         \"PH000000_001\": {}\n                    }\n                },\n                \"SJ000000_001\": {\n                    \"VS000000_000\": {\n                        \"PH000000_000\": {},\n                         \"PH000000_001\": {}\n                    }\n                }\n            }\n        }\"\"\"\n        ds = Dataset.get_current()\n        schema_id = ds.get_schema_id()\n\n\n    #################### Start class-specific attributes ###################\n    # def add_criteria(self, var_id: str, value, logic: str) -&gt; None:\n    #     \"\"\"Add a criterion to the subset.\n    #     Possible values for logic are: \"&gt;\", \"&lt;\", \"&gt;=\", \"&lt;=\", \"==\", \"!=\", \"in\", \"not in\", \"is\", \"is not\", \"contains\", \"not contains\".\"\"\"\n    #     from ResearchOS import Variable\n    #     logic_options = [\"&gt;\", \"&lt;\", \"&gt;=\", \"&lt;=\", \"==\", \"!=\", \"in\", \"not in\", \"is\", \"is not\", \"contains\", \"not contains\"]\n    #     if logic not in logic_options:\n    #         raise ValueError(\"Invalid logic value.\")\n    #     if var_id not in Variable.get_all_ids():\n    #         raise ValueError(\"Invalid variable ID.\")\n    #     self.criteria.append((var_id, value, logic))\n\n    #################### Start Source objects ####################\n    def get_processes(self) -&gt; list:\n        \"\"\"Return a list of process objects that belong to this subset.\"\"\"\n        pr_ids = self._get_all_source_object_ids(cls = Process)\n        return [Process(id = pr_id) for pr_id in pr_ids]\n\n    def get_plots(self) -&gt; list:\n        \"\"\"Return a list of plot objects that belong to this subset.\"\"\"\n        pl_ids = self._get_all_source_object_ids(cls = Plot)\n        return [Plot(id = pl_id) for pl_id in pl_ids]\n\n    def get_trials(self) -&gt; list:\n        \"\"\"Return a list of trial objects that belong to this subset.\"\"\"\n        tr_ids = self._get_all_source_object_ids(cls = Trial)\n        return [Trial(id = tr_id) for tr_id in tr_ids]\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the attributes that are required by ResearchOS. Other attributes can be added &amp; modified later.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the attributes that are required by ResearchOS.\n    Other attributes can be added &amp; modified later.\"\"\"\n    super().__init__(all_default_attrs, **kwargs)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def __setattr__(self, name: str, value: Any, action: Action = None, validate: bool = True) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    ResearchObjectHandler._setattr_type_specific(self, name, value, action, validate, complex_attrs_list)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.get_plots","title":"<code>get_plots()</code>","text":"<p>Return a list of plot objects that belong to this subset.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def get_plots(self) -&gt; list:\n    \"\"\"Return a list of plot objects that belong to this subset.\"\"\"\n    pl_ids = self._get_all_source_object_ids(cls = Plot)\n    return [Plot(id = pl_id) for pl_id in pl_ids]\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.get_processes","title":"<code>get_processes()</code>","text":"<p>Return a list of process objects that belong to this subset.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def get_processes(self) -&gt; list:\n    \"\"\"Return a list of process objects that belong to this subset.\"\"\"\n    pr_ids = self._get_all_source_object_ids(cls = Process)\n    return [Process(id = pr_id) for pr_id in pr_ids]\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.get_subset","title":"<code>get_subset()</code>","text":"<p>Resolve the conditions to the actual subset of data. Returns a dict of lists of dicts [of lists...]. Example usage: subset = {     \"DS000000_000\": {         \"SJ000000_000\": {             \"VS000000_000\": {                 \"PH000000_000\": {},                  \"PH000000_001\": {}             }         },         \"SJ000000_001\": {             \"VS000000_000\": {                 \"PH000000_000\": {},                  \"PH000000_001\": {}             }         }     } }</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def get_subset(self) -&gt; dict[dict]:\n    \"\"\"Resolve the conditions to the actual subset of data. Returns a dict of lists of dicts [of lists...].\n    Example usage:\n    subset = {\n        \"DS000000_000\": {\n            \"SJ000000_000\": {\n                \"VS000000_000\": {\n                    \"PH000000_000\": {},\n                     \"PH000000_001\": {}\n                }\n            },\n            \"SJ000000_001\": {\n                \"VS000000_000\": {\n                    \"PH000000_000\": {},\n                     \"PH000000_001\": {}\n                }\n            }\n        }\n    }\"\"\"\n    ds = Dataset.get_current()\n    schema_id = ds.get_schema_id()\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.get_trials","title":"<code>get_trials()</code>","text":"<p>Return a list of trial objects that belong to this subset.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def get_trials(self) -&gt; list:\n    \"\"\"Return a list of trial objects that belong to this subset.\"\"\"\n    tr_ids = self._get_all_source_object_ids(cls = Trial)\n    return [Trial(id = tr_id) for tr_id in tr_ids]\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.load","title":"<code>load()</code>","text":"<p>Load the dataset-specific attributes from the database in an attribute-specific way.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the dataset-specific attributes from the database in an attribute-specific way.\"\"\"\n    PipelineObject.load(self) # Load the attributes specific to it being a PipelineObject.\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.validate_conditions","title":"<code>validate_conditions(conditions)</code>","text":"<p>Validate the condition recursively. Example usage: conditions = {     \"and\": [         [vr1.id, \"&lt;\", 4],         {             \"or\": [                 [vr1.id, \"&gt;\", 2],                 [vr1.id, \"=\", 7]             ]         }     ] }</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def validate_conditions(self, conditions: dict) -&gt; None:\n    \"\"\"Validate the condition recursively.\n    Example usage:\n    conditions = {\n        \"and\": [\n            [vr1.id, \"&lt;\", 4],\n            {\n                \"or\": [\n                    [vr1.id, \"&gt;\", 2],\n                    [vr1.id, \"=\", 7]\n                ]\n            }\n        ]\n    }\n    \"\"\"        \n    # Validate a single condition.\n    if isinstance(conditions, list):\n        conn = DBConnectionFactory.create_db_connection().conn\n        if len(conditions) != 3:\n            raise ValueError(\"Condition must be a list of length 3.\")\n        if not IDCreator(conn).is_ro_id(conditions[0]):\n            raise ValueError(\"Variable ID must be a valid Variable ID.\")\n        if conditions[1] not in logic_options:\n            raise ValueError(\"Invalid logic.\")\n        if conditions[1] in numeric_logic_options and not isinstance(conditions[2], int):\n            raise ValueError(\"Numeric logical symbols must have an int value.\")\n        try:\n            a = json.dumps(conditions[2])\n        except:\n            raise ValueError(\"Value must be JSON serializable.\")\n        return\n\n    # Validate the \"and\"/\"or\" keys.\n    if not isinstance(conditions, dict):\n        raise ValueError(\"Condition must be a dict.\")\n    if \"and\" not in conditions and \"or\" not in conditions:\n        raise ValueError(\"Condition must contain an 'and' or 'or' key.\")\n    if \"and\" in conditions and \"or\" in conditions:\n        raise ValueError(\"Condition cannot contain both 'and' and 'or' keys.\")\n\n    for key, value in conditions.items():\n        if key not in (\"and\", \"or\"):\n            raise ValueError(\"Invalid key in condition.\")\n        if not isinstance(value, list):\n            raise ValueError(\"Value must be a list.\")\n        if not isinstance(value, (list, dict)):\n            raise ValueError(\"Value must be a list of lists or dicts.\")\n        a = [self.validate_conditions(cond) for cond in value] # Assigned to a just to make interpreter happy.\n</code></pre>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ResearchOS (pre-alpha version)","text":"<p>This is the documentation for ResearchOS, a Python package for scientific computing.</p>"},{"location":"#install-with-pip","title":"Install with pip","text":"<pre><code>pip install researchos\n</code></pre>"},{"location":"#project-description","title":"Project Description","text":"<p>Scientific computing is currently fractured, with many competing data standards (or lack thereof) and data processing tools that do not have a common way to communicate. ResearchOS provides a generalized framework to perform scientific computing of any kind, in a modular, easily shareable format.</p> <p>The primary innovation behind ResearchOS is to treat every single piece of the scientific data analysis workflow as an object, complete with ID and metadata. While this incurs some code overhead, the ability to have a standardized way to communicate between different parts of a pipeline and to share and integrate others' pipelines is invaluable, and sorely needed in the scientific computing community.</p>"},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"#minimum-todo-to-process-data","title":"Minimum TODO to Process Data","text":"<ul> <li>[ ] Implement Logsheet<ul> <li>[ ] Implement read logsheet.<ul> <li>[ ] Populate the database with the logsheet data.</li> </ul> </li> </ul> </li> <li>[ ] Implement saving participant data to disk/the database.<ul> <li>[ ] Implement data schema for participant data</li> </ul> </li> <li>[ ] Implement Process<ul> <li>[ ] Level</li> <li>[ ] How to structure the Process methods?</li> </ul> </li> <li>[ ] Implement Subset</li> </ul>"},{"location":"#version-01","title":"Version 0.1","text":"<ul> <li>[x] Do multiple things with one Action.</li> <li>[x] Create research objects, save and load them with attributes</li> <li>[x] Create edges between research objects and allow the edges to have their own attributes.</li> <li>[ ] Load and save even complex attributes (e.g. list of dicts) with JSON. Right now I'm just using json.loads()/dumps() but I may need something more sophisticated.</li> <li>[ ] Implement Logsheet<ul> <li>[ ] Implement read logsheet.<ul> <li>[ ] Populate the database with the logsheet data.</li> </ul> </li> </ul> </li> <li>[ ] Implement saving participant data to disk/the database.<ul> <li>[ ] Implement data schema for participant data</li> </ul> </li> <li>[ ] Implement subsets.</li> <li>[ ] Publish my proof of concept to JOSS.</li> </ul>"},{"location":"#version-02","title":"Version 0.2","text":"<ul> <li>[ ] Implement Plots</li> <li>[ ] Implement Stats</li> <li>[ ] Create a graph of research objects and edges</li> <li>[ ] Implement rollback-able version history for research objects</li> <li>[ ] Enhance multi-user support on the same machine.</li> <li>[ ] Look into CI/CD best practices, improve test coverage.</li> <li>[ ] Import/export a ResearchObject for sharing with other users.</li> <li>[ ] Export stats results to LaTeX tables.</li> <li>[ ] Export images to LaTeX figures.<ul> <li>[ ] For images with transparent backgrounds, allow them to be stacked so that multiple can be compared at once.</li> </ul> </li> </ul>"},{"location":"#version-03-and-beyond","title":"Version 0.3 and beyond","text":"<ul> <li>[ ] Implement a MariaDB-based backend for ResearchOS so that it can be used in a multi-user environment.</li> <li>[ ] Implement password-based authentication for the MariaDB backend.</li> <li>[ ] Implement a web-based frontend for ResearchOS.</li> <li>[ ] Get journals on board with ResearchOS so that they can accept ResearchObjects with submissions.</li> <li>[ ] Integrate ResearchOS with participant management systems like RedHat so that people &amp; data are linked.</li> </ul>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#example-1","title":"Example 1","text":"<p>Let's use ResearchOS to create a simple one step pipeline that reads a single number from a text file, squares it, and stores that value.</p> <p>First, after creating a new project directory and activating a virtual environment in that directory, install ResearchOS:</p> <pre><code>pip install researchos\n</code></pre> <p>Next, in the command line, run the following command:</p> <pre><code>python -m researchos quick-start\n</code></pre> <p>This will perform the following actions:</p> <ol> <li> <p>Create a new directory called 'researchos_db' in the current directory.</p> </li> <li> <p>Create a .db file in the 'researchos_db' directory with the proper schema.</p> </li> <li> <p>Create a new Project object in the .db file, and sets it to be the current Project.</p> </li> </ol> <p>Then, create a file called <code>example1.py</code> with the following contents:</p> <pre><code>from researchos.pipeline_objects.project import Project\n</code></pre> <p>This will create a new project.</p>"},{"location":"DiGraph/digraph/","title":"DiGraph","text":""},{"location":"DiGraph/digraph/#introduction","title":"Introduction","text":"<p>The NetworkX MultiDiGraph (directional graph that can have multiple parallel edges) is the data structure that organizes the relationships between all of the different research objects. Just like the objects themselves, the Research Object DiGraph models the relationships between all research objects across all projects. This is especially useful when there are objects that are common to multiple projects - they can be reused! And updates to the object in one project can propagate to other projects, if desired.</p> <p>The Research Object DiGraph consists of both Data Objects and Pipeline Objects - therefore it can become quite large and cumbersome to work with. For example if there are 10 Trial objects (DataObject) each referencing 10 Variable objects (DataObject &amp; PipelineObject), this can quickly become quite large (100 connections in this small example). Often, it is not necessary to have both DataObjects and PipelineObjects in the same graph. Therefore, Data Object DiGraphs and Pipeline Object DiGraphs can be created separately by using <code>data_objects = True</code> and <code>pipeline_objects = True</code> keyword arguments.</p> <p>Subgraphs can also be created by specifying the top level node. For example, to work with just one project's DiGraph, use the <code>source_node = {research_object_id}</code> keyword argument in the constructor, where <code>{research_object_id}</code> is the Project object's ID.</p>"},{"location":"DiGraph/digraph/#note-about-adding-objects-to-the-digraph","title":"Note About Adding Objects to the DiGraph","text":"<p>When adding objects to the DiGraph, they must exist before being added to the DiGraph! In the future the ability to create objects by adding them to the DiGraph may be added, but for now object creation and addition to the DiGraph are two entirely separate steps.</p>"},{"location":"Quick%20Start/quickstart/","title":"Quick Start","text":""},{"location":"Quick%20Start/quickstart/#to-run-quick-start-after-installing","title":"To run Quick Start after installing:","text":"<pre><code>&gt; researchos-quickstart\n</code></pre> <p>That will do the following:</p> <ol> <li> <p>Create a new directory called <code>researchos-quickstart</code>.</p> </li> <li> <p>Create a new quickstart.py file in that directory containing the basic steps for a new Project:</p> </li> </ol>"},{"location":"Quick%20Start/quickstart_py/","title":"quickstart.py","text":""},{"location":"Research%20Object%20Types/research_object/","title":"Research Objects","text":""},{"location":"Research%20Object%20Types/research_object/#overview","title":"Overview","text":"<p>Everything within the ResearchOS framework is a Research Object - at the highest level are User objects, and at the lowest are Variable objects. All Research Objects are stored in the database, and are accessible by using the methods provided in the ResearchOS API.</p> <p>All Research Objects are one or both of the following:</p> <ul> <li> <p>Data Objects - objects that are involved in storing data, such as a Subject or Trial. These objects are typically created by a Process, and are used to store data that is generated by a Process. It is perhaps helpful to recognize that these objects are similar in nature to the \"factors\" of a statistical analysis.</p> </li> <li> <p>Pipeline Objects - objects that are involved in performing data analysis, such as a Process or Project.</p> </li> </ul> <p></p> <p>We will now go over the different methods within the ResearchObject class</p> <p>One research object. Parent class of Data Objects &amp; Pipeline Objects.</p> Source code in <code>src\\ResearchOS\\research_object.py</code> <pre><code>class ResearchObject():\n    \"\"\"One research object. Parent class of Data Objects &amp; Pipeline Objects.\"\"\"\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __eq__(self, other):\n        if isinstance(other, ResearchObject):\n            return self.id == other.id and self is other\n        return False\n\n    def __getattribute__(self, name: str) -&gt; Any:\n        \"\"\"Get the value of an attribute. Only does any magic if the attribute exists already and is a VR.\"\"\"        \n        subclasses = ResearchObject.__subclasses__()\n        vr_class = [x for x in subclasses if (hasattr(x,\"prefix\") and x.prefix == \"VR\")][0]\n        try:\n            value = super().__getattribute__(name) # Throw the default error.\n        except AttributeError as e:\n            raise e        \n        if isinstance(value, vr_class):\n            value = ResearchObjectHandler.load_vr_value(self, value)\n        return value\n\n    def __setattr__(self, name, value, action: Action = None, validate: bool = True, all_attrs: DefaultAttrs = None) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n        if hasattr(self, name) and getattr(self, name) == value:\n            return # No change.\n\n        # Ensure that the criteria to set the attribute are met.\n        if not str(name).isidentifier():\n            raise ValueError(f\"{name} is not a valid attribute name.\") # Offers some protection for having to eval() the name to get custom function names.        \n        if name == \"id\":\n            raise ValueError(\"Cannot change the ID of a research object.\")\n        if name == \"prefix\":\n            raise ValueError(\"Cannot change the prefix of a research object.\")\n        if name == \"name\":\n            if not str(value).isidentifier():\n                raise ValueError(f\"name attribute, value: {value} is not a valid attribute name.\") \n\n        # Set the attribute.\n        if all_attrs is None:\n            all_attrs = DefaultAttrs(self)\n        if action is None:            \n            action = Action(name = \"attribute_changed\")\n        ResearchObjectHandler._setattr(self, name, value, action, validate, all_attrs.default_attrs, all_attrs.complex_attrs)\n        if action.do_exec:        \n            action.execute()\n\n    def __new__(cls, **kwargs):\n        \"\"\"Create a new research object in memory. If the object already exists in memory with this ID, return the existing object.\"\"\"\n        kwargs = ResearchObjectHandler.check_inputs(cls, kwargs)\n        id = kwargs[\"id\"]\n        # keys = \",\".join([key for key in ResearchObjectHandler.instances.keys()])\n        # print(\"Keys: \" + keys)\n        if id in ResearchObjectHandler.instances:\n            ResearchObjectHandler.counts[id] += 1\n            ResearchObjectHandler.instances[id].__dict__[\"prev_loaded\"] = True\n            return ResearchObjectHandler.instances[id]\n        ResearchObjectHandler.counts[id] = 1\n        instance = super(ResearchObject, cls).__new__(cls)\n        ResearchObjectHandler.instances[id] = instance\n        ResearchObjectHandler.instances[id].__dict__[\"prev_loaded\"] = False\n        instance.__dict__[\"prev_loaded\"] = False        \n        return instance    \n\n    def __init__(self, **orig_kwargs):\n        \"\"\"Initialize the research object.\"\"\"   \n        self.__dict__[\"id\"] = orig_kwargs[\"id\"] # Put the ID in the __dict__ so that it is not overwritten by the __setattr__ method.\n        del orig_kwargs[\"id\"] # Remove the ID from the kwargs so that it is not set as an attribute.        \n        attrs = DefaultAttrs(self) # Get the default attributes for the class.\n        default_attrs = attrs.default_attrs\n\n        # Will be overwritten if creating a new object.\n        action = Action(name = f\"set object attributes\")\n        kwargs = orig_kwargs # Because the defaults will have all been set, don't include them.\n        prev_exists = ResearchObjectHandler.object_exists(self.id)\n        if not self.prev_loaded and prev_exists:\n            # Load the existing object's attributes from the database.\n            ResearchObjectHandler._load_ro(self, default_attrs)\n        elif not prev_exists:\n            # Create a new object.\n            action = Action(name = f\"created object\")\n            ResearchObjectHandler._create_ro(self, action = action) # Create the object in the database.\n            kwargs = default_attrs | orig_kwargs # Set defaults, but allow them to be overwritten by the kwargs.\n        del self.__dict__[\"prev_loaded\"] # Remove the prev_loaded attribute from the object.\n\n        # Set the attributes.\n        for key in kwargs:\n            validate = True # Default is to validate any attribute.        \n            # If previously loaded, don't overwrite a default attribute with its default value. If it was specified as a kwarg, then use that specified value.\n            if key in self.__dict__ and key not in orig_kwargs:\n                continue\n            # If the attribute value is a default value, don't validate it.\n            if key in default_attrs and kwargs[key] == default_attrs[key]:\n                validate = False\n            self.__setattr__(key, kwargs[key], action = action, validate = validate, all_attrs = attrs)\n            action.execute() # Commit the action to the database.    \n\n    def get_vr(self, name: str) -&gt; Any:\n        \"\"\"Get the VR itself instead of its value.\"\"\"\n        return self.__dict__[name]\n\n    def get_dataset_id(self) -&gt; str:\n        \"\"\"Get the most recent dataset ID.\"\"\"        \n        sqlquery = f\"SELECT action_id, dataset_id FROM data_address_schemas\"\n        pool = SQLiteConnectionPool()\n        conn = pool.get_connection()\n        cursor = conn.cursor()\n        result = cursor.execute(sqlquery).fetchall()\n        pool.return_connection(conn)\n        # ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num=0)\n        if not result:\n            raise ValueError(\"Need to create a dataset and set up its schema first.\")\n        dataset_id = result[-1][1]        \n        return dataset_id\n\n    def get_current_schema_id(self, dataset_id: str) -&gt; str:\n        conn = ResearchObjectHandler.pool.get_connection()\n        sqlquery = f\"SELECT action_id FROM data_address_schemas WHERE dataset_id = '{dataset_id}'\"\n        action_ids = conn.cursor().execute(sqlquery).fetchall()\n        action_ids = ResearchObjectHandler._get_time_ordered_result(action_ids, action_col_num=0)\n        action_id_schema = action_ids[0][0] if action_ids else None\n        if action_id_schema is None:\n            ResearchObjectHandler.pool.return_connection(conn)\n            return # If the schema is empty and the addresses are empty, this is likely initialization so just return.\n\n        sqlquery = f\"SELECT schema_id FROM data_address_schemas WHERE dataset_id = '{dataset_id}' AND action_id = '{action_id_schema}'\"\n        schema_id = conn.execute(sqlquery).fetchone()\n        schema_id = schema_id[0] if schema_id else None\n        ResearchObjectHandler.pool.return_connection(conn)\n        return schema_id\n</code></pre>"},{"location":"Research%20Object%20Types/research_object/#src.ResearchOS.research_object.ResearchObject.__getattribute__","title":"<code>__getattribute__(name)</code>","text":"<p>Get the value of an attribute. Only does any magic if the attribute exists already and is a VR.</p> Source code in <code>src\\ResearchOS\\research_object.py</code> <pre><code>def __getattribute__(self, name: str) -&gt; Any:\n    \"\"\"Get the value of an attribute. Only does any magic if the attribute exists already and is a VR.\"\"\"        \n    subclasses = ResearchObject.__subclasses__()\n    vr_class = [x for x in subclasses if (hasattr(x,\"prefix\") and x.prefix == \"VR\")][0]\n    try:\n        value = super().__getattribute__(name) # Throw the default error.\n    except AttributeError as e:\n        raise e        \n    if isinstance(value, vr_class):\n        value = ResearchObjectHandler.load_vr_value(self, value)\n    return value\n</code></pre>"},{"location":"Research%20Object%20Types/research_object/#src.ResearchOS.research_object.ResearchObject.__init__","title":"<code>__init__(**orig_kwargs)</code>","text":"<p>Initialize the research object.</p> Source code in <code>src\\ResearchOS\\research_object.py</code> <pre><code>def __init__(self, **orig_kwargs):\n    \"\"\"Initialize the research object.\"\"\"   \n    self.__dict__[\"id\"] = orig_kwargs[\"id\"] # Put the ID in the __dict__ so that it is not overwritten by the __setattr__ method.\n    del orig_kwargs[\"id\"] # Remove the ID from the kwargs so that it is not set as an attribute.        \n    attrs = DefaultAttrs(self) # Get the default attributes for the class.\n    default_attrs = attrs.default_attrs\n\n    # Will be overwritten if creating a new object.\n    action = Action(name = f\"set object attributes\")\n    kwargs = orig_kwargs # Because the defaults will have all been set, don't include them.\n    prev_exists = ResearchObjectHandler.object_exists(self.id)\n    if not self.prev_loaded and prev_exists:\n        # Load the existing object's attributes from the database.\n        ResearchObjectHandler._load_ro(self, default_attrs)\n    elif not prev_exists:\n        # Create a new object.\n        action = Action(name = f\"created object\")\n        ResearchObjectHandler._create_ro(self, action = action) # Create the object in the database.\n        kwargs = default_attrs | orig_kwargs # Set defaults, but allow them to be overwritten by the kwargs.\n    del self.__dict__[\"prev_loaded\"] # Remove the prev_loaded attribute from the object.\n\n    # Set the attributes.\n    for key in kwargs:\n        validate = True # Default is to validate any attribute.        \n        # If previously loaded, don't overwrite a default attribute with its default value. If it was specified as a kwarg, then use that specified value.\n        if key in self.__dict__ and key not in orig_kwargs:\n            continue\n        # If the attribute value is a default value, don't validate it.\n        if key in default_attrs and kwargs[key] == default_attrs[key]:\n            validate = False\n        self.__setattr__(key, kwargs[key], action = action, validate = validate, all_attrs = attrs)\n        action.execute() # Commit the action to the database.    \n</code></pre>"},{"location":"Research%20Object%20Types/research_object/#src.ResearchOS.research_object.ResearchObject.__new__","title":"<code>__new__(**kwargs)</code>","text":"<p>Create a new research object in memory. If the object already exists in memory with this ID, return the existing object.</p> Source code in <code>src\\ResearchOS\\research_object.py</code> <pre><code>def __new__(cls, **kwargs):\n    \"\"\"Create a new research object in memory. If the object already exists in memory with this ID, return the existing object.\"\"\"\n    kwargs = ResearchObjectHandler.check_inputs(cls, kwargs)\n    id = kwargs[\"id\"]\n    # keys = \",\".join([key for key in ResearchObjectHandler.instances.keys()])\n    # print(\"Keys: \" + keys)\n    if id in ResearchObjectHandler.instances:\n        ResearchObjectHandler.counts[id] += 1\n        ResearchObjectHandler.instances[id].__dict__[\"prev_loaded\"] = True\n        return ResearchObjectHandler.instances[id]\n    ResearchObjectHandler.counts[id] = 1\n    instance = super(ResearchObject, cls).__new__(cls)\n    ResearchObjectHandler.instances[id] = instance\n    ResearchObjectHandler.instances[id].__dict__[\"prev_loaded\"] = False\n    instance.__dict__[\"prev_loaded\"] = False        \n    return instance    \n</code></pre>"},{"location":"Research%20Object%20Types/research_object/#src.ResearchOS.research_object.ResearchObject.__setattr__","title":"<code>__setattr__(name, value, action=None, validate=True, all_attrs=None)</code>","text":"<p>Set the attribute value. If the attribute value is not valid, an error is thrown.</p> Source code in <code>src\\ResearchOS\\research_object.py</code> <pre><code>def __setattr__(self, name, value, action: Action = None, validate: bool = True, all_attrs: DefaultAttrs = None) -&gt; None:\n    \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"\n    if hasattr(self, name) and getattr(self, name) == value:\n        return # No change.\n\n    # Ensure that the criteria to set the attribute are met.\n    if not str(name).isidentifier():\n        raise ValueError(f\"{name} is not a valid attribute name.\") # Offers some protection for having to eval() the name to get custom function names.        \n    if name == \"id\":\n        raise ValueError(\"Cannot change the ID of a research object.\")\n    if name == \"prefix\":\n        raise ValueError(\"Cannot change the prefix of a research object.\")\n    if name == \"name\":\n        if not str(value).isidentifier():\n            raise ValueError(f\"name attribute, value: {value} is not a valid attribute name.\") \n\n    # Set the attribute.\n    if all_attrs is None:\n        all_attrs = DefaultAttrs(self)\n    if action is None:            \n        action = Action(name = \"attribute_changed\")\n    ResearchObjectHandler._setattr(self, name, value, action, validate, all_attrs.default_attrs, all_attrs.complex_attrs)\n    if action.do_exec:        \n        action.execute()\n</code></pre>"},{"location":"Research%20Object%20Types/research_object/#src.ResearchOS.research_object.ResearchObject.get_dataset_id","title":"<code>get_dataset_id()</code>","text":"<p>Get the most recent dataset ID.</p> Source code in <code>src\\ResearchOS\\research_object.py</code> <pre><code>def get_dataset_id(self) -&gt; str:\n    \"\"\"Get the most recent dataset ID.\"\"\"        \n    sqlquery = f\"SELECT action_id, dataset_id FROM data_address_schemas\"\n    pool = SQLiteConnectionPool()\n    conn = pool.get_connection()\n    cursor = conn.cursor()\n    result = cursor.execute(sqlquery).fetchall()\n    pool.return_connection(conn)\n    # ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num=0)\n    if not result:\n        raise ValueError(\"Need to create a dataset and set up its schema first.\")\n    dataset_id = result[-1][1]        \n    return dataset_id\n</code></pre>"},{"location":"Research%20Object%20Types/research_object/#src.ResearchOS.research_object.ResearchObject.get_vr","title":"<code>get_vr(name)</code>","text":"<p>Get the VR itself instead of its value.</p> Source code in <code>src\\ResearchOS\\research_object.py</code> <pre><code>def get_vr(self, name: str) -&gt; Any:\n    \"\"\"Get the VR itself instead of its value.\"\"\"\n    return self.__dict__[name]\n</code></pre>"},{"location":"Research%20Object%20Types/variable/","title":"Variable","text":"<p>             Bases: <code>ResearchObject</code></p> <p>Variable class.</p> Source code in <code>src\\ResearchOS\\variable.py</code> <pre><code>class Variable(ResearchObject):\n    \"\"\"Variable class.\"\"\"\n\n    prefix: str = \"VR\"\n\n    ## Level methods\n\n    def validate_level(self, level: type) -&gt; None:\n        \"\"\"Check that the level is of a valid type.\"\"\"\n        from ResearchOS.DataObjects.data_object import DataObject\n        if not isinstance(level, type):\n            raise ValueError(\"Level must be a type.\")\n        if level not in DataObject.__subclasses__():\n            raise ValueError(\"Level must be a DataObject.\")\n\n    def to_json_level(self, level: type) -&gt; dict:\n        \"\"\"Return the level as a JSON object.\"\"\"\n        if level is None:\n            return json.dumps(level)\n        return json.dumps(level.prefix)\n\n    def from_json_level(self, level: str) -&gt; type:\n        \"\"\"Return the level as a JSON object.\"\"\"\n        from ResearchOS.DataObjects.data_object import DataObject\n        level_str = json.loads(level)\n        if level_str is None:\n            return level_str\n        subclasses = DataObject.__subclasses__()\n        return [cls for cls in subclasses if cls.prefix == level_str][0]\n</code></pre>"},{"location":"Research%20Object%20Types/variable/#src.ResearchOS.variable.Variable.from_json_level","title":"<code>from_json_level(level)</code>","text":"<p>Return the level as a JSON object.</p> Source code in <code>src\\ResearchOS\\variable.py</code> <pre><code>def from_json_level(self, level: str) -&gt; type:\n    \"\"\"Return the level as a JSON object.\"\"\"\n    from ResearchOS.DataObjects.data_object import DataObject\n    level_str = json.loads(level)\n    if level_str is None:\n        return level_str\n    subclasses = DataObject.__subclasses__()\n    return [cls for cls in subclasses if cls.prefix == level_str][0]\n</code></pre>"},{"location":"Research%20Object%20Types/variable/#src.ResearchOS.variable.Variable.to_json_level","title":"<code>to_json_level(level)</code>","text":"<p>Return the level as a JSON object.</p> Source code in <code>src\\ResearchOS\\variable.py</code> <pre><code>def to_json_level(self, level: type) -&gt; dict:\n    \"\"\"Return the level as a JSON object.\"\"\"\n    if level is None:\n        return json.dumps(level)\n    return json.dumps(level.prefix)\n</code></pre>"},{"location":"Research%20Object%20Types/variable/#src.ResearchOS.variable.Variable.validate_level","title":"<code>validate_level(level)</code>","text":"<p>Check that the level is of a valid type.</p> Source code in <code>src\\ResearchOS\\variable.py</code> <pre><code>def validate_level(self, level: type) -&gt; None:\n    \"\"\"Check that the level is of a valid type.\"\"\"\n    from ResearchOS.DataObjects.data_object import DataObject\n    if not isinstance(level, type):\n        raise ValueError(\"Level must be a type.\")\n    if level not in DataObject.__subclasses__():\n        raise ValueError(\"Level must be a DataObject.\")\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/","title":"Data Objects","text":"<p>             Bases: <code>ResearchObject</code></p> <p>The parent class for all data objects. Data objects represent some form of data storage, and approximately map to statistical factors.</p> Source code in <code>src\\ResearchOS\\DataObjects\\data_object.py</code> <pre><code>class DataObject(ResearchObject):\n    \"\"\"The parent class for all data objects. Data objects represent some form of data storage, and approximately map to statistical factors.\"\"\"    \n\n    def __delattr__(self, name: str, action: Action = None) -&gt; None:\n        \"\"\"Delete an attribute. If it's a builtin attribute, don't delete it.\n        If it's a VR, make sure it's \"deleted\" from the database.\"\"\"\n        default_attrs = DefaultAttrs(self).default_attrs\n        if name in default_attrs:\n            raise AttributeError(\"Cannot delete a builtin attribute.\")\n        if name not in self.__dict__:\n            raise AttributeError(\"No such attribute.\")\n        if action is None:\n            action = Action(name = \"delete_attribute\")\n        vr_id = self.__dict__[name].id\n        sqlquery = f\"INSERT INTO vr_dataobjects (action_id, dataobject_id, vr_id, is_active) VALUES ('{action.id}', '{self.id}', '{vr_id}', 0)\"\n        if action is None:\n            action = Action(name = \"delete_attribute\")\n        action.add_sql_query(sqlquery)\n        action.execute()\n        del self.__dict__[name]\n\n    def load_dataobject_vrs(self) -&gt; None:\n        \"\"\"Load data values from the database.\"\"\"\n        # 1. Get all of the latest address_id &amp; vr_id combinations (that have not been overwritten) for the current schema for the current database.\n        # Get the schema_id.\n        # TODO: Put the schema_id into the data_values table.\n        # 1. Get all of the VRs for the current object.\n        from ResearchOS.variable import Variable        \n\n        sqlquery = f\"SELECT vr_id FROM vr_dataobjects WHERE dataobject_id = '{self.id}' AND is_active = 1\"\n        conn = ResearchObjectHandler.pool.get_connection()\n        cursor = conn.cursor()\n        vr_ids = cursor.execute(sqlquery).fetchall()\n        vr_ids = [x[0] for x in vr_ids]\n        ResearchObjectHandler.pool.return_connection(conn)\n        for vr_id in vr_ids:\n            vr = Variable(id = vr_id)\n            self.__dict__[vr.name] = vr\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.__delattr__","title":"<code>__delattr__(name, action=None)</code>","text":"<p>Delete an attribute. If it's a builtin attribute, don't delete it. If it's a VR, make sure it's \"deleted\" from the database.</p> Source code in <code>src\\ResearchOS\\DataObjects\\data_object.py</code> <pre><code>def __delattr__(self, name: str, action: Action = None) -&gt; None:\n    \"\"\"Delete an attribute. If it's a builtin attribute, don't delete it.\n    If it's a VR, make sure it's \"deleted\" from the database.\"\"\"\n    default_attrs = DefaultAttrs(self).default_attrs\n    if name in default_attrs:\n        raise AttributeError(\"Cannot delete a builtin attribute.\")\n    if name not in self.__dict__:\n        raise AttributeError(\"No such attribute.\")\n    if action is None:\n        action = Action(name = \"delete_attribute\")\n    vr_id = self.__dict__[name].id\n    sqlquery = f\"INSERT INTO vr_dataobjects (action_id, dataobject_id, vr_id, is_active) VALUES ('{action.id}', '{self.id}', '{vr_id}', 0)\"\n    if action is None:\n        action = Action(name = \"delete_attribute\")\n    action.add_sql_query(sqlquery)\n    action.execute()\n    del self.__dict__[name]\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/data_object/#src.ResearchOS.DataObjects.data_object.DataObject.load_dataobject_vrs","title":"<code>load_dataobject_vrs()</code>","text":"<p>Load data values from the database.</p> Source code in <code>src\\ResearchOS\\DataObjects\\data_object.py</code> <pre><code>def load_dataobject_vrs(self) -&gt; None:\n    \"\"\"Load data values from the database.\"\"\"\n    # 1. Get all of the latest address_id &amp; vr_id combinations (that have not been overwritten) for the current schema for the current database.\n    # Get the schema_id.\n    # TODO: Put the schema_id into the data_values table.\n    # 1. Get all of the VRs for the current object.\n    from ResearchOS.variable import Variable        \n\n    sqlquery = f\"SELECT vr_id FROM vr_dataobjects WHERE dataobject_id = '{self.id}' AND is_active = 1\"\n    conn = ResearchObjectHandler.pool.get_connection()\n    cursor = conn.cursor()\n    vr_ids = cursor.execute(sqlquery).fetchall()\n    vr_ids = [x[0] for x in vr_ids]\n    ResearchObjectHandler.pool.return_connection(conn)\n    for vr_id in vr_ids:\n        vr = Variable(id = vr_id)\n        self.__dict__[vr.name] = vr\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/","title":"Dataset","text":"<p>Inherits from DataObject</p> <p>             Bases: <code>DataObject</code></p> <p>A dataset is one set of data. Class-specific Attributes: 1. data path: The root folder location of the dataset. 2. data schema: The schema of the dataset (specified as a list of classes)</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>class Dataset(DataObject):\n    \"\"\"A dataset is one set of data.\n    Class-specific Attributes:\n    1. data path: The root folder location of the dataset.\n    2. data schema: The schema of the dataset (specified as a list of classes)\"\"\"\n\n    prefix: str = \"DS\"\n\n    ### Schema Methods\n\n    def validate_schema(self, schema: list) -&gt; None:\n        \"\"\"Validate that the data schema follows the proper format.\n        Must be a dict of dicts, where all keys are Python types matching a DataObject subclass, and the lowest levels are empty.\"\"\"\n        from ResearchOS.research_object import ResearchObject\n        subclasses = ResearchObject.__subclasses__()\n        dataobj_subclasses = DataObject.__subclasses__()\n        vr = [x for x in subclasses if hasattr(x,\"prefix\") and x.prefix == \"VR\"][0]                \n\n        graph = nx.MultiDiGraph()\n        try:\n            graph.add_edges_from(schema)\n        except nx.NetworkXError:\n            raise ValueError(\"The schema must be provided as an edge list!\")\n        if not nx.is_directed_acyclic_graph(graph):\n            raise ValueError(\"The schema must be a directed acyclic graph!\")\n\n        non_subclass = [node for node in graph if node not in dataobj_subclasses]\n        if non_subclass:\n            raise ValueError(\"The schema must only include DataObject subclasses!\")\n\n        if Dataset not in graph:\n            raise ValueError(\"The schema must include the Dataset class as a source node!\")\n\n        if vr in graph:\n            raise ValueError(\"The schema must not include the Variable class as a target node!\")\n\n        # nodes_with_no_targets = [node for node, out_degree in graph.out_degree() if out_degree == 0]\n        # nodes_with_a_source = [node for node, in_degree in graph.in_degree() if in_degree &gt; 0]\n        # if graph[Dataset] in nodes_with_no_targets or graph[Dataset] in nodes_with_a_source:\n        #     raise ValueError(\"The schema must include the Dataset class as a source node and not a target node!\")\n\n    def save_schema(self, schema: list, action: Action) -&gt; None:\n        \"\"\"Save the schema to the database.\"\"\"\n        # 1. Convert the list of types to a list of str.\n        str_schema = []\n        for sch in schema:\n            classes = []\n            for cls in sch:\n                classes.append(cls.prefix)\n            str_schema.append(classes)\n        # 2. Convert the list of str to a json string.\n        json_schema = json.dumps(str_schema)\n\n        # 3. Save the schema to the database.        \n        schema_id = IDCreator().create_action_id()\n        sqlquery = f\"INSERT INTO data_address_schemas (schema_id, levels_edge_list, dataset_id, action_id) VALUES ('{schema_id}', '{json_schema}', '{self.id}', '{action.id}')\"\n        action.add_sql_query(sqlquery)\n\n    def load_schema(self) -&gt; None:\n        \"\"\"Load the schema from the database and convert it via json.\"\"\"\n        # 1. Get the dataset ID\n        id = self.id\n        # 2. Get the most recent action ID for the dataset in the data_address_schemas table.\n        schema_id = self.get_current_schema_id(id)\n        sqlquery = f\"SELECT levels_edge_list FROM data_address_schemas WHERE schema_id = '{schema_id}'\"\n        conn = ResearchObjectHandler.pool.get_connection()\n        result = conn.execute(sqlquery).fetchone()\n\n        # 5. If the schema is not None, convert the string to a list of types.\n        str_schema = json.loads(result[0])\n        schema = []\n        for sch in str_schema:\n            for idx, prefix in enumerate(sch):\n                sch[idx] = ResearchObjectHandler._prefix_to_class(prefix)\n            schema.append(sch)  \n\n        # 6. Store the schema as an attribute of the dataset.\n        self.__dict__[\"schema\"] = schema\n\n    ### Dataset path methods\n\n    def validate_dataset_path(self, path: str) -&gt; None:\n        \"\"\"Validate the dataset path.\"\"\"\n        import os\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path is not a path or does not currently exist!\")\n\n    ### Address Methods\n\n    def validate_addresses(self, addresses: list) -&gt; None:\n        \"\"\"Validate that the addresses are in the correct format.\"\"\"\n        self.validate_schema(self.schema)   \n\n        try:\n            graph = nx.MultiDiGraph()\n            graph.add_edges_from(addresses)\n        except nx.NetworkXError:\n            raise ValueError(\"The addresses must be provided as an edge list!\")\n\n        if not nx.is_directed_acyclic_graph(graph):\n            raise ValueError(\"The addresses must be a directed acyclic graph!\")\n\n        non_ro_id = [node for node in graph if not IDCreator().is_ro_id(node)]\n        if non_ro_id:\n            raise ValueError(\"The addresses must only include ResearchObject ID's!\")\n\n        if not graph[self.id]:\n            raise ValueError(\"The addresses must include the dataset ID!\")\n\n        vrs = [node for node in graph if node.startswith(\"VR\")]\n        if vrs:\n            raise ValueError(\"The addresses must not include Variable ID's!\")\n\n        schema = self.schema\n        schema_graph = nx.MultiDiGraph()\n        schema_graph.add_edges_from(schema)\n        for address_edge in addresses:\n            cls0 = ResearchObjectHandler._prefix_to_class(address_edge[0])\n            cls1 = ResearchObjectHandler._prefix_to_class(address_edge[1])\n            if cls0 not in schema_graph.predecessors(cls1) or cls1 not in schema_graph.successors(cls0):\n                raise ValueError(\"The addresses must match the schema!\")\n\n    def save_addresses(self, addresses: list, action: Action) -&gt; None:\n        \"\"\"Save the addresses to the data_addresses table in the database.\"\"\"        \n        # 1. Get the schema_id for the current dataset_id that has not been overwritten by an Action.       \n        dataset_id = self.id\n        schema_id = self.get_current_schema_id(dataset_id)\n        for address_names in addresses:   \n            sqlquery = f\"INSERT INTO data_addresses (target_object_id, source_object_id, schema_id, action_id) VALUES ('{address_names[0]}', '{address_names[1]}', '{schema_id}', '{action.id}')\"\n            action.add_sql_query(sqlquery)\n        self.__dict__[\"address_graph\"] = self.addresses_to_graph(addresses)\n        self.__dict__[\"addresses\"] = addresses\n\n    def load_addresses(self) -&gt; list:\n        \"\"\"Load the addresses from the database.\"\"\"\n        pool = SQLiteConnectionPool()        \n        schema_id = self.get_current_schema_id(self.id)\n        conn = pool.get_connection()\n\n        # 2. Get the addresses for the current schema_id.\n        sqlquery = f\"SELECT target_object_id, source_object_id FROM data_addresses WHERE schema_id = '{schema_id}'\"\n        addresses = conn.execute(sqlquery).fetchall()\n        pool.return_connection(conn)\n\n        # 3. Convert the addresses to a list of lists.\n        addresses = [list(address) for address in addresses]        \n\n        self.__dict__[\"addresses\"] = addresses\n        self.__dict__[\"address_graph\"] = self.addresses_to_graph(addresses)\n\n    def addresses_to_graph(self, addresses: list) -&gt; nx.MultiDiGraph:\n        \"\"\"Convert the addresses edge list to a MultiDiGraph.\"\"\"\n        G = nx.MultiDiGraph()\n        # To avoid recursion, set the lines with Dataset manually so there is no self reference.\n        address_copy = copy.deepcopy(addresses)\n        for idx, address in enumerate(addresses):\n            if address[0] == self.id: # Include the Dataset as the source node.\n                cls1 = ResearchObjectHandler._prefix_to_class(address[1])\n                G.add_edge(self, cls1(id = address[1]))\n                address_copy.remove(address)\n\n        addresses = address_copy\n        subclasses = DataObject.__subclasses__()\n        cls_dict = {cls.prefix: cls for cls in subclasses}\n        idcreator = IDCreator()\n        for address_edge in addresses:            \n            cls0 = cls_dict[idcreator.get_prefix(address_edge[0])]\n            cls1 = cls_dict[idcreator.get_prefix(address_edge[1])]\n            G.add_edge(cls0(id = address_edge[0]), cls1(id = address_edge[1]))\n        return G\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.addresses_to_graph","title":"<code>addresses_to_graph(addresses)</code>","text":"<p>Convert the addresses edge list to a MultiDiGraph.</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>def addresses_to_graph(self, addresses: list) -&gt; nx.MultiDiGraph:\n    \"\"\"Convert the addresses edge list to a MultiDiGraph.\"\"\"\n    G = nx.MultiDiGraph()\n    # To avoid recursion, set the lines with Dataset manually so there is no self reference.\n    address_copy = copy.deepcopy(addresses)\n    for idx, address in enumerate(addresses):\n        if address[0] == self.id: # Include the Dataset as the source node.\n            cls1 = ResearchObjectHandler._prefix_to_class(address[1])\n            G.add_edge(self, cls1(id = address[1]))\n            address_copy.remove(address)\n\n    addresses = address_copy\n    subclasses = DataObject.__subclasses__()\n    cls_dict = {cls.prefix: cls for cls in subclasses}\n    idcreator = IDCreator()\n    for address_edge in addresses:            \n        cls0 = cls_dict[idcreator.get_prefix(address_edge[0])]\n        cls1 = cls_dict[idcreator.get_prefix(address_edge[1])]\n        G.add_edge(cls0(id = address_edge[0]), cls1(id = address_edge[1]))\n    return G\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.load_addresses","title":"<code>load_addresses()</code>","text":"<p>Load the addresses from the database.</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>def load_addresses(self) -&gt; list:\n    \"\"\"Load the addresses from the database.\"\"\"\n    pool = SQLiteConnectionPool()        \n    schema_id = self.get_current_schema_id(self.id)\n    conn = pool.get_connection()\n\n    # 2. Get the addresses for the current schema_id.\n    sqlquery = f\"SELECT target_object_id, source_object_id FROM data_addresses WHERE schema_id = '{schema_id}'\"\n    addresses = conn.execute(sqlquery).fetchall()\n    pool.return_connection(conn)\n\n    # 3. Convert the addresses to a list of lists.\n    addresses = [list(address) for address in addresses]        \n\n    self.__dict__[\"addresses\"] = addresses\n    self.__dict__[\"address_graph\"] = self.addresses_to_graph(addresses)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.load_schema","title":"<code>load_schema()</code>","text":"<p>Load the schema from the database and convert it via json.</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>def load_schema(self) -&gt; None:\n    \"\"\"Load the schema from the database and convert it via json.\"\"\"\n    # 1. Get the dataset ID\n    id = self.id\n    # 2. Get the most recent action ID for the dataset in the data_address_schemas table.\n    schema_id = self.get_current_schema_id(id)\n    sqlquery = f\"SELECT levels_edge_list FROM data_address_schemas WHERE schema_id = '{schema_id}'\"\n    conn = ResearchObjectHandler.pool.get_connection()\n    result = conn.execute(sqlquery).fetchone()\n\n    # 5. If the schema is not None, convert the string to a list of types.\n    str_schema = json.loads(result[0])\n    schema = []\n    for sch in str_schema:\n        for idx, prefix in enumerate(sch):\n            sch[idx] = ResearchObjectHandler._prefix_to_class(prefix)\n        schema.append(sch)  \n\n    # 6. Store the schema as an attribute of the dataset.\n    self.__dict__[\"schema\"] = schema\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.save_addresses","title":"<code>save_addresses(addresses, action)</code>","text":"<p>Save the addresses to the data_addresses table in the database.</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>def save_addresses(self, addresses: list, action: Action) -&gt; None:\n    \"\"\"Save the addresses to the data_addresses table in the database.\"\"\"        \n    # 1. Get the schema_id for the current dataset_id that has not been overwritten by an Action.       \n    dataset_id = self.id\n    schema_id = self.get_current_schema_id(dataset_id)\n    for address_names in addresses:   \n        sqlquery = f\"INSERT INTO data_addresses (target_object_id, source_object_id, schema_id, action_id) VALUES ('{address_names[0]}', '{address_names[1]}', '{schema_id}', '{action.id}')\"\n        action.add_sql_query(sqlquery)\n    self.__dict__[\"address_graph\"] = self.addresses_to_graph(addresses)\n    self.__dict__[\"addresses\"] = addresses\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.save_schema","title":"<code>save_schema(schema, action)</code>","text":"<p>Save the schema to the database.</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>def save_schema(self, schema: list, action: Action) -&gt; None:\n    \"\"\"Save the schema to the database.\"\"\"\n    # 1. Convert the list of types to a list of str.\n    str_schema = []\n    for sch in schema:\n        classes = []\n        for cls in sch:\n            classes.append(cls.prefix)\n        str_schema.append(classes)\n    # 2. Convert the list of str to a json string.\n    json_schema = json.dumps(str_schema)\n\n    # 3. Save the schema to the database.        \n    schema_id = IDCreator().create_action_id()\n    sqlquery = f\"INSERT INTO data_address_schemas (schema_id, levels_edge_list, dataset_id, action_id) VALUES ('{schema_id}', '{json_schema}', '{self.id}', '{action.id}')\"\n    action.add_sql_query(sqlquery)\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.validate_addresses","title":"<code>validate_addresses(addresses)</code>","text":"<p>Validate that the addresses are in the correct format.</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>def validate_addresses(self, addresses: list) -&gt; None:\n    \"\"\"Validate that the addresses are in the correct format.\"\"\"\n    self.validate_schema(self.schema)   \n\n    try:\n        graph = nx.MultiDiGraph()\n        graph.add_edges_from(addresses)\n    except nx.NetworkXError:\n        raise ValueError(\"The addresses must be provided as an edge list!\")\n\n    if not nx.is_directed_acyclic_graph(graph):\n        raise ValueError(\"The addresses must be a directed acyclic graph!\")\n\n    non_ro_id = [node for node in graph if not IDCreator().is_ro_id(node)]\n    if non_ro_id:\n        raise ValueError(\"The addresses must only include ResearchObject ID's!\")\n\n    if not graph[self.id]:\n        raise ValueError(\"The addresses must include the dataset ID!\")\n\n    vrs = [node for node in graph if node.startswith(\"VR\")]\n    if vrs:\n        raise ValueError(\"The addresses must not include Variable ID's!\")\n\n    schema = self.schema\n    schema_graph = nx.MultiDiGraph()\n    schema_graph.add_edges_from(schema)\n    for address_edge in addresses:\n        cls0 = ResearchObjectHandler._prefix_to_class(address_edge[0])\n        cls1 = ResearchObjectHandler._prefix_to_class(address_edge[1])\n        if cls0 not in schema_graph.predecessors(cls1) or cls1 not in schema_graph.successors(cls0):\n            raise ValueError(\"The addresses must match the schema!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.validate_dataset_path","title":"<code>validate_dataset_path(path)</code>","text":"<p>Validate the dataset path.</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>def validate_dataset_path(self, path: str) -&gt; None:\n    \"\"\"Validate the dataset path.\"\"\"\n    import os\n    if not os.path.exists(path):\n        raise ValueError(\"Specified path is not a path or does not currently exist!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.validate_schema","title":"<code>validate_schema(schema)</code>","text":"<p>Validate that the data schema follows the proper format. Must be a dict of dicts, where all keys are Python types matching a DataObject subclass, and the lowest levels are empty.</p> Source code in <code>src\\ResearchOS\\DataObjects\\dataset.py</code> <pre><code>def validate_schema(self, schema: list) -&gt; None:\n    \"\"\"Validate that the data schema follows the proper format.\n    Must be a dict of dicts, where all keys are Python types matching a DataObject subclass, and the lowest levels are empty.\"\"\"\n    from ResearchOS.research_object import ResearchObject\n    subclasses = ResearchObject.__subclasses__()\n    dataobj_subclasses = DataObject.__subclasses__()\n    vr = [x for x in subclasses if hasattr(x,\"prefix\") and x.prefix == \"VR\"][0]                \n\n    graph = nx.MultiDiGraph()\n    try:\n        graph.add_edges_from(schema)\n    except nx.NetworkXError:\n        raise ValueError(\"The schema must be provided as an edge list!\")\n    if not nx.is_directed_acyclic_graph(graph):\n        raise ValueError(\"The schema must be a directed acyclic graph!\")\n\n    non_subclass = [node for node in graph if node not in dataobj_subclasses]\n    if non_subclass:\n        raise ValueError(\"The schema must only include DataObject subclasses!\")\n\n    if Dataset not in graph:\n        raise ValueError(\"The schema must include the Dataset class as a source node!\")\n\n    if vr in graph:\n        raise ValueError(\"The schema must not include the Variable class as a target node!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/phase/","title":"Phase","text":"<p>             Bases: <code>DataObject</code></p> <p>Phase class.</p> Source code in <code>src\\ResearchOS\\DataObjects\\phase.py</code> <pre><code>class Phase(DataObject):\n    \"\"\"Phase class.\"\"\"\n\n    prefix = \"PH\"\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/subject/","title":"Subject","text":"<p>             Bases: <code>DataObject</code></p> <p>Subject class.</p> Source code in <code>src\\ResearchOS\\DataObjects\\subject.py</code> <pre><code>class Subject(DataObject):\n    \"\"\"Subject class.\"\"\"\n\n    prefix: str = \"SJ\"\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/trial/","title":"Trial","text":"<p>             Bases: <code>DataObject</code></p> Source code in <code>src\\ResearchOS\\DataObjects\\trial.py</code> <pre><code>class Trial(DataObject):\n\n    prefix = \"TR\"\n</code></pre>"},{"location":"Research%20Object%20Types/Data%20Object%20Types/visit/","title":"Visit","text":"<p>             Bases: <code>DataObject</code></p> Source code in <code>src\\ResearchOS\\DataObjects\\visit.py</code> <pre><code>class Visit(DataObject):\n\n    prefix: str = \"VS\"\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/analysis/","title":"Analysis","text":"<p>             Bases: <code>PipelineObject</code></p> Source code in <code>src\\ResearchOS\\PipelineObjects\\analysis.py</code> <pre><code>class Analysis(PipelineObject):\n\n    prefix = \"AN\"\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/","title":"Logsheet","text":"<p>             Bases: <code>PipelineObject</code></p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>class Logsheet(PipelineObject):\n\n    prefix = \"LG\"\n\n    def read_and_clean_logsheet(self, nrows: int = None) -&gt; list:\n        \"\"\"Read the logsheet (CSV only) and clean it.\"\"\"\n        logsheet = []\n        if platform.system() == \"Windows\":\n            first_elem_prefix = \"\u00ef\u00bb\u00bf\"\n        else:\n            first_elem_prefix = '\\ufeff'\n        with open(self.path, \"r\") as f:\n            reader = csv.reader(f, delimiter=',', quotechar='\"')            \n\n            for row_num, row in enumerate(reader):                                    \n                logsheet.append(row)\n                if nrows is not None and row_num == nrows-1:\n                    break\n\n        # 7. Check that the headers all match the logsheet.\n        logsheet[0][0] = logsheet[0][0][len(first_elem_prefix):]\n        return logsheet\n\n    ### Logsheet path\n\n    def validate_path(self, path: str) -&gt; None:\n        \"\"\"Validate the logsheet path.\"\"\"\n        # 1. Check that the path exists in the file system.\n        import os\n        if not isinstance(path, str):\n            raise ValueError(\"Path must be a string!\")\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path does not exist!\")\n        # 2. Check that the path is a file.\n        if not os.path.isfile(path):\n            raise ValueError(\"Specified path is not a file!\")\n        # 3. Check that the file is a CSV.\n        if not path.endswith((\"csv\", \"xlsx\", \"xls\")):\n            raise ValueError(\"Specified file is not a CSV!\")\n\n    ### Logsheet headers\n\n    def validate_headers(self, headers: list) -&gt; None:\n        \"\"\"Validate the logsheet headers. These are the headers that are in the logsheet file.\n        The headers must be a list of tuples, where each tuple has 3 elements:\n        1. A string (the name of the header)\n        2. A type (the type of the header)\n        3. A valid variable ID (the ID of the Variable that the header corresponds to)\"\"\"\n        self.validate_path(self.path)\n\n        # 1. Check that the headers are a list.\n        if not isinstance(headers, list):\n            raise ValueError(\"Headers must be a list!\")\n\n        # 2. Check that the headers are a list of tuples and meet the other requirements.        \n        for header in headers:\n            if not isinstance(header, tuple):\n                raise ValueError(\"Headers must be a list of tuples!\")\n            # 3. Check that each header tuple has 3 elements.        \n            if len(header) != 4:\n                raise ValueError(\"Each header tuple must have 4 elements!\")\n            # 4. Check that the first element of each header tuple is a string.        \n            if not isinstance(header[0], str):\n                raise ValueError(\"First element of each header tuple must be a string!\")\n            if header[1] not in [str, int, float]:\n                raise ValueError(\"Second element of each header tuple must be a Python type!\")\n            if header[2] not in DataObject.__subclasses__():\n                raise ValueError(\"Third element of each header tuple must be a ResearchObject subclass!\")\n            # 6. Check that the third element of each header tuple is a valid variable ID.                \n            if not header[3].startswith(Variable.prefix) or not ResearchObjectHandler.object_exists(header[3]):\n                raise ValueError(\"Fourth element of each header tuple must be a valid pre-existing variable ID!\")\n\n        logsheet = self.read_and_clean_logsheet(nrows = 1)\n        headers_in_logsheet = logsheet[0]\n        header_names = [header[0] for header in headers]\n        missing = [header for header in headers_in_logsheet if header not in header_names]\n\n        if len(missing) &gt; 0:\n            raise ValueError(f\"The headers {missing} do not match between logsheet and code!\")\n\n    def to_json_headers(self, headers: list) -&gt; str:\n        \"\"\"Convert the headers to a JSON string.\n        Also sets the VR's name and level.\"\"\"\n        str_headers = []\n        for header in headers:\n            # Update the Variable object with the name if it is not already set, and the level.\n            vr = Variable(id = header[3])\n            vr.name = header[0]\n            vr.level = header[2]\n            str_headers.append((header[0], str(header[1])[8:-2], header[2].prefix, header[3]))\n        return json.dumps(str_headers)\n\n    def from_json_headers(self, json_var: str) -&gt; list:\n        \"\"\"Convert the JSON string to a list of headers.\"\"\"\n        subclasses = DataObject.__subclasses__()\n        str_var = json.loads(json_var)\n        headers = []\n        mapping = {\n            \"str\": str,\n            \"int\": int,\n            \"float\": float\n        }\n        for header in str_var:\n            cls_header = [cls for cls in subclasses if cls.prefix == header[2]][0]\n            headers.append((header[0], mapping[header[1]], cls_header, header[3]))                \n        return headers\n\n    ### Num header rows\n\n    def validate_num_header_rows(self, num_header_rows: int) -&gt; None:\n        \"\"\"Validate the number of header rows. If it is not valid, the value is rejected.\"\"\"                \n        if not isinstance(num_header_rows, (int, float)):\n            raise ValueError(\"Num header rows must be numeric!\")\n        if num_header_rows&lt;0:\n            raise ValueError(\"Num header rows must be positive!\")\n        if num_header_rows % 1 != 0:\n            raise ValueError(\"Num header rows must be an integer!\")  \n\n    ### Class column names\n\n    def validate_class_column_names(self, class_column_names: dict) -&gt; None:\n        \"\"\"Validate the class column names. Must be a dict where the keys are the column names in the logsheet and the values are the DataObject subclasses.\"\"\"\n        self.validate_path(self.path)\n        # 1. Check that the class column names are a dict.\n        if not isinstance(class_column_names, dict):\n            raise ValueError(\"Class column names must be a dict!\")\n        # 2. Check that the class column names are a dict of str to type.        \n        for key, value in class_column_names.items():\n            if not isinstance(key, str):\n                raise ValueError(\"Keys of class column names must be strings!\")\n            if not issubclass(value, DataObject):\n                raise ValueError(\"Values of class column names must be Python types that subclass DataObject!\")\n\n        headers = self.read_and_clean_logsheet(nrows = 1)[0]\n        if not all([header in headers for header in class_column_names.keys()]):\n            raise ValueError(\"The class column names must be in the logsheet headers!\")\n\n    def from_json_class_column_names(self, json_var: dict) -&gt; dict:\n        \"\"\"Convert the dict from JSON string where values are class prefixes to a dict where keys are column names and values are DataObject subclasses.\"\"\"     \n        prefix_var = json.loads(json_var)\n        class_column_names = {}\n        all_classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n        for key, prefix in prefix_var.items():\n            for cls in all_classes:\n                if hasattr(cls, \"prefix\") and cls.prefix == prefix:\n                    class_column_names[key] = cls\n                    break\n        return class_column_names\n\n    def to_json_class_column_names(self, var: dict) -&gt; dict:\n        \"\"\"Convert the dict from a dict where keys are column names and values are DataObject subclasses to a JSON string where values are class prefixes.\"\"\"        \n        prefix_var = {}\n        for key in var:\n            prefix_var[key] = var[key].prefix\n        return json.dumps(prefix_var)\n\n    #################### Start class-specific methods ####################\n    def load_xlsx(self) -&gt; list:\n        \"\"\"Load the logsheet as a list of lists using Pandas.\"\"\"        \n        df = pd.read_excel(self.path, header = None)\n        return df.values.tolist()\n\n    def read_logsheet(self) -&gt; None:\n        \"\"\"Run the logsheet import process.\"\"\"\n        ds = Dataset(id = self.get_dataset_id())\n        self.validate_class_column_names(self.class_column_names)\n        self.validate_headers(self.headers)\n        self.validate_num_header_rows(self.num_header_rows)\n        self.validate_path(self.path)\n\n        # 1. Load the logsheet (using built-in Python libraries)\n        if self.path.endswith((\"xlsx\", \"xls\")):\n            full_logsheet = self.load_xlsx()\n        else:\n            full_logsheet = self.read_and_clean_logsheet()\n\n        if len(full_logsheet) &lt; self.num_header_rows:\n            raise ValueError(\"The number of header rows is greater than the number of rows in the logsheet!\")\n\n        # Run the logsheet import.\n        # headers = full_logsheet[0:self.num_header_rows]\n        if len(full_logsheet) == self.num_header_rows:\n            logsheet = []\n        else:\n            logsheet = full_logsheet[self.num_header_rows:]\n\n        # logsheet = logsheet[0:50] # For testing purposes, only read the first 50 rows.\n\n        # For each row, connect instances of the appropriate DataObject subclass to all other instances of appropriate DataObject subclasses.\n        headers_in_logsheet = full_logsheet[0]\n        all_headers = self.headers\n        header_names = [header[0] for header in self.headers]\n        header_types = [header[1] for header in self.headers]\n        header_levels = [header[2] for header in self.headers]\n        header_vrids = [header[3] for header in self.headers]\n        # Load/create all of the Variables\n        vr_list = []\n        vr_obj_list = []\n        for idx, vr_id in enumerate(header_vrids):\n            vr = Variable(id = vr_id)\n            assert vr.level == header_levels[idx]\n            vr_obj_list.append(vr)\n            vr_list.append(vr.id)\n\n        # Order the class column names by precedence in the schema so that higher level objects always exist before lower level.\n        schema = ds.schema\n        schema_graph = nx.DiGraph()\n        schema_graph.add_edges_from(schema)\n        order = list(nx.topological_sort(schema_graph))\n        if len(order) &lt;= 1:\n            raise ValueError(\"The schema must have at least 2 elements including the Dataset!\")\n        order = order[1:] # Remove the Dataset class from the order.\n        dobj_column_names = []\n        for cls in order:\n            for column_name, cls_item in self.class_column_names.items():\n                if cls is cls_item:\n                    dobj_column_names.append(column_name)\n\n        # Create the data objects.\n        # Get all of the names of the data objects, after they're cleaned for SQLite.\n        cols_idx = [headers_in_logsheet.index(header) for header in dobj_column_names] # Get the indices of the data objects columns.\n        dobj_names = [] # The matrix of data object names (values in the logsheet).\n        for row in logsheet:\n            dobj_names.append([])\n            for idx in cols_idx:\n                raw_value = row[idx]\n                type_class = header_types[idx]\n                value = self.clean_value(type_class, raw_value)\n                dobj_names[-1].append(value)\n        for row_num, row in enumerate(dobj_names):\n            if not all([str(cell).isidentifier() for cell in row]):\n                raise ValueError(f\"Row # (1-based): {row_num+self.num_header_rows+1} all data object names must be non-empty and valid variable names!\")\n        [row.insert(0, ds.id) for row in dobj_names] # Prepend the Dataset to the first column of each row.\n        name_ids_dict = {} # The dict that maps the values (names) to the IDs. Separate dict for each class, each class is a top-level key of the dict.\n        name_ids_dict[Dataset] = {ds.name: ds.id}\n        name_dobjs_dict = {}\n        name_dobjs_dict[Dataset] = {ds.name: ds}\n        for cls in order:\n            name_ids_dict[cls] = {} # Initialize the dict for this class.            \n            name_dobjs_dict[cls] = {}\n\n        # Create the DataObject instances in the dict.        \n        all_dobjs_ordered = [] # The list of lists of DataObject instances, ordered by the order of the schema.                \n        for row_num, row in enumerate(dobj_names):\n            row = row[1:]\n            all_dobjs_ordered.append([ds]) # Add the Dataset to the beginning of each row.\n            for idx in range(len(row)):\n                cls = order[idx] # The class to create.\n                col_idx = cols_idx[idx] # The index of the column in the logsheet.\n                value = self.clean_value(header_types[col_idx], row[idx])\n                if value not in name_ids_dict[cls]:                    \n                    name_ids_dict[cls][value] = IDCreator().create_ro_id(cls)\n                    dobj = cls(id = name_ids_dict[cls][value], name = value) # Create the research object.                \n                    name_dobjs_dict[cls][value] = dobj\n                dobj = name_dobjs_dict[cls][value]\n                all_dobjs_ordered[-1].append(dobj) # Matrix of all research objects.\n                print(\"Creating DataObject, Row: \", row_num, \"Column: \", cls.prefix, \"Value: \", value, \"ID: \", dobj.id, \"Memory Loc: \", id(dobj))\n\n        # Arrange the address ID's that were generated into an edge list.\n        # Then assign that to the Dataset.\n        addresses = []\n        for row in all_dobjs_ordered:\n            for idx, dobj in enumerate(row):\n                if idx == 0:\n                    continue\n                ids = [row[idx-1].id, dobj.id]\n                if ids not in addresses:\n                    addresses.append(ids)\n        ds.addresses = addresses # Store addresses, also creates address_graph.\n\n\n        # Assign the values to the DataObject instances.\n        # Validates that the logsheet is of valid format.\n        # i.e. Doesn't have conflicting values for one level (empty/None is OK)        \n        action = Action(name = \"read logsheet\")\n        action.commit = True\n        attrs_cache_dict = {}\n        for row_num, row in enumerate(logsheet):\n            row_dobjs = all_dobjs_ordered[row_num][1:]\n\n            # Assign all of the data to the appropriate DataObject instances.\n            # Includes the \"data object columns\" so that the DataObjects have an attribute with the name of the header name.\n            for header in all_headers:\n                name = header[0]\n                col_idx = headers_in_logsheet.index(name)                \n                type_class = header[1]\n                level = header[2]\n                level_idx = order.index(level)\n                vr_id = header[3]                \n                value = self.clean_value(type_class, row[headers_in_logsheet.index(name)])\n                # Set up the cache dict for this data object.\n                if not row_dobjs[level_idx].id in attrs_cache_dict:\n                    attrs_cache_dict[row_dobjs[level_idx].id] = {}\n                # Set up the cache dict for this data object for this attribute.\n                if name not in attrs_cache_dict[row_dobjs[level_idx].id]:\n                    attrs_cache_dict[row_dobjs[level_idx].id][name] = None\n                print(\"Row: \", row_num, \"Column: \", name, \"Value: \", value)\n                prev_value = attrs_cache_dict[row_dobjs[level_idx].id][name]\n                # prev_value = getattr(row_dobjs[level_idx], name, None) # May not exist yet.                \n                if prev_value is not None:                    \n                    if prev_value == value or value is None:\n                        continue\n                    conn = action.pool.get_connection()\n                    action.pool.return_connection(conn)\n                    raise ValueError(f\"Row # (1-based): {row_num+self.num_header_rows+1} Column: {name} has conflicting values!\")                \n                row_dobjs[level_idx].__setattr__(name, value, action = action) # Set the attribute of this DataObject instance to the value in the logsheet.                \n                attrs_cache_dict[row_dobjs[level_idx].id][name] = value\n                dobj = row_dobjs[level_idx]\n\n    def clean_value(self, type_class: type, raw_value: Any) -&gt; Any:\n        \"\"\"Convert to proper type and clean the value of the logsheet cell.\"\"\"\n        try:\n            value = type_class(raw_value)\n        except ValueError:\n            value = raw_value\n        if isinstance(value, str):\n            value = value.replace(\"'\", \"''\") # Handle single quotes.\n        if value == '': # Empty\n            value = None\n        return value\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.clean_value","title":"<code>clean_value(type_class, raw_value)</code>","text":"<p>Convert to proper type and clean the value of the logsheet cell.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def clean_value(self, type_class: type, raw_value: Any) -&gt; Any:\n    \"\"\"Convert to proper type and clean the value of the logsheet cell.\"\"\"\n    try:\n        value = type_class(raw_value)\n    except ValueError:\n        value = raw_value\n    if isinstance(value, str):\n        value = value.replace(\"'\", \"''\") # Handle single quotes.\n    if value == '': # Empty\n        value = None\n    return value\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.from_json_class_column_names","title":"<code>from_json_class_column_names(json_var)</code>","text":"<p>Convert the dict from JSON string where values are class prefixes to a dict where keys are column names and values are DataObject subclasses.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def from_json_class_column_names(self, json_var: dict) -&gt; dict:\n    \"\"\"Convert the dict from JSON string where values are class prefixes to a dict where keys are column names and values are DataObject subclasses.\"\"\"     \n    prefix_var = json.loads(json_var)\n    class_column_names = {}\n    all_classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n    for key, prefix in prefix_var.items():\n        for cls in all_classes:\n            if hasattr(cls, \"prefix\") and cls.prefix == prefix:\n                class_column_names[key] = cls\n                break\n    return class_column_names\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.from_json_headers","title":"<code>from_json_headers(json_var)</code>","text":"<p>Convert the JSON string to a list of headers.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def from_json_headers(self, json_var: str) -&gt; list:\n    \"\"\"Convert the JSON string to a list of headers.\"\"\"\n    subclasses = DataObject.__subclasses__()\n    str_var = json.loads(json_var)\n    headers = []\n    mapping = {\n        \"str\": str,\n        \"int\": int,\n        \"float\": float\n    }\n    for header in str_var:\n        cls_header = [cls for cls in subclasses if cls.prefix == header[2]][0]\n        headers.append((header[0], mapping[header[1]], cls_header, header[3]))                \n    return headers\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.load_xlsx","title":"<code>load_xlsx()</code>","text":"<p>Load the logsheet as a list of lists using Pandas.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def load_xlsx(self) -&gt; list:\n    \"\"\"Load the logsheet as a list of lists using Pandas.\"\"\"        \n    df = pd.read_excel(self.path, header = None)\n    return df.values.tolist()\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.read_and_clean_logsheet","title":"<code>read_and_clean_logsheet(nrows=None)</code>","text":"<p>Read the logsheet (CSV only) and clean it.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def read_and_clean_logsheet(self, nrows: int = None) -&gt; list:\n    \"\"\"Read the logsheet (CSV only) and clean it.\"\"\"\n    logsheet = []\n    if platform.system() == \"Windows\":\n        first_elem_prefix = \"\u00ef\u00bb\u00bf\"\n    else:\n        first_elem_prefix = '\\ufeff'\n    with open(self.path, \"r\") as f:\n        reader = csv.reader(f, delimiter=',', quotechar='\"')            \n\n        for row_num, row in enumerate(reader):                                    \n            logsheet.append(row)\n            if nrows is not None and row_num == nrows-1:\n                break\n\n    # 7. Check that the headers all match the logsheet.\n    logsheet[0][0] = logsheet[0][0][len(first_elem_prefix):]\n    return logsheet\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.read_logsheet","title":"<code>read_logsheet()</code>","text":"<p>Run the logsheet import process.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def read_logsheet(self) -&gt; None:\n    \"\"\"Run the logsheet import process.\"\"\"\n    ds = Dataset(id = self.get_dataset_id())\n    self.validate_class_column_names(self.class_column_names)\n    self.validate_headers(self.headers)\n    self.validate_num_header_rows(self.num_header_rows)\n    self.validate_path(self.path)\n\n    # 1. Load the logsheet (using built-in Python libraries)\n    if self.path.endswith((\"xlsx\", \"xls\")):\n        full_logsheet = self.load_xlsx()\n    else:\n        full_logsheet = self.read_and_clean_logsheet()\n\n    if len(full_logsheet) &lt; self.num_header_rows:\n        raise ValueError(\"The number of header rows is greater than the number of rows in the logsheet!\")\n\n    # Run the logsheet import.\n    # headers = full_logsheet[0:self.num_header_rows]\n    if len(full_logsheet) == self.num_header_rows:\n        logsheet = []\n    else:\n        logsheet = full_logsheet[self.num_header_rows:]\n\n    # logsheet = logsheet[0:50] # For testing purposes, only read the first 50 rows.\n\n    # For each row, connect instances of the appropriate DataObject subclass to all other instances of appropriate DataObject subclasses.\n    headers_in_logsheet = full_logsheet[0]\n    all_headers = self.headers\n    header_names = [header[0] for header in self.headers]\n    header_types = [header[1] for header in self.headers]\n    header_levels = [header[2] for header in self.headers]\n    header_vrids = [header[3] for header in self.headers]\n    # Load/create all of the Variables\n    vr_list = []\n    vr_obj_list = []\n    for idx, vr_id in enumerate(header_vrids):\n        vr = Variable(id = vr_id)\n        assert vr.level == header_levels[idx]\n        vr_obj_list.append(vr)\n        vr_list.append(vr.id)\n\n    # Order the class column names by precedence in the schema so that higher level objects always exist before lower level.\n    schema = ds.schema\n    schema_graph = nx.DiGraph()\n    schema_graph.add_edges_from(schema)\n    order = list(nx.topological_sort(schema_graph))\n    if len(order) &lt;= 1:\n        raise ValueError(\"The schema must have at least 2 elements including the Dataset!\")\n    order = order[1:] # Remove the Dataset class from the order.\n    dobj_column_names = []\n    for cls in order:\n        for column_name, cls_item in self.class_column_names.items():\n            if cls is cls_item:\n                dobj_column_names.append(column_name)\n\n    # Create the data objects.\n    # Get all of the names of the data objects, after they're cleaned for SQLite.\n    cols_idx = [headers_in_logsheet.index(header) for header in dobj_column_names] # Get the indices of the data objects columns.\n    dobj_names = [] # The matrix of data object names (values in the logsheet).\n    for row in logsheet:\n        dobj_names.append([])\n        for idx in cols_idx:\n            raw_value = row[idx]\n            type_class = header_types[idx]\n            value = self.clean_value(type_class, raw_value)\n            dobj_names[-1].append(value)\n    for row_num, row in enumerate(dobj_names):\n        if not all([str(cell).isidentifier() for cell in row]):\n            raise ValueError(f\"Row # (1-based): {row_num+self.num_header_rows+1} all data object names must be non-empty and valid variable names!\")\n    [row.insert(0, ds.id) for row in dobj_names] # Prepend the Dataset to the first column of each row.\n    name_ids_dict = {} # The dict that maps the values (names) to the IDs. Separate dict for each class, each class is a top-level key of the dict.\n    name_ids_dict[Dataset] = {ds.name: ds.id}\n    name_dobjs_dict = {}\n    name_dobjs_dict[Dataset] = {ds.name: ds}\n    for cls in order:\n        name_ids_dict[cls] = {} # Initialize the dict for this class.            \n        name_dobjs_dict[cls] = {}\n\n    # Create the DataObject instances in the dict.        \n    all_dobjs_ordered = [] # The list of lists of DataObject instances, ordered by the order of the schema.                \n    for row_num, row in enumerate(dobj_names):\n        row = row[1:]\n        all_dobjs_ordered.append([ds]) # Add the Dataset to the beginning of each row.\n        for idx in range(len(row)):\n            cls = order[idx] # The class to create.\n            col_idx = cols_idx[idx] # The index of the column in the logsheet.\n            value = self.clean_value(header_types[col_idx], row[idx])\n            if value not in name_ids_dict[cls]:                    \n                name_ids_dict[cls][value] = IDCreator().create_ro_id(cls)\n                dobj = cls(id = name_ids_dict[cls][value], name = value) # Create the research object.                \n                name_dobjs_dict[cls][value] = dobj\n            dobj = name_dobjs_dict[cls][value]\n            all_dobjs_ordered[-1].append(dobj) # Matrix of all research objects.\n            print(\"Creating DataObject, Row: \", row_num, \"Column: \", cls.prefix, \"Value: \", value, \"ID: \", dobj.id, \"Memory Loc: \", id(dobj))\n\n    # Arrange the address ID's that were generated into an edge list.\n    # Then assign that to the Dataset.\n    addresses = []\n    for row in all_dobjs_ordered:\n        for idx, dobj in enumerate(row):\n            if idx == 0:\n                continue\n            ids = [row[idx-1].id, dobj.id]\n            if ids not in addresses:\n                addresses.append(ids)\n    ds.addresses = addresses # Store addresses, also creates address_graph.\n\n\n    # Assign the values to the DataObject instances.\n    # Validates that the logsheet is of valid format.\n    # i.e. Doesn't have conflicting values for one level (empty/None is OK)        \n    action = Action(name = \"read logsheet\")\n    action.commit = True\n    attrs_cache_dict = {}\n    for row_num, row in enumerate(logsheet):\n        row_dobjs = all_dobjs_ordered[row_num][1:]\n\n        # Assign all of the data to the appropriate DataObject instances.\n        # Includes the \"data object columns\" so that the DataObjects have an attribute with the name of the header name.\n        for header in all_headers:\n            name = header[0]\n            col_idx = headers_in_logsheet.index(name)                \n            type_class = header[1]\n            level = header[2]\n            level_idx = order.index(level)\n            vr_id = header[3]                \n            value = self.clean_value(type_class, row[headers_in_logsheet.index(name)])\n            # Set up the cache dict for this data object.\n            if not row_dobjs[level_idx].id in attrs_cache_dict:\n                attrs_cache_dict[row_dobjs[level_idx].id] = {}\n            # Set up the cache dict for this data object for this attribute.\n            if name not in attrs_cache_dict[row_dobjs[level_idx].id]:\n                attrs_cache_dict[row_dobjs[level_idx].id][name] = None\n            print(\"Row: \", row_num, \"Column: \", name, \"Value: \", value)\n            prev_value = attrs_cache_dict[row_dobjs[level_idx].id][name]\n            # prev_value = getattr(row_dobjs[level_idx], name, None) # May not exist yet.                \n            if prev_value is not None:                    \n                if prev_value == value or value is None:\n                    continue\n                conn = action.pool.get_connection()\n                action.pool.return_connection(conn)\n                raise ValueError(f\"Row # (1-based): {row_num+self.num_header_rows+1} Column: {name} has conflicting values!\")                \n            row_dobjs[level_idx].__setattr__(name, value, action = action) # Set the attribute of this DataObject instance to the value in the logsheet.                \n            attrs_cache_dict[row_dobjs[level_idx].id][name] = value\n            dobj = row_dobjs[level_idx]\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.to_json_class_column_names","title":"<code>to_json_class_column_names(var)</code>","text":"<p>Convert the dict from a dict where keys are column names and values are DataObject subclasses to a JSON string where values are class prefixes.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def to_json_class_column_names(self, var: dict) -&gt; dict:\n    \"\"\"Convert the dict from a dict where keys are column names and values are DataObject subclasses to a JSON string where values are class prefixes.\"\"\"        \n    prefix_var = {}\n    for key in var:\n        prefix_var[key] = var[key].prefix\n    return json.dumps(prefix_var)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.to_json_headers","title":"<code>to_json_headers(headers)</code>","text":"<p>Convert the headers to a JSON string. Also sets the VR's name and level.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def to_json_headers(self, headers: list) -&gt; str:\n    \"\"\"Convert the headers to a JSON string.\n    Also sets the VR's name and level.\"\"\"\n    str_headers = []\n    for header in headers:\n        # Update the Variable object with the name if it is not already set, and the level.\n        vr = Variable(id = header[3])\n        vr.name = header[0]\n        vr.level = header[2]\n        str_headers.append((header[0], str(header[1])[8:-2], header[2].prefix, header[3]))\n    return json.dumps(str_headers)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_class_column_names","title":"<code>validate_class_column_names(class_column_names)</code>","text":"<p>Validate the class column names. Must be a dict where the keys are the column names in the logsheet and the values are the DataObject subclasses.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def validate_class_column_names(self, class_column_names: dict) -&gt; None:\n    \"\"\"Validate the class column names. Must be a dict where the keys are the column names in the logsheet and the values are the DataObject subclasses.\"\"\"\n    self.validate_path(self.path)\n    # 1. Check that the class column names are a dict.\n    if not isinstance(class_column_names, dict):\n        raise ValueError(\"Class column names must be a dict!\")\n    # 2. Check that the class column names are a dict of str to type.        \n    for key, value in class_column_names.items():\n        if not isinstance(key, str):\n            raise ValueError(\"Keys of class column names must be strings!\")\n        if not issubclass(value, DataObject):\n            raise ValueError(\"Values of class column names must be Python types that subclass DataObject!\")\n\n    headers = self.read_and_clean_logsheet(nrows = 1)[0]\n    if not all([header in headers for header in class_column_names.keys()]):\n        raise ValueError(\"The class column names must be in the logsheet headers!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_headers","title":"<code>validate_headers(headers)</code>","text":"<p>Validate the logsheet headers. These are the headers that are in the logsheet file. The headers must be a list of tuples, where each tuple has 3 elements: 1. A string (the name of the header) 2. A type (the type of the header) 3. A valid variable ID (the ID of the Variable that the header corresponds to)</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def validate_headers(self, headers: list) -&gt; None:\n    \"\"\"Validate the logsheet headers. These are the headers that are in the logsheet file.\n    The headers must be a list of tuples, where each tuple has 3 elements:\n    1. A string (the name of the header)\n    2. A type (the type of the header)\n    3. A valid variable ID (the ID of the Variable that the header corresponds to)\"\"\"\n    self.validate_path(self.path)\n\n    # 1. Check that the headers are a list.\n    if not isinstance(headers, list):\n        raise ValueError(\"Headers must be a list!\")\n\n    # 2. Check that the headers are a list of tuples and meet the other requirements.        \n    for header in headers:\n        if not isinstance(header, tuple):\n            raise ValueError(\"Headers must be a list of tuples!\")\n        # 3. Check that each header tuple has 3 elements.        \n        if len(header) != 4:\n            raise ValueError(\"Each header tuple must have 4 elements!\")\n        # 4. Check that the first element of each header tuple is a string.        \n        if not isinstance(header[0], str):\n            raise ValueError(\"First element of each header tuple must be a string!\")\n        if header[1] not in [str, int, float]:\n            raise ValueError(\"Second element of each header tuple must be a Python type!\")\n        if header[2] not in DataObject.__subclasses__():\n            raise ValueError(\"Third element of each header tuple must be a ResearchObject subclass!\")\n        # 6. Check that the third element of each header tuple is a valid variable ID.                \n        if not header[3].startswith(Variable.prefix) or not ResearchObjectHandler.object_exists(header[3]):\n            raise ValueError(\"Fourth element of each header tuple must be a valid pre-existing variable ID!\")\n\n    logsheet = self.read_and_clean_logsheet(nrows = 1)\n    headers_in_logsheet = logsheet[0]\n    header_names = [header[0] for header in headers]\n    missing = [header for header in headers_in_logsheet if header not in header_names]\n\n    if len(missing) &gt; 0:\n        raise ValueError(f\"The headers {missing} do not match between logsheet and code!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_num_header_rows","title":"<code>validate_num_header_rows(num_header_rows)</code>","text":"<p>Validate the number of header rows. If it is not valid, the value is rejected.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def validate_num_header_rows(self, num_header_rows: int) -&gt; None:\n    \"\"\"Validate the number of header rows. If it is not valid, the value is rejected.\"\"\"                \n    if not isinstance(num_header_rows, (int, float)):\n        raise ValueError(\"Num header rows must be numeric!\")\n    if num_header_rows&lt;0:\n        raise ValueError(\"Num header rows must be positive!\")\n    if num_header_rows % 1 != 0:\n        raise ValueError(\"Num header rows must be an integer!\")  \n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.validate_path","title":"<code>validate_path(path)</code>","text":"<p>Validate the logsheet path.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\logsheet.py</code> <pre><code>def validate_path(self, path: str) -&gt; None:\n    \"\"\"Validate the logsheet path.\"\"\"\n    # 1. Check that the path exists in the file system.\n    import os\n    if not isinstance(path, str):\n        raise ValueError(\"Path must be a string!\")\n    if not os.path.exists(path):\n        raise ValueError(\"Specified path does not exist!\")\n    # 2. Check that the path is a file.\n    if not os.path.isfile(path):\n        raise ValueError(\"Specified path is not a file!\")\n    # 3. Check that the file is a CSV.\n    if not path.endswith((\"csv\", \"xlsx\", \"xls\")):\n        raise ValueError(\"Specified file is not a CSV!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/pipeline_object/","title":"Pipeline Objects","text":"<p>             Bases: <code>ResearchObject</code></p> <p>Parent class of all pipeline objects: Projects, Analyses, Logsheets, Process Groups, Processes, Variables, SpecifyTrials, Views</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\pipeline_object.py</code> <pre><code>class PipelineObject(ResearchObject):\n    \"\"\"Parent class of all pipeline objects: Projects, Analyses, Logsheets, Process Groups, Processes, Variables, SpecifyTrials, Views\"\"\"\n\n    def _add_source_object_id(self, source_object_id: str, action: Action = None) -&gt; None:\n        \"\"\"Add a source object ID to the current object.\"\"\"\n        target_object_id = self.id\n        is_active = 1\n        self._update_pipeline_edge(source_object_id, target_object_id, is_active, action)\n\n    def _remove_source_object_id(self, source_object_id: str, action: Action = None) -&gt; None:\n        \"\"\"Remove a source object ID from the current object.\"\"\"\n        target_object_id = self.id\n        is_active = 0\n        self._update_pipeline_edge(source_object_id, target_object_id, is_active, action)\n\n    def _add_target_object_id(self, target_object_id: str, action: Action = None) -&gt; None:\n        \"\"\"Add a target object ID to the current object.\"\"\"\n        source_object_id = self.id\n        is_active = 1\n        self._update_pipeline_edge(source_object_id, target_object_id, is_active, action)\n\n    def _remove_target_object_id(self, target_object_id: str, action: Action = None) -&gt; None:\n        \"\"\"Remove a target object ID from the current object.\"\"\"\n        source_object_id = self.id\n        is_active = 0\n        self._update_pipeline_edge(source_object_id, target_object_id, is_active, action)\n\n    def _update_pipeline_edge(self, source_object_id: str, target_object_id: str, is_active: bool, action: Action = None) -&gt; None:\n        \"\"\"Update one pipeline edge.\"\"\"\n        if action is None:\n            action = Action(name = \"update_pipeline_edges\")\n        sqlquery = f\"INSERT INTO pipelineobjects_graph (action_id, source_object_id, target_object_id, is_active) VALUES ('{action.id}', '{source_object_id}', '{target_object_id}', {is_active})\"        \n        action.add_sql_query(sqlquery)\n        action.execute()\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/","title":"Process","text":"<p>             Bases: <code>PipelineObject</code></p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>class Process(PipelineObject):\n\n    prefix = \"PR\"\n\n    ## mfunc_name (MATLAB function name) methods\n\n    def validate_mfunc_name(self, mfunc_name: str) -&gt; None:\n        self.validate_mfolder(self.mfolder)\n        if not self.is_matlab and mfunc_name is None: \n            return\n        if not isinstance(mfunc_name, str):\n            raise ValueError(\"Function name must be a string!\")\n        if not str(mfunc_name).isidentifier():\n            raise ValueError(\"Function name must be a valid variable name!\")\n        if not os.path.exists(os.path.join(self.mfolder, mfunc_name + \".m\")):\n            raise ValueError(\"Function name must reference an existing MATLAB function in the specified folder.\")\n\n    ## mfolder (MATLAB folder) methods\n\n    def validate_mfolder(self, mfolder: str) -&gt; None:\n        if not self.is_matlab and mfolder is None:\n            return\n        if not self.is_matlab:\n            raise ValueError(\"mfolder must be None if is_matlab is False.\")\n        if not isinstance(mfolder, str):\n            raise ValueError(\"Path must be a string!\")\n        if not os.path.exists(mfolder):\n            raise ValueError(\"Path must be a valid existing folder path!\")\n\n    ## method (Python method) methods\n\n    def validate_method(self, method: Callable) -&gt; None:\n        if method is None and self.is_matlab:\n            return\n        if not self.is_matlab:\n            raise ValueError(\"Method must be None if is_matlab is False.\")\n        if not isinstance(method, Callable):\n            raise ValueError(\"Method must be a callable function!\")\n        if method.__module__ not in sys.modules:\n            raise ValueError(\"Method must be in an imported module!\")\n\n    def from_json_method(self, json_method: str) -&gt; Callable:\n        \"\"\"Convert a JSON string to a method.\n        Returns None if the method name is not found (e.g. if code changed locations or something)\"\"\"\n        method_name = json.loads(json_method)\n        module_name, *attribute_path = method_name.split(\".\")        \n        module = importlib.import_module(module_name)\n        attribute = module\n        for attr in attribute_path:\n            attribute = getattr(attribute, attr)\n        return attribute\n\n    def to_json_method(self, method: Callable) -&gt; str:\n        \"\"\"Convert a method to a JSON string.\"\"\"\n        if method is None:\n            return json.dumps(None)\n        return json.dumps(method.__module__ + \".\" + method.__qualname__)\n\n    ## level (Process level) methods\n\n    def validate_level(self, level: type) -&gt; None:\n        if not isinstance(level, type):\n            raise ValueError(\"Level must be a type!\")\n\n    def from_json_level(self, level: str) -&gt; type:\n        \"\"\"Convert a JSON string to a Process level.\"\"\"\n        classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n        for cls in classes:\n            if hasattr(cls, \"prefix\") and cls.prefix == level:\n                return cls\n\n    def to_json_level(self, level: type) -&gt; str:\n        \"\"\"Convert a Process level to a JSON string.\"\"\"\n        return json.dumps(level.prefix)\n\n    ## subset_id (Subset ID) methods\n\n    def validate_subset_id(self, subset_id: str) -&gt; None:\n        \"\"\"Validate that the subset ID is correct.\"\"\"\n        if not ResearchObjectHandler.object_exists(subset_id):\n            raise ValueError(\"Subset ID must reference an existing Subset.\")\n\n    ## input &amp; output VRs methods\n\n    def validate_input_vrs(self, inputs: dict) -&gt; None:\n        \"\"\"Validate that the input variables are correct.\"\"\"        \n        if not self.is_matlab:\n            input_vr_names_in_code = get_input_variable_names(self.method)\n        self._validate_vrs(inputs, input_vr_names_in_code)\n\n    def validate_output_vrs(self, outputs: dict) -&gt; None:\n        \"\"\"Validate that the output variables are correct.\"\"\"        \n        if not self.is_matlab:\n            output_vr_names_in_code = get_returned_variable_names(self.method)\n        self._validate_vrs(outputs, output_vr_names_in_code)    \n\n    def _validate_vrs(self, vr: dict, vr_names_in_code: list) -&gt; None:\n        \"\"\"Validate that the input and output variables are correct. They should follow the same format.\n        The format is a dictionary with the variable name as the key and the variable ID as the value.\"\"\"       \n        self.validate_method(self.method) \n        if not isinstance(vr, dict):\n            raise ValueError(\"Variables must be a dictionary.\")\n        for key, value in vr.items():\n            if not isinstance(key, str):\n                raise ValueError(\"Variable names in code must be strings.\")\n            if not str(key).isidentifier():\n                raise ValueError(\"Variable names in code must be valid variable names.\")\n            if not isinstance(value, Variable):\n                raise ValueError(\"Variable ID's must be Variable objects.\")\n            if not ResearchObjectHandler.object_exists(value.id):\n                raise ValueError(\"Variable ID's must reference existing Variables.\")\n        if not self.is_matlab and not all([vr_name in vr_names_in_code for vr_name in vr.keys()]):\n            raise ValueError(\"Output variables must be returned by the method.\")\n\n    def from_json_input_vrs(self, input_vrs: str) -&gt; dict:\n        \"\"\"Convert a JSON string to a dictionary of input variables.\"\"\"\n        input_vrs_dict = json.loads(input_vrs)\n        return {key: Variable(id = value) for key, value in input_vrs_dict.items()}\n\n    def to_json_input_vrs(self, input_vrs: dict) -&gt; str:\n        \"\"\"Convert a dictionary of input variables to a JSON string.\"\"\"     \n        return json.dumps({key: value.id for key, value in input_vrs.items()})\n\n    def from_json_output_vrs(self, output_vrs: str) -&gt; dict:\n        \"\"\"Convert a JSON string to a dictionary of output variables.\"\"\"\n        output_vrs_dict = json.loads(output_vrs)\n        return {key: Variable(id = value) for key, value in output_vrs_dict.items()}\n\n    def to_json_output_vrs(self, output_vrs: dict) -&gt; str:\n        \"\"\"Convert a dictionary of output variables to a JSON string.\"\"\"\n        return json.dumps({key: value.id for key, value in output_vrs.items()})\n\n    def set_input_vrs(self, **kwargs) -&gt; None:\n        \"\"\"Convenience function to set the input variables with named variables rather than a dict.\"\"\"\n        action = Action(name = \"set_input_vrs\")\n        self.__setattr__(\"input_vrs\", kwargs, action = action)\n\n    def set_output_vrs(self, **kwargs) -&gt; None:\n        \"\"\"Convenience function to set the output variables with named variables rather than a dict.\"\"\"\n        action = Action(name = \"set_output_vrs\")\n        self.__setattr__(\"output_vrs\", kwargs, action = action)\n\n    def run(self) -&gt; None:\n        \"\"\"Execute the attached method.\n        kwargs are the input VR's.\"\"\"\n        ds = Dataset(id = self.get_dataset_id())\n        # 1. Validate that the level &amp; method have been properly set.\n        self.validate_method(self.method)\n        self.validate_level(self.level)\n\n        # TODO: Fix this to work with MATLAB.\n        if not self.is_matlab:\n            output_var_names_in_code = get_returned_variable_names(self.method)\n\n        # 2. Validate that the input &amp; output variables have been properly set.\n        self.validate_input_vrs(self.input_vrs)\n        self.validate_output_vrs(self.output_vrs)\n\n        # 3. Validate that the subsets have been properly set.\n        self.validate_subset_id(self.subset_id)\n\n        schema_id = self.get_current_schema_id(ds.id)\n\n        # 4. Run the method.\n        # Get the subset of the data.\n        subset_graph = Subset(id = self.subset_id).get_subset()\n\n        action = Action(name = f\"Running {self.mfunc_name} on {self.level.__name__}s.\")\n\n        # Do the setup for MATLAB.\n        if self.is_matlab:\n            eng.addpath(self.mfolder, nargout=0)\n\n        level_nodes = sorted([node for node in subset_graph if isinstance(node, self.level)], key = lambda x: x.name)\n        # Iterate over each data object at this level (e.g. all ros.Trial objects in the subset.)\n        schema = ds.schema\n        schema_graph = nx.MultiDiGraph(schema)\n        schema_order = list(nx.topological_sort(schema_graph))\n        pool_data = SQLiteConnectionPool(name = \"data\")\n        conn_data = pool_data.get_connection()\n        cursor_data = conn_data.cursor()\n        for node in level_nodes:\n            # Get the values for the input variables for this DataObject node.\n            print(f\"Running {self.mfunc_name} on {node.name}.\")\n            vr_values_in = {}\n            anc_nodes = nx.ancestors(subset_graph, node)\n            node_lineage = [node] + [anc_node for anc_node in anc_nodes]\n            for var_name_in_code, vr in self.input_vrs.items():\n                vr_found = False\n                if vr.hard_coded_value is not None:\n                    vr_values_in[var_name_in_code] = vr.hard_coded_value\n                    vr_found = True                \n                for curr_node in node_lineage:\n                    if hasattr(curr_node, vr.name):\n                        vr_values_in[var_name_in_code] = getattr(curr_node, vr.name)\n                        vr_found = True\n                        break\n                if not vr_found:\n                    raise ValueError(f\"Variable {vr.name} ({vr.id}) not found in __dict__ of {node}.\")\n\n            # Get the lineage so I can get the file path.\n            ordered_levels = []\n            for level in schema_order:\n                ordered_levels.append([n for n in node_lineage if isinstance(n, level)])\n            data_path = ds.dataset_path\n            for level in ordered_levels[1:]:\n                data_path = os.path.join(data_path, level[0].name)\n            # TODO: Make the name of this variable not hard-coded.\n            if \"c3dFilePath\" in vr_values_in:\n                vr_values_in[\"c3dFilePath\"] = data_path + \".c3d\"\n\n            # Check if the values for the input variables are up to date. If so, skip this node.\n            check_vr_values_in = {vr_name: vr_val for vr_name, vr_val in vr_values_in.items()}\n            run_process = False\n            for vr_name, vr_val in check_vr_values_in.items():\n                blob = pickle.dumps(vr_val)\n                hash_val = sha256(blob).hexdigest()\n\n                # Get the ID for this hash.\n                sqlquery = \"SELECT data_blob_id FROM data_values_blob WHERE data_hash = ?\"\n                result = cursor_data.execute(sqlquery, (hash_val,)).fetchall() \n                # If None, then do processing.\n                if len(result) == 0:\n                    run_process = True\n                    break               \n\n                 # If not None, check if it's the most recent.\n                data_blob_id = result[0][0]\n                sqlquery = \"SELECT action_id, data_blob_id FROM data_values WHERE dataobject_id = ? AND vr_id = ? AND schema_id = ? AND data_blob_id = ?\"\n                params = (node.id, self.input_vrs[vr_name].id, schema_id, data_blob_id)\n                result = cursor_data.execute(sqlquery, params).fetchall()\n                time_ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num = 0)\n                latest_data_blob_id = time_ordered_result[0][1]\n                if latest_data_blob_id != data_blob_id:\n                    run_process = True\n                    break\n\n            if not run_process:\n                print(f\"Skipping {node.name} ({node.id}).\")\n                continue\n\n            # NOTE: For now, assuming that there is only one return statement in the entire method.\n            if self.is_matlab:\n                vr_vals_in = list(vr_values_in.values())\n                fcn = getattr(eng, self.mfunc_name)                \n                vr_values_out = fcn(*vr_vals_in, nargout=len(self.output_vrs))\n            else:\n                vr_values_out = self.method(**vr_values_in) # Ensure that the method returns a tuple.\n            if not isinstance(vr_values_out, tuple):\n                vr_values_out = (vr_values_out,)\n            if len(vr_values_out) != len(self.output_vrs):\n                raise ValueError(\"The number of variables returned by the method must match the number of output variables registered with this Process instance.\")\n            if not all(vr in self.output_vrs for vr in output_var_names_in_code):\n                raise ValueError(\"All of the variable names returned by this method must have been previously registered with this Process instance.\")            \n\n            # Set the output variables for this DataObject node.\n            idx = -1 # For MATLAB. Requires that the args are in the proper order.\n            for vr_name, vr in self.output_vrs.items():\n                if not self.is_matlab:\n                    idx = output_var_names_in_code.index(vr_name) # Ensure I'm pulling the right VR name because the order of the VR's coming out, and the order in the output_vrs dict are probably different.\n                else:\n                    idx += 1                \n                self.__setattr__(node, vr_name, vr_values_out[idx], action = action)\n                print(f\"In {node.name} ({node.id}): Saved VR {vr_name} (VR: {vr.id}).\")\n\n        if self.is_matlab:\n            eng.rmpath(self.mfolder, nargout=0)   \n\n        pool.return_connection(conn)\n</code></pre> <p>To run a Process with process.run(): 1. Create a new Process object, with id specified as a kwarg. 2. Set the level of the Process object as a DataObject subclass. 3. If a MATLAB function is to be run, set self.is_matlab to True. 4. Set the input and output variables.     a. Use self.set_input_var() and self.set_output_var() to set the input and output variables using kwargs.     b. Alternatively, assign self.input_vrs and self.output_vrs directly as dicts with keys for the variable names in code, and the Variable objects are the values.     NOTE: Ensure that all of the variables are pre-existing, and that their vr.name are unique from all other variables in the pipeline.</p>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.from_json_input_vrs","title":"<code>from_json_input_vrs(input_vrs)</code>","text":"<p>Convert a JSON string to a dictionary of input variables.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def from_json_input_vrs(self, input_vrs: str) -&gt; dict:\n    \"\"\"Convert a JSON string to a dictionary of input variables.\"\"\"\n    input_vrs_dict = json.loads(input_vrs)\n    return {key: Variable(id = value) for key, value in input_vrs_dict.items()}\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.from_json_level","title":"<code>from_json_level(level)</code>","text":"<p>Convert a JSON string to a Process level.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def from_json_level(self, level: str) -&gt; type:\n    \"\"\"Convert a JSON string to a Process level.\"\"\"\n    classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n    for cls in classes:\n        if hasattr(cls, \"prefix\") and cls.prefix == level:\n            return cls\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.from_json_method","title":"<code>from_json_method(json_method)</code>","text":"<p>Convert a JSON string to a method. Returns None if the method name is not found (e.g. if code changed locations or something)</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def from_json_method(self, json_method: str) -&gt; Callable:\n    \"\"\"Convert a JSON string to a method.\n    Returns None if the method name is not found (e.g. if code changed locations or something)\"\"\"\n    method_name = json.loads(json_method)\n    module_name, *attribute_path = method_name.split(\".\")        \n    module = importlib.import_module(module_name)\n    attribute = module\n    for attr in attribute_path:\n        attribute = getattr(attribute, attr)\n    return attribute\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.from_json_output_vrs","title":"<code>from_json_output_vrs(output_vrs)</code>","text":"<p>Convert a JSON string to a dictionary of output variables.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def from_json_output_vrs(self, output_vrs: str) -&gt; dict:\n    \"\"\"Convert a JSON string to a dictionary of output variables.\"\"\"\n    output_vrs_dict = json.loads(output_vrs)\n    return {key: Variable(id = value) for key, value in output_vrs_dict.items()}\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.run","title":"<code>run()</code>","text":"<p>Execute the attached method. kwargs are the input VR's.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Execute the attached method.\n    kwargs are the input VR's.\"\"\"\n    ds = Dataset(id = self.get_dataset_id())\n    # 1. Validate that the level &amp; method have been properly set.\n    self.validate_method(self.method)\n    self.validate_level(self.level)\n\n    # TODO: Fix this to work with MATLAB.\n    if not self.is_matlab:\n        output_var_names_in_code = get_returned_variable_names(self.method)\n\n    # 2. Validate that the input &amp; output variables have been properly set.\n    self.validate_input_vrs(self.input_vrs)\n    self.validate_output_vrs(self.output_vrs)\n\n    # 3. Validate that the subsets have been properly set.\n    self.validate_subset_id(self.subset_id)\n\n    schema_id = self.get_current_schema_id(ds.id)\n\n    # 4. Run the method.\n    # Get the subset of the data.\n    subset_graph = Subset(id = self.subset_id).get_subset()\n\n    action = Action(name = f\"Running {self.mfunc_name} on {self.level.__name__}s.\")\n\n    # Do the setup for MATLAB.\n    if self.is_matlab:\n        eng.addpath(self.mfolder, nargout=0)\n\n    level_nodes = sorted([node for node in subset_graph if isinstance(node, self.level)], key = lambda x: x.name)\n    # Iterate over each data object at this level (e.g. all ros.Trial objects in the subset.)\n    schema = ds.schema\n    schema_graph = nx.MultiDiGraph(schema)\n    schema_order = list(nx.topological_sort(schema_graph))\n    pool_data = SQLiteConnectionPool(name = \"data\")\n    conn_data = pool_data.get_connection()\n    cursor_data = conn_data.cursor()\n    for node in level_nodes:\n        # Get the values for the input variables for this DataObject node.\n        print(f\"Running {self.mfunc_name} on {node.name}.\")\n        vr_values_in = {}\n        anc_nodes = nx.ancestors(subset_graph, node)\n        node_lineage = [node] + [anc_node for anc_node in anc_nodes]\n        for var_name_in_code, vr in self.input_vrs.items():\n            vr_found = False\n            if vr.hard_coded_value is not None:\n                vr_values_in[var_name_in_code] = vr.hard_coded_value\n                vr_found = True                \n            for curr_node in node_lineage:\n                if hasattr(curr_node, vr.name):\n                    vr_values_in[var_name_in_code] = getattr(curr_node, vr.name)\n                    vr_found = True\n                    break\n            if not vr_found:\n                raise ValueError(f\"Variable {vr.name} ({vr.id}) not found in __dict__ of {node}.\")\n\n        # Get the lineage so I can get the file path.\n        ordered_levels = []\n        for level in schema_order:\n            ordered_levels.append([n for n in node_lineage if isinstance(n, level)])\n        data_path = ds.dataset_path\n        for level in ordered_levels[1:]:\n            data_path = os.path.join(data_path, level[0].name)\n        # TODO: Make the name of this variable not hard-coded.\n        if \"c3dFilePath\" in vr_values_in:\n            vr_values_in[\"c3dFilePath\"] = data_path + \".c3d\"\n\n        # Check if the values for the input variables are up to date. If so, skip this node.\n        check_vr_values_in = {vr_name: vr_val for vr_name, vr_val in vr_values_in.items()}\n        run_process = False\n        for vr_name, vr_val in check_vr_values_in.items():\n            blob = pickle.dumps(vr_val)\n            hash_val = sha256(blob).hexdigest()\n\n            # Get the ID for this hash.\n            sqlquery = \"SELECT data_blob_id FROM data_values_blob WHERE data_hash = ?\"\n            result = cursor_data.execute(sqlquery, (hash_val,)).fetchall() \n            # If None, then do processing.\n            if len(result) == 0:\n                run_process = True\n                break               \n\n             # If not None, check if it's the most recent.\n            data_blob_id = result[0][0]\n            sqlquery = \"SELECT action_id, data_blob_id FROM data_values WHERE dataobject_id = ? AND vr_id = ? AND schema_id = ? AND data_blob_id = ?\"\n            params = (node.id, self.input_vrs[vr_name].id, schema_id, data_blob_id)\n            result = cursor_data.execute(sqlquery, params).fetchall()\n            time_ordered_result = ResearchObjectHandler._get_time_ordered_result(result, action_col_num = 0)\n            latest_data_blob_id = time_ordered_result[0][1]\n            if latest_data_blob_id != data_blob_id:\n                run_process = True\n                break\n\n        if not run_process:\n            print(f\"Skipping {node.name} ({node.id}).\")\n            continue\n\n        # NOTE: For now, assuming that there is only one return statement in the entire method.\n        if self.is_matlab:\n            vr_vals_in = list(vr_values_in.values())\n            fcn = getattr(eng, self.mfunc_name)                \n            vr_values_out = fcn(*vr_vals_in, nargout=len(self.output_vrs))\n        else:\n            vr_values_out = self.method(**vr_values_in) # Ensure that the method returns a tuple.\n        if not isinstance(vr_values_out, tuple):\n            vr_values_out = (vr_values_out,)\n        if len(vr_values_out) != len(self.output_vrs):\n            raise ValueError(\"The number of variables returned by the method must match the number of output variables registered with this Process instance.\")\n        if not all(vr in self.output_vrs for vr in output_var_names_in_code):\n            raise ValueError(\"All of the variable names returned by this method must have been previously registered with this Process instance.\")            \n\n        # Set the output variables for this DataObject node.\n        idx = -1 # For MATLAB. Requires that the args are in the proper order.\n        for vr_name, vr in self.output_vrs.items():\n            if not self.is_matlab:\n                idx = output_var_names_in_code.index(vr_name) # Ensure I'm pulling the right VR name because the order of the VR's coming out, and the order in the output_vrs dict are probably different.\n            else:\n                idx += 1                \n            self.__setattr__(node, vr_name, vr_values_out[idx], action = action)\n            print(f\"In {node.name} ({node.id}): Saved VR {vr_name} (VR: {vr.id}).\")\n\n    if self.is_matlab:\n        eng.rmpath(self.mfolder, nargout=0)   \n\n    pool.return_connection(conn)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.set_input_vrs","title":"<code>set_input_vrs(**kwargs)</code>","text":"<p>Convenience function to set the input variables with named variables rather than a dict.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def set_input_vrs(self, **kwargs) -&gt; None:\n    \"\"\"Convenience function to set the input variables with named variables rather than a dict.\"\"\"\n    action = Action(name = \"set_input_vrs\")\n    self.__setattr__(\"input_vrs\", kwargs, action = action)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.set_output_vrs","title":"<code>set_output_vrs(**kwargs)</code>","text":"<p>Convenience function to set the output variables with named variables rather than a dict.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def set_output_vrs(self, **kwargs) -&gt; None:\n    \"\"\"Convenience function to set the output variables with named variables rather than a dict.\"\"\"\n    action = Action(name = \"set_output_vrs\")\n    self.__setattr__(\"output_vrs\", kwargs, action = action)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.to_json_input_vrs","title":"<code>to_json_input_vrs(input_vrs)</code>","text":"<p>Convert a dictionary of input variables to a JSON string.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def to_json_input_vrs(self, input_vrs: dict) -&gt; str:\n    \"\"\"Convert a dictionary of input variables to a JSON string.\"\"\"     \n    return json.dumps({key: value.id for key, value in input_vrs.items()})\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.to_json_level","title":"<code>to_json_level(level)</code>","text":"<p>Convert a Process level to a JSON string.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def to_json_level(self, level: type) -&gt; str:\n    \"\"\"Convert a Process level to a JSON string.\"\"\"\n    return json.dumps(level.prefix)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.to_json_method","title":"<code>to_json_method(method)</code>","text":"<p>Convert a method to a JSON string.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def to_json_method(self, method: Callable) -&gt; str:\n    \"\"\"Convert a method to a JSON string.\"\"\"\n    if method is None:\n        return json.dumps(None)\n    return json.dumps(method.__module__ + \".\" + method.__qualname__)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.to_json_output_vrs","title":"<code>to_json_output_vrs(output_vrs)</code>","text":"<p>Convert a dictionary of output variables to a JSON string.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def to_json_output_vrs(self, output_vrs: dict) -&gt; str:\n    \"\"\"Convert a dictionary of output variables to a JSON string.\"\"\"\n    return json.dumps({key: value.id for key, value in output_vrs.items()})\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.validate_input_vrs","title":"<code>validate_input_vrs(inputs)</code>","text":"<p>Validate that the input variables are correct.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def validate_input_vrs(self, inputs: dict) -&gt; None:\n    \"\"\"Validate that the input variables are correct.\"\"\"        \n    if not self.is_matlab:\n        input_vr_names_in_code = get_input_variable_names(self.method)\n    self._validate_vrs(inputs, input_vr_names_in_code)\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.validate_output_vrs","title":"<code>validate_output_vrs(outputs)</code>","text":"<p>Validate that the output variables are correct.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def validate_output_vrs(self, outputs: dict) -&gt; None:\n    \"\"\"Validate that the output variables are correct.\"\"\"        \n    if not self.is_matlab:\n        output_vr_names_in_code = get_returned_variable_names(self.method)\n    self._validate_vrs(outputs, output_vr_names_in_code)    \n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/process/#src.ResearchOS.PipelineObjects.process.Process.validate_subset_id","title":"<code>validate_subset_id(subset_id)</code>","text":"<p>Validate that the subset ID is correct.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\process.py</code> <pre><code>def validate_subset_id(self, subset_id: str) -&gt; None:\n    \"\"\"Validate that the subset ID is correct.\"\"\"\n    if not ResearchObjectHandler.object_exists(subset_id):\n        raise ValueError(\"Subset ID must reference an existing Subset.\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/","title":"Project","text":"<p>Inherits from PipelineObject</p> <p>             Bases: <code>PipelineObject</code></p> <p>A project is a collection of analyses. Class-specific Attributes: 1. current_analysis_id: The ID of the current analysis for this project. 2. current_dataset_id: The ID of the current dataset for this project. 3. project path: The root folder location of the project.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\project.py</code> <pre><code>class Project(PipelineObject):\n    \"\"\"A project is a collection of analyses.\n    Class-specific Attributes:\n    1. current_analysis_id: The ID of the current analysis for this project.\n    2. current_dataset_id: The ID of the current dataset for this project.\n    3. project path: The root folder location of the project.\"\"\"\n\n    prefix: str = \"PJ\"\n\n    ## current_analysis_id\n\n    def validate_current_analysis_id(self, id: str) -&gt; None:\n        \"\"\"Validate the current analysis ID. If it is not valid, the value is rejected.\"\"\"        \n        if not isinstance(id, str):\n            raise ValueError(\"Specified value is not a string!\")\n        if not self.is_id(id):\n            raise ValueError(\"Specified value is not an ID!\")\n        parsed_id = self.parse_id(id)\n        if parsed_id[0] != \"AN\":\n            raise ValueError(\"Specified ID is not an Analysis!\")\n        if not self.object_exists(id):\n            raise ValueError(\"Analysis does not exist!\")\n\n    ## current_dataset_id\n\n    def validate_current_dataset_id(self, id: str) -&gt; None:\n        \"\"\"Validate the current dataset ID. If it is not valid, the value is rejected.\"\"\"\n        if not self.is_id(id):\n            raise ValueError(\"Specified value is not an ID!\")\n        parsed_id = self.parse_id(id)\n        if parsed_id[0] != \"DS\":\n            raise ValueError(\"Specified ID is not a Dataset!\")\n        if not self.object_exists(id):\n            raise ValueError(\"Dataset does not exist!\")\n\n    ## project_path\n\n    def validate_project_path(self, path: str) -&gt; None:\n        \"\"\"Validate the project path. If it is not valid, the value is rejected.\"\"\"\n        # 1. Check that the path exists in the file system.\n        import os\n        if not isinstance(path, str):\n            raise ValueError(\"Specified path is not a string!\")\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path is not a path or does not currently exist!\")        \n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.validate_current_analysis_id","title":"<code>validate_current_analysis_id(id)</code>","text":"<p>Validate the current analysis ID. If it is not valid, the value is rejected.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\project.py</code> <pre><code>def validate_current_analysis_id(self, id: str) -&gt; None:\n    \"\"\"Validate the current analysis ID. If it is not valid, the value is rejected.\"\"\"        \n    if not isinstance(id, str):\n        raise ValueError(\"Specified value is not a string!\")\n    if not self.is_id(id):\n        raise ValueError(\"Specified value is not an ID!\")\n    parsed_id = self.parse_id(id)\n    if parsed_id[0] != \"AN\":\n        raise ValueError(\"Specified ID is not an Analysis!\")\n    if not self.object_exists(id):\n        raise ValueError(\"Analysis does not exist!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.validate_current_dataset_id","title":"<code>validate_current_dataset_id(id)</code>","text":"<p>Validate the current dataset ID. If it is not valid, the value is rejected.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\project.py</code> <pre><code>def validate_current_dataset_id(self, id: str) -&gt; None:\n    \"\"\"Validate the current dataset ID. If it is not valid, the value is rejected.\"\"\"\n    if not self.is_id(id):\n        raise ValueError(\"Specified value is not an ID!\")\n    parsed_id = self.parse_id(id)\n    if parsed_id[0] != \"DS\":\n        raise ValueError(\"Specified ID is not a Dataset!\")\n    if not self.object_exists(id):\n        raise ValueError(\"Dataset does not exist!\")\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/project/#src.ResearchOS.PipelineObjects.project.Project.validate_project_path","title":"<code>validate_project_path(path)</code>","text":"<p>Validate the project path. If it is not valid, the value is rejected.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\project.py</code> <pre><code>def validate_project_path(self, path: str) -&gt; None:\n    \"\"\"Validate the project path. If it is not valid, the value is rejected.\"\"\"\n    # 1. Check that the path exists in the file system.\n    import os\n    if not isinstance(path, str):\n        raise ValueError(\"Specified path is not a string!\")\n    if not os.path.exists(path):\n        raise ValueError(\"Specified path is not a path or does not currently exist!\")        \n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/","title":"Subset","text":"<p>Inherits from PipelineObject</p> <p>             Bases: <code>PipelineObject</code></p> <p>Provides rules to select a subset of data from a dataset.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\subset.py</code> <pre><code>class Subset(PipelineObject):\n    \"\"\"Provides rules to select a subset of data from a dataset.\"\"\"\n\n    prefix = \"SS\"\n\n    ## conditions\n\n    def validate_conditions(self, conditions: dict) -&gt; None:\n        \"\"\"Validate the condition recursively.\n        Example usage:\n        conditions = {\n            \"and\": [\n                [vr1.id, \"&lt;\", 4],\n                {\n                    \"or\": [\n                        [vr1.id, \"&gt;\", 2],\n                        [vr1.id, \"=\", 7]\n                    ]\n                }\n            ]\n        }\n        \"\"\"        \n        # Validate a single condition.\n        if isinstance(conditions, list):\n            if len(conditions) != 3:\n                raise ValueError(\"Condition must be a list of length 3.\")\n            if not IDCreator().is_ro_id(conditions[0]):\n                raise ValueError(\"Variable ID must be a valid Variable ID.\")\n            if not ResearchObjectHandler.object_exists(conditions[0]):\n                raise ValueError(\"Variable must be pre-existing.\")\n            if conditions[1] not in logic_options:\n                raise ValueError(\"Invalid logic.\")\n            if conditions[1] in numeric_logic_options and not isinstance(conditions[2], int):\n                raise ValueError(\"Numeric logical symbols must have an int value.\")\n            try:\n                a = json.dumps(conditions[2])\n            except:\n                raise ValueError(\"Value must be JSON serializable.\")\n            return\n\n        # Validate the \"and\"/\"or\" keys.\n        if not isinstance(conditions, dict):\n            raise ValueError(\"Condition must be a dict.\")\n        if \"and\" not in conditions and \"or\" not in conditions:\n            raise ValueError(\"Condition must contain an 'and' or 'or' key.\")\n        if \"and\" in conditions and \"or\" in conditions:\n            raise ValueError(\"Condition cannot contain both 'and' and 'or' keys.\")\n\n        for key, value in conditions.items():\n            if key not in (\"and\", \"or\"):\n                raise ValueError(\"Invalid key in condition.\")\n            if not isinstance(value, list):\n                raise ValueError(\"Value must be a list.\")\n            if not isinstance(value, (list, dict)):\n                raise ValueError(\"Value must be a list of lists or dicts.\")\n            a = [self.validate_conditions(cond) for cond in value] # Assigned to a just to make interpreter happy.\n\n    def get_subset(self) -&gt; nx.MultiDiGraph:\n        \"\"\"Resolve the conditions to the actual subset of data.\"\"\"\n        # 1. Get the dataset.\n        ds = Dataset(id = self.get_dataset_id())\n\n        # 2. For each node in the address_graph, check if it meets the conditions.\n        nodes_for_subgraph = []\n        G = ds.address_graph\n        sorted_nodes = list(nx.topological_sort(G))\n        for idx, node in enumerate(sorted_nodes):\n            # ro = ResearchObjectHandler._prefix_to_class(node.prefix)(id = node.id)\n            if not self.meets_conditions(node, self.conditions, G):\n                continue\n            curr_nodes = [node]\n            curr_nodes.extend(nx.ancestors(G, node))\n            nodes_for_subgraph.extend([node for node in curr_nodes if node not in nodes_for_subgraph])\n        return G.subgraph(nodes_for_subgraph) # Maintains the relationships between all of the nodes in the subgraph.\n\n    def meets_conditions(self, node: Any, conditions: dict, G: nx.MultiDiGraph) -&gt; bool:\n        \"\"\"Check if the node meets the conditions.\"\"\"\n        if isinstance(conditions, dict):\n            if \"and\" in conditions:\n                return all([self.meets_conditions(node, cond, G) for cond in conditions[\"and\"]])\n            if \"or\" in conditions:\n                return any([self.meets_conditions(node, cond, G) for cond in conditions[\"or\"]])\n\n        # Check the condition.\n        vr_id = conditions[0]\n        logic = conditions[1]\n        value = conditions[2]\n        vr = Variable(id = vr_id)\n        vr_name = vr.name\n        if not hasattr(node, vr_name):\n            anc_nodes = nx.ancestors(G, node)\n            found_attr = False\n            for anc_node in anc_nodes:\n                if not hasattr(anc_node, vr_name):\n                    continue\n                found_attr = True\n                break\n            if found_attr and self.meets_conditions(anc_node, conditions, G):\n                return True\n            return False\n        vr_value = getattr(node, vr_name)\n\n        # This is probably shoddy logic, but it'll serve as a first pass to handle None types.\n        if logic in plural_logic:\n            if logic == \"contains\" and vr_value is None:\n                return False\n            elif logic == \"not contains\" and vr_value is None and value is not None:                \n                return True\n            elif logic == \"in\" and value is None:\n                return False\n            elif logic == \"not in\" and value is None:\n                return True\n\n        # Numeric\n        bool_val = False\n        if logic == \"&gt;\" and vr_value &gt; value:\n            bool_val = True\n        elif logic == \"&lt;\" and vr_value &lt; value:\n            bool_val = True\n        elif logic == \"&gt;=\" and vr_value &gt;= value:\n            bool_val = True\n        elif logic == \"&lt;=\" and vr_value &lt;= value:\n            bool_val = True\n        # Any type\n        elif logic in [\"==\",\"=\"] and vr_value == value:\n            bool_val = True\n        elif logic == \"!=\" and vr_value != value:\n            bool_val = True\n        elif logic == \"in\" and vr_value in value:\n            bool_val = True\n        elif logic == \"not in\" and vr_value not in value:\n            bool_val = True\n        elif logic == \"is\" and vr_value is value:\n            bool_val = True\n        elif logic == \"is not\" and vr_value is not value:\n            bool_val = True\n        elif logic == \"contains\" and value in vr_value:\n            bool_val = True\n        elif logic == \"not contains\" and not value in vr_value:\n            bool_val = True\n\n        return bool_val\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.get_subset","title":"<code>get_subset()</code>","text":"<p>Resolve the conditions to the actual subset of data.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\subset.py</code> <pre><code>def get_subset(self) -&gt; nx.MultiDiGraph:\n    \"\"\"Resolve the conditions to the actual subset of data.\"\"\"\n    # 1. Get the dataset.\n    ds = Dataset(id = self.get_dataset_id())\n\n    # 2. For each node in the address_graph, check if it meets the conditions.\n    nodes_for_subgraph = []\n    G = ds.address_graph\n    sorted_nodes = list(nx.topological_sort(G))\n    for idx, node in enumerate(sorted_nodes):\n        # ro = ResearchObjectHandler._prefix_to_class(node.prefix)(id = node.id)\n        if not self.meets_conditions(node, self.conditions, G):\n            continue\n        curr_nodes = [node]\n        curr_nodes.extend(nx.ancestors(G, node))\n        nodes_for_subgraph.extend([node for node in curr_nodes if node not in nodes_for_subgraph])\n    return G.subgraph(nodes_for_subgraph) # Maintains the relationships between all of the nodes in the subgraph.\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.meets_conditions","title":"<code>meets_conditions(node, conditions, G)</code>","text":"<p>Check if the node meets the conditions.</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\subset.py</code> <pre><code>def meets_conditions(self, node: Any, conditions: dict, G: nx.MultiDiGraph) -&gt; bool:\n    \"\"\"Check if the node meets the conditions.\"\"\"\n    if isinstance(conditions, dict):\n        if \"and\" in conditions:\n            return all([self.meets_conditions(node, cond, G) for cond in conditions[\"and\"]])\n        if \"or\" in conditions:\n            return any([self.meets_conditions(node, cond, G) for cond in conditions[\"or\"]])\n\n    # Check the condition.\n    vr_id = conditions[0]\n    logic = conditions[1]\n    value = conditions[2]\n    vr = Variable(id = vr_id)\n    vr_name = vr.name\n    if not hasattr(node, vr_name):\n        anc_nodes = nx.ancestors(G, node)\n        found_attr = False\n        for anc_node in anc_nodes:\n            if not hasattr(anc_node, vr_name):\n                continue\n            found_attr = True\n            break\n        if found_attr and self.meets_conditions(anc_node, conditions, G):\n            return True\n        return False\n    vr_value = getattr(node, vr_name)\n\n    # This is probably shoddy logic, but it'll serve as a first pass to handle None types.\n    if logic in plural_logic:\n        if logic == \"contains\" and vr_value is None:\n            return False\n        elif logic == \"not contains\" and vr_value is None and value is not None:                \n            return True\n        elif logic == \"in\" and value is None:\n            return False\n        elif logic == \"not in\" and value is None:\n            return True\n\n    # Numeric\n    bool_val = False\n    if logic == \"&gt;\" and vr_value &gt; value:\n        bool_val = True\n    elif logic == \"&lt;\" and vr_value &lt; value:\n        bool_val = True\n    elif logic == \"&gt;=\" and vr_value &gt;= value:\n        bool_val = True\n    elif logic == \"&lt;=\" and vr_value &lt;= value:\n        bool_val = True\n    # Any type\n    elif logic in [\"==\",\"=\"] and vr_value == value:\n        bool_val = True\n    elif logic == \"!=\" and vr_value != value:\n        bool_val = True\n    elif logic == \"in\" and vr_value in value:\n        bool_val = True\n    elif logic == \"not in\" and vr_value not in value:\n        bool_val = True\n    elif logic == \"is\" and vr_value is value:\n        bool_val = True\n    elif logic == \"is not\" and vr_value is not value:\n        bool_val = True\n    elif logic == \"contains\" and value in vr_value:\n        bool_val = True\n    elif logic == \"not contains\" and not value in vr_value:\n        bool_val = True\n\n    return bool_val\n</code></pre>"},{"location":"Research%20Object%20Types/Pipeline%20Object%20Types/subset/#src.ResearchOS.PipelineObjects.subset.Subset.validate_conditions","title":"<code>validate_conditions(conditions)</code>","text":"<p>Validate the condition recursively. Example usage: conditions = {     \"and\": [         [vr1.id, \"&lt;\", 4],         {             \"or\": [                 [vr1.id, \"&gt;\", 2],                 [vr1.id, \"=\", 7]             ]         }     ] }</p> Source code in <code>src\\ResearchOS\\PipelineObjects\\subset.py</code> <pre><code>def validate_conditions(self, conditions: dict) -&gt; None:\n    \"\"\"Validate the condition recursively.\n    Example usage:\n    conditions = {\n        \"and\": [\n            [vr1.id, \"&lt;\", 4],\n            {\n                \"or\": [\n                    [vr1.id, \"&gt;\", 2],\n                    [vr1.id, \"=\", 7]\n                ]\n            }\n        ]\n    }\n    \"\"\"        \n    # Validate a single condition.\n    if isinstance(conditions, list):\n        if len(conditions) != 3:\n            raise ValueError(\"Condition must be a list of length 3.\")\n        if not IDCreator().is_ro_id(conditions[0]):\n            raise ValueError(\"Variable ID must be a valid Variable ID.\")\n        if not ResearchObjectHandler.object_exists(conditions[0]):\n            raise ValueError(\"Variable must be pre-existing.\")\n        if conditions[1] not in logic_options:\n            raise ValueError(\"Invalid logic.\")\n        if conditions[1] in numeric_logic_options and not isinstance(conditions[2], int):\n            raise ValueError(\"Numeric logical symbols must have an int value.\")\n        try:\n            a = json.dumps(conditions[2])\n        except:\n            raise ValueError(\"Value must be JSON serializable.\")\n        return\n\n    # Validate the \"and\"/\"or\" keys.\n    if not isinstance(conditions, dict):\n        raise ValueError(\"Condition must be a dict.\")\n    if \"and\" not in conditions and \"or\" not in conditions:\n        raise ValueError(\"Condition must contain an 'and' or 'or' key.\")\n    if \"and\" in conditions and \"or\" in conditions:\n        raise ValueError(\"Condition cannot contain both 'and' and 'or' keys.\")\n\n    for key, value in conditions.items():\n        if key not in (\"and\", \"or\"):\n            raise ValueError(\"Invalid key in condition.\")\n        if not isinstance(value, list):\n            raise ValueError(\"Value must be a list.\")\n        if not isinstance(value, (list, dict)):\n            raise ValueError(\"Value must be a list of lists or dicts.\")\n        a = [self.validate_conditions(cond) for cond in value] # Assigned to a just to make interpreter happy.\n</code></pre>"}]}
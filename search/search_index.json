{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Developer/","title":"Overview","text":"<p>This is the internal developer documentation.</p> <p>Every project created using this software is output as a <code>pip install</code>able package by default using <code>&gt;ros create &lt;name&gt;</code>.</p> <p>This software is intended for anyone who works with data, from researchers to developers to data scientists. It aims to facilitate sharing code with others, and utilize others' code.</p> <p>There are three main ways that this software is intended to be used:</p> <ol> <li> <p>As a researcher coding your own data analysis, developing new algorithms or data visualizations. Your project, which includes code and data, is organized into a package.</p> </li> <li> <p>As a developer creating a new package for others to use. Typically, there is little or no data associated with these packages.</p> </li> <li> <p>As the project matures and packages that provide common methodologies come to be commonly used, many or most users may simply be consumers of pre-existing pipelines, utilizing their own or others' data, and uniquely combining packages into new analyses, which in turn are packaged to be shared with others.</p> </li> </ol>"},{"location":"Developer/#developer-index","title":"Developer Index","text":""},{"location":"Developer/#packages","title":"Packages","text":"<p>Describes how the package dependencies are specified and organized.</p>"},{"location":"Developer/#compilation","title":"Compilation","text":"<p>Describes how the Directed Acyclic Graph (DAG) is compiled from the .toml settings files across packages.</p>"},{"location":"Developer/#running","title":"Running","text":"<p>Describes the process of running the code on the specified dataset.</p>"},{"location":"Developer/Bridges/","title":"Overview","text":"<p>Bridges are the mechanism by which packages that have been built independently can be connected to one another to form a cohesive data processing pipeline. They are defined in the <code>bridges.toml</code> file in each package's folder. A missing <code>bridges.toml</code> file in a package indicates that by default that package does not depend on any other packages. The value of any Unspecified variables must then be listed in each project's <code>bridges.toml</code> file.</p> <p>For example, a simple package to compute, plot, and summarize linear momentum (p) given a velocity (v) and mass (m) (<code>p = m * v</code>) may be missing a <code>bridges.toml</code> file entirely, because the package's computations are self-contained. When this package is then used in a data analysis project, that project's <code>bridges.toml</code> file will specify which variables from the other functions in the pipeline should be used as the inputs to linear momentum computations.</p> <p>In the <code>bridges.toml</code> file, often there are multiple outputs that are all connected to the same input of another function. When there are multiple output variables connecting to one input variable, that indicates that the DAG must be branched into separate lineages. For example, if linear momentum and angular momentum both are inputs to a function to compute summary statistics, then the DAG will split into two separate lineages for all downstream dependencies, one for linear momentum and one for angular momentum. This can be repeated recursively for as many branches as necessary.</p> <p>Another case in which branching can be specified is when a Constant is provided as a list. ResearchOS will branch the DAG beginning at the input variable of interest, creating a separate lineage for each value in the list. This is useful for running multiple versions of the same analysis, for example, with different parameters.</p>"},{"location":"Developer/Bridges/bridges_toml/","title":"Bridges.toml","text":"<p>Describes the structure of the <code>bridges.toml</code> file, which is used to configure bridges. This file is used to connect packages together in a data processing pipeline. </p> <p>Note that even variables that already have values specified in an earlier <code>bridges.toml</code> file will be overwritten if specified here.</p>"},{"location":"Developer/Bridges/bridges_toml/#dynamic-variables","title":"Dynamic Variables","text":"<p>Dynamic variables should be specified using \"input\" format, with multiple source variables and one target variable. The below example bridge shows two source variables connecting to a target variable. This means that the downstream pipeline will be split, with one branch for each source variable.</p> <pre><code>[example_bridge_name]\nsources = [\n    \"source_package_name1.source_process_name1.source_variable_name1\",\n    \"source_package_name2.source_process_name2.source_variable_name2\",\n]\n\ntargets = [\n    \"target_package_name1.target_process_name1.target_variable_name1\",\n]\n</code></pre>"},{"location":"Developer/Bridges/bridges_toml/#hard-coded-variables","title":"Hard-Coded Variables","text":"<p>Hard-coded variables should also be specified with the \"input\" format. Note that the sources are simply hard-coded values. If a list of multiple values are provided, then the downstream pipeline will be split, with one branch for each value. <pre><code>[example_hard_coded_bridge_name]\nsources = [\n    0, 1, 2,\n]\ntargets = [\n    \"target_package_name1.target_process_name1.target_variable_name1\",\n]\n</code></pre></p> <p>If a list is intended to be the input, then encapsulate the list with additional brackets. The below example provides two versions of the same input: a list, and a scalar. <pre><code>[example_hard_coded_bridge_name]\nsources = [\n    [0, 1, 2],\n    3\n]\ntargets = [\n    \"target_package_name1.target_process_name1.target_variable_name1\",\n]\n</code></pre></p>"},{"location":"Developer/Compilation/","title":"Overview","text":"<p>Compilation is the process of reading each package's <code>pyproject.toml</code> file and constructing a Directed Acylic Graph (DAG) that represents the flow of data in the data processing pipeline.</p> <p>Compilation consist of two steps. Step one (Connectivity) constructs the shape of the DAG, relying only on the inputs, outputs, and bridges of each package. Step two (Running) applies all of the metadata to the nodes of the DAG.</p>"},{"location":"Developer/Compilation/connectivity/","title":"Connectivity","text":"<p>To construct the Directed Acyclic Graph (DAG), the following steps are performed within the <code>compile_packages_to_dag()</code> function:</p> <ul> <li> <p>The project name is obtained from the <code>pyproject.toml</code> file's <code>[\"project\"][\"name\"]</code> key.</p> </li> <li> <p>The logsheet metadata is obtained from the project's <code>package-settings.toml</code> file. <code>[\"outputs\"]</code> key is added to the logsheet, populated based on the <code>logsheet.headers</code> key from that file.</p> </li> <li> <p>The rest of the package's settings are obtained from the <code>package-settings.toml</code> file: <code>DATASET_FILE_SCHEMA</code>, <code>DATASET_SCHEMA</code>, <code>SAVE_DATA_FOLDER</code>, <code>RAW_DATA_FOLDER</code>.</p> </li> <li> <p>Get the package names from the all the folders within the <code>project_folder</code> and <code>packages_parent_folders</code> that begin with <code>ros-</code>.</p> </li> </ul> <p>Warning</p> <p>It is possible that <code>pyproject.toml</code> field and the folder name do not match, which could cause issues. </p> <p>In the future this discovery method should use the <code>pyproject.toml</code> files so that it is more robust and explicit, and uses the <code>pyproject.toml</code> file as the single source of truth.</p> <ul> <li> <p>Create the DAG for each individual package. The following types of nodes are added here, with connectivity-related metadata only: Runnables, Input Variables, and Output Variables. Minimum specifications for this step in the <code>.toml</code> files are:</p> <ul> <li>All Runnables:<ul> <li>\"inputs\": At least one input variable specified.</li> <li>Process:<ul> <li>\"outputs\": Can be an empty list, or a list of output variables.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Get the bridges listed for each package from the package's <code>bridges.toml</code> file. The bridges are used to connect the packages together, and are applied in the topological order of the packages.</p> <ul> <li>This process converts Unspecified Input Variables to Dynamic Input Variables if a bridge exists for them. If not, they remain Unspecified.</li> </ul> </li> </ul>"},{"location":"Developer/Compilation/running/","title":"Running","text":"<p>The second step in compilation consists of adding all of the information needed for running each function.</p> <p>Note</p> <p>This page is a work in progress. The information here is incomplete and may be inaccurate.</p>"},{"location":"Developer/DAG/","title":"Directional Acyclic Graph (DAG)","text":"<p>The Directed Acyclic Graph (DAG) is a NetworkX MultiDiGraph that represents the dependencies between the various tasks in a project. The DAG is created by the <code>ros compile</code> command, which reads the <code>.toml</code> files in the project and creates a graph of the tasks and their dependencies. The DAG is then used to determine the order in which the tasks should be run, and whether a task needs to be re-run based on the inputs and outputs of the tasks.</p> <p>The DAG is structured such that the index of each node is a UUID from Python's <code>uuid</code> module. This allows for easy identification of nodes and edges, and for easy serialization and deserialization of the DAG. Within each node is a \"node\" attribute containing an instance of either a <code>Runnable</code> or a <code>Variable</code> class.</p> <p>Each node of the DAG represents either:</p> <ol> <li> <p>A Runnable: A task that can be run. May be a Process, a Plot, or a Stats task.</p> </li> <li> <p>A Variable: A node that connects to or from a Runnable, representing an input or output variable of the Runnable. Also connects to other Variables to form the dependencies between Runnables.</p> </li> </ol> <p>Only the nodes in the DAG contain metadata needed to run the tasks and pass data between them. The edges indicate connections between the nodes, but do not contain any metadata.</p>"},{"location":"Developer/DAG/#hashing","title":"Hashing","text":"<p>Each node can be hashed in either a \"static\" or \"concrete\" fashion, to borrow nomenclature from the OOP world. The static hash is used to uniquely identify the node in the DAG, while the concrete hash is used to uniquely identify the node for specific data objects. The Weisfeiler Lehman Subgraph Hash algorithm from NetworkX is used to uniquely hash each node in the DAG based on its \"node\" attribute. Each subclass of <code>Runnable</code> and <code>Variable</code> has implemented a <code>__hash__</code> method that returns a unique hash for that node based on its metadata for <code>static</code> and <code>concrete</code> hashing.</p> <p>Todo</p> <p>OPEN QUESTION: How do I cache the concrete DAG? I cant very well load every Variable from every data object to hash all of them every time I want to run a task. I need to cache the concrete DAG somehow.</p> <p>Mirroring this concept of \"static\" and \"concrete\" hashes, the most recently ran version of the DAG itself will also be stored in two ways: a \"static\" version stored in the \"static_dag.toml\" file and a \"concrete_dag.toml\" file in the root of the saved data's directory. The \"static\" DAG is used to determine the order in which tasks should be run, while the \"concrete\" DAG is used to determine if a task needs to be re-run based on the data objects' hashes.</p>"},{"location":"Developer/DAG/#static-dag","title":"Static DAG","text":"<p>A dict of dicts with three levels of nesting, where the keys to the outer dicts are the static hashes of the runnable nodes. The keys to the inner dicts are the static hashes of the runnable nodes that the outer node depends on. Finally, the keys to the innermost dicts are the variable names for the task, and the values are the static hashes of the variables for that data object.</p> <pre><code>[&lt;runnable_static_hash&gt;]\n&lt;variable_name&gt; = &lt;variable_static_hash&gt;\n</code></pre>"},{"location":"Developer/DAG/#concrete-dag","title":"Concrete DAG","text":"<p>A dict of dicts with three levels of nesting, where the keys to the outer dicts are the data object full names. The keys to the inner dicts are the static hashes of the runnable nodes. Finally, the keys to the innermost dicts are variable names for the task, and the values are the concrete hashes of the variables for that data object. NOTE: The edges between nodes are not represented here, as that information is contained within the static hashes.</p> <pre><code>[&lt;data_object_full_name&gt;.&lt;runnable_static_hash&gt;]\n&lt;runnable_name&gt; = &lt;runnable_full_name&gt;\n&lt;variable_name&gt; = &lt;variable_concrete_hash&gt;\n</code></pre>"},{"location":"Developer/DAG/#runnable-nodes","title":"Runnable Nodes","text":""},{"location":"Developer/DAG/#naming-convention","title":"Naming Convention","text":"<p>A Runnable node's full name is <code>package_name.task_name</code>. Often (but not required) the <code>task_name</code> is similar to the name of the function to be run by that task.</p> <p>Tip</p> <p>For simplicity, a Runnable node may in certain circumstances be referred to only by its <code>task_name</code> in the <code>.toml</code> files. In that case, the <code>package_name</code> is assumed to be the package defined by the root folder's <code>package.toml</code> file.</p> <p>For example, if <code>root/pyproject.toml</code> defines package <code>package1</code>, then a task within that package may be referred to as simply <code>task1</code>, and we know it is referring to <code>package1.task1</code>.</p>"},{"location":"Developer/DAG/#metadata","title":"Metadata","text":"<p>Each Runnable node contains metadata that is used to run the task. This metadata is stored in the <code>.toml</code> file for the task, and also in the DAG node. The metadata includes:</p> <ul> <li>full_name: The full name of the task, as described above.</li> <li>subset: The subset of data to run the task on.</li> <li>level: The level of data object to run the task on.</li> <li>batch: The batch of data objects to run the task on.</li> </ul>"},{"location":"Developer/DAG/#static-hash","title":"Static Hash","text":"<p>Metadata included in the static hash:</p> <ul> <li>The full name of the task</li> <li>The subset of data to run the task on</li> <li>A dictionary of input variables, where the keys are the input names and the values are the corresponding Variable nodes' full names.</li> <li>A dictionary of output variables, where the keys are the output names and the values are the corresponding Variable nodes' full names.</li> <li>NOTE: A task's static hash is added to the record as having been successfully completed after the task has been successfully run over all data objects in the subset. If any of the above metadata changes after that, the task will be re-run.</li> </ul>"},{"location":"Developer/DAG/#concrete-hash","title":"Concrete Hash","text":"<p>Metadata included in the concrete hash:</p> <ul> <li>The static hash of the task</li> <li>A dictionary of input variables, where the keys are the input names and the values are the corresponding Variable nodes' concrete hashes.</li> </ul>"},{"location":"Developer/DAG/#variable-nodes","title":"Variable Nodes","text":""},{"location":"Developer/DAG/#naming-convention_1","title":"Naming Convention","text":"<p>A Variable node's full name is <code>package_name.task_name.variable_name</code>. The <code>variable_name</code> is the name of the input or output variable of the Runnable.</p> <p>Warning</p> <p>This naming convention does not distinguish between input and output variables. Therefore, a particular task's .toml file can not contain an input and output variable with the same name.</p> <p>If \"data\" is an input variable, then \"data\" cannot be an output variable of the same task. We suggest using more specific variable names to avoid this issue.</p>"},{"location":"Developer/DAG/#metadata_1","title":"Metadata","text":"<p>Each Variable node contains metadata that is used to pass data between tasks. This metadata is stored in the <code>.toml</code> file for the task and in the <code>bridges.toml</code> file(s), and also in the DAG node. The metadata includes:</p> <ul> <li>full_name: The full name of the variable, as described above.</li> </ul>"},{"location":"Developer/DAG/#static-hash_1","title":"Static Hash","text":"<p>Metadata included in the static hash is below. Exactly what is being hashed depends on the type of Variable (hardcoded, dynamic input, data object attribute, output, etc.):</p> <ul> <li>For hardcoded Variables: The variable value</li> <li>For dynamic input Variables: The Variable's full name</li> <li>For data object attributes: The dict defining which attribute of the data object to use</li> <li>For paths: The contents of the file at the path. If that file does not exist or was not found, then the hash is the hash of an empty string \"\"<ul> <li>\"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"</li> </ul> </li> <li>For output Variables: The Variable's full name</li> </ul>"},{"location":"Developer/DAG/#concrete-hash_1","title":"Concrete Hash","text":"<p>Metadata included in the concrete hash is below. This is the hash used to name the variable uniquely in the Parquet files, and to determine if the data has changed since the last time the task was run. Exactly what is being hashed depends on the type of Variable (hardcoded, dynamic input, data object attribute, output, etc.):</p> <ul> <li>For hardcoded Variables: The variable value</li> <li>For dynamic input Variables: The input data value</li> <li>For data object attributes: The value of the data object's attribute</li> <li>For paths: The contents of the file at the path. If that file does not exist or was not found, then the hash is the hash of an empty string \"\"<ul> <li>\"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"</li> </ul> </li> <li>For output Variables: The output data value</li> </ul>"},{"location":"Developer/Nodes/","title":"Overview","text":"<p>The Directed Acyclic Graph (DAG) is comprised of nodes of various kinds:</p> <p>Runnables: These are the operations that are run in the pipeline. They can be processes, plots, or summaries.</p> <p>Variables: The data that is passed between the runnable nodes. Variables can be input or output variables.</p> <p>Note</p> <p>This page is a work in progress. The information here is incomplete and may be inaccurate.</p>"},{"location":"Developer/Nodes/Runnables/","title":"Overview","text":"<p>Runnables are the functions, visualizations, and summaries that comprise the Directed Acylic Graph.</p>"},{"location":"Developer/Nodes/Runnables/logsheet/","title":"Logsheet","text":"<p>A type of Runnable</p> <p>Preferred format is: <pre><code>[logsheet]\nnum_header_rows = 1\ndataset_factors = [\"{factor1}\", \"{factor2}\"] # In hierarchical order. The first factor is the highest level of the hierarchy.\npath = \"path/to/logsheet.csv\"\n\n[logsheet.headers]\n{factor1}.column_name = \"{Column Header Name}\"\n{factor1}.type = \"{type}\"\n{factor1}.level = \"{level}\"\n</code></pre></p>"},{"location":"Developer/Nodes/Runnables/logsheet/#attributes","title":"Attributes","text":""},{"location":"Developer/Nodes/Runnables/logsheet/#headers","title":"Headers","text":"<p>A dictionary where each key is a <code>{Column Header Name}</code> (the name of the column in the logsheet) and <code>{type}</code> is the type of the column. The type can be one of the following: <code>\"str\"</code>, <code>\"num\"</code>, <code>\"bool\"</code>. </p>"},{"location":"Developer/Nodes/Runnables/logsheet/#outputs","title":"Outputs","text":"<p>A list of strings where each string is the name of an output. This is generated from the headers' names.</p>"},{"location":"Developer/Nodes/Runnables/logsheet/#num-header-rows","title":"Num Header Rows","text":"<p>An integer describing the number of rows that are headers in the logsheet. For example, if set to 3, then the 4th row is the first row of data.</p>"},{"location":"Developer/Nodes/Runnables/logsheet/#dataset-factors","title":"Dataset Factors","text":"<p>An ordered list of the variables that are factors in the dataset. The first factor is the highest level of the hierarchy.</p>"},{"location":"Developer/Nodes/Runnables/plot/","title":"Plot","text":"<p>A type of Runnable</p>"},{"location":"Developer/Nodes/Runnables/process/","title":"Process","text":"<p>A type of Runnable</p>"},{"location":"Developer/Nodes/Runnables/stats/","title":"Stats","text":"<p>A type of Runnable</p>"},{"location":"Developer/Nodes/Variables/","title":"Overview","text":"<p>Variable nodes represent the data that is passed between the Runnable nodes. Variables can be input or output variables, and are always connected to a Runnable node.</p> <p>Behind the scenes, for each data object being operated on, and for each Runnable, the input variables of type Constant need to be resolved before they can be used. For example, if an input variable is of type <code>LoadConstantFromFile</code>, then the file should be loaded and the value placed into <code>self.value</code> before the Runnable can be executed.</p>"},{"location":"Developer/Nodes/Variables/output_variables/","title":"Output Variables","text":"<p>Output variables represent the data that is being output from a process node. In the DAG, it connects to the subsequent input variables.</p> <p>Output variables are specified as a list of variable names in the <code>outputs</code> key of the process node in the .toml file. Because named output variables are typically not supported, the order of the variable names listed must match the order of the variables returned by the processing function. The names do not need to match the name of the variables returned, but the order must match. See below for an example: <pre><code>[example_process_name]\ninputs.input1 = \"input1\"\noutputs = [\"output1\", \"output2\"]\n</code></pre> This .toml file entry indicates that the example process returns two outputs, <code>output1</code> and <code>output2</code>, in that order.</p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/","title":"Overview","text":"<p>Input variables represent the data that is flowing into a process, plot, or summary. They are always connected to a Runnable node. They can be specified in a variety of ways, including hard-coded values, dynamic variables, or variables that are passed from other processes. Input variables are specified using the \"inputs\" key with named keys for each corresponding input variable. In the below example, there are two inputs, <code>input1</code> and <code>input2</code>: <pre><code>[example_process_name]\ninputs.input1 = \"?\"\ninputs.input2 = \"?\"\noutputs = [\"output1\"]\n</code></pre></p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/#unspecified-variables","title":"Unspecified Variables","text":"<p>Input variables that have the value <code>\"?\"</code> are considered unspecified. These variables are not connected to any other variable nodes in the package. By leaving the variable unspecified, package creators can leave the variable open for the user to specify using the <code>bridges.toml</code> file. If the variable is not specified in the <code>bridges.toml</code> file, the variable remains unspecified. See the above code block for an example of unspecified variables.</p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/#dynamic-variables","title":"Dynamic Variables","text":"<p>To use the output of another function in the package as an input to a Runnable, the source Runnable's name &amp; output variable should be specified. This is done by setting the input variable to a string in the format <code>\"{runnable_name}.{output_variable}\"</code>. For example: <pre><code>[example_process_name]\ninputs.input1 = \"example_process_name1.output1\"\ninputs.input2 = \"example_process_name2.output1\"\noutputs = [\"output1\"]\n</code></pre> If there is no runnable and output variable by that name (or if the value is not a string), then the variable value is treated as a hard-coded value.</p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/dynamic/","title":"Dynamic","text":"<p>Input variables that are from the output of a prior Process.</p> <p>Note</p> <p>This page is a work in progress. The information here is incomplete and may be inaccurate.</p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/hard_coded/","title":"Hard-Coded","text":"<p>Hard-coded variables specified by the user. If a list is provided, then the node is split into N nodes, where N is the length of the list. Each node is assigned a variable from the list. Hard-coded variables can consist of any combination of the following types:</p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/hard_coded/#hard-coded","title":"Hard-Coded","text":"<p>If an input variable is a hard-coded value, a default value can be specified. For example: <pre><code>[example_process_name]\ninputs.input1 = \"hardcoded_value\"\noutputs = [\"output1\"]\n</code></pre> Note that frequently the user will want to change hard-coded values for \"what if\" analyses. This can be done by specifying the value in the <code>bridges.toml</code> file, which overwrites the value here.</p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/hard_coded/#load-from-file","title":"Load From File","text":"<p>If a hardcoded variable is complex, or there are many versions of this hard-coded variable, it may be useful to load the variable from a file. JSON and TOML are supported. This can be done by specifying the input variable as a dictionary with the key <code>__load_file__</code> and the value as the file path relative to the project's root directory. For example:</p> <p>Syntax is: <pre><code>[example_process]\ninputs.hard_coded1 = {__load_file__ = 'path/to/file'}\n</code></pre></p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/hard_coded/#data-object-name","title":"Data Object Name","text":"<p>The name of the data object to be used as the variable. Specify the data object factor name.</p> <p>Syntax is: <pre><code>[example_process]\ninputs.hard_coded1 = {__data_object__ = 'Subject'}\n</code></pre></p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/hard_coded/#raw-data-file-path","title":"Raw Data File Path","text":"<p>The path to the raw data file to be used as the variable. Specify the data object factor name.</p> <p>Syntax is: <pre><code>[example_process]\ninputs.hard_coded1 = {__data_file__ = 'Subject'}\n</code></pre></p> <p>Note</p> <p>This page is a work in progress. The information here is incomplete and may be inaccurate.</p>"},{"location":"Developer/Nodes/Variables/Input%20Variables/unspecified/","title":"Unspecified","text":"<p>Input variables that are not specified in the package itself because they are supposed to take input from another package.</p> <p>Note</p> <p>This page is a work in progress. The information here is incomplete and may be inaccurate.</p>"},{"location":"Developer/Package_Setup/","title":"Overview","text":"<p>Packages Overview</p>"},{"location":"Developer/Package_Setup/#terminology","title":"Terminology","text":"<p>Package: A folder called <code>&lt;name&gt;</code> generated from <code>ros create &lt;name&gt;</code> with the following folder structure. </p> <p>Package Names</p> <p>The package name must begin with <code>ros-</code> otherwise creating the package folder will fail.</p> <p>Project: A package folder in combination with a dataset to perform data analysis.</p> <p>Dataset: A logsheet &amp; data files.</p> <p>The dependencies for a specific package are specified in the package's <code>pyproject.toml</code> file, in the <code>dependencies</code> table, see below for a minimal working example:</p> <pre><code>[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.researchos]\nindex = 'index.toml'\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src\"]\n\n[project]\nname = \"test-project\"\nversion = \"0.0.1\"\nrequires-python = \"&gt;=3.7\"\nauthors = [\n    {name = \"Mitchell Tillman &lt;my.email@gmail.com&gt;\"},\n]\ndescription = \"Example Package\"\nreadme = \"README.md\"\ndependencies = [\n    \"ros-example-package\"\n]\n</code></pre> <p>This tells <code>pip</code> that the <code>ros-example-package</code> and all of its dependencies need to be installed when installing the <code>test-project</code> package.</p>"},{"location":"Developer/Package_Setup/discovery/","title":"Discovery","text":"<p>Packages are discovered within the current package's folder.</p>"},{"location":"Developer/Package_Setup/discovery/#discover_packages","title":"discover_packages","text":""},{"location":"Developer/Package_Setup/discovery/#inputs","title":"Inputs:","text":"<p>project_folder: The current package's root folder. Default: current directory</p> <p>parent_folders: The folders that may contain packages of interest. Default: current directory.</p>"},{"location":"Developer/Package_Setup/discovery/#outputs","title":"Outputs:","text":"<p>package_folders: The list of folders inside of the <code>project_folder</code> and <code>parent_folders</code> starting with <code>ros-</code>. This includes packages installed by <code>pip</code> and other package managers.</p> <p>Package Names</p> <p>The package names are defined in each package's <code>pyproject.toml</code> file's <code>[\"project\"][\"name\"]</code> tables.</p>"},{"location":"Developer/Running/","title":"Overview","text":"<p>Overview of the process of running the functions in the DAG.</p> <p>Note</p> <p>This page is a work in progress. The information here is incomplete and may be inaccurate.</p> <p>After the DAG is compiled, the following steps are taken to run the specified pipeline: 1. Sort the Runnable nodes in topological order. Special consideration is taken to ensure that the non-dynamic or unspecified variables do not adversely affect the node order.</p> <ol> <li> <p>Check whether the MATLAB engine needs to be loaded. If so, load it.</p> </li> <li> <p>For each Runnable node in the topologically ordered list, convert the attributes that affect the variables' hash to a data structure that can be hashed such as a frozen dict, and store it in the same index in a tuple.</p> </li> <li> <p>Those attributes include inputs, outputs, runnable level, batch, function name, language</p> </li> <li> <p>For each node in the topologically ordered list, run the node.</p> </li> </ol>"},{"location":"Developer/Running/running_runnables/","title":"Running a Runnable","text":"<p>Note</p> <p>This page is a work in progress. The information here is incomplete and may be inaccurate.</p>"},{"location":"Developer/Running/selecting_runnables/","title":"Selecting Runnables","text":"<p>Describes how the system knows which nodes in a DAG to run and which to skip.</p> <p>Note</p> <p>This page is a work in progress. The information here is incomplete and may be inaccurate.</p>"},{"location":"Developer/Running/unique_identifiers/","title":"Uniqely Identifying Nodes","text":"<p>Nodes must be uniquely identifiable to load and save them to disk reliably. However, the polyfurcation of the DAG can lead to multiple nodes with the same name. To address this, each node is assigned a unique identifier (Python's UUID4). This identifier is a string that is unique across all nodes in the project, but only for a singular run of the program (because Python's UUID functions are not idempotent). Thus, the node is not identified by its name or UUID, but rather by the SHA 256 hash of the entire DAG that preceded (are ancestors of) the particular node.</p> <p>The hard-coded Input Variable nodes need to be resolved to the particular Data Objects in question before being hashable. For example, if the node is a DataObjectName node, its value may be specified by the user as \"Subject\". For each Subject, the node's value will be resolved to a particular Data Object name.</p> <p>Hashing the graph requires specific attributes for Runnable and Variable nodes. </p> <p>The following attributes of a Runnable node are used to compute the hash of a DAG: 1. node name 2. factor name 3. batch name (eventually this should change to be the dict of data objects in the batch, because a batch_name could stay the same or change without affecting the data objects or computations) 4. language</p> <p>The following attributes of a Variable node are used to compute the hash of a DAG: 1. node name 2. value 3. slices</p> <p>For Runnables and Variables, the relevant attributes are put into a dict, and the dict is json-serialized. Once all nodes are in a hashable form, the hash is computed using NetworkX's Weisfeler Lehman graph hash algorithm. node_attr is set to <code>serialized</code> because that is the attribute that contains the hashable form of the node. </p> <p>For each node, the hash of its ancestor graph is computed. This hash is stored in the node's <code>hash_value</code> attribute. This hash is used to uniquely identify the node.</p>"},{"location":"Home/","title":"Welcome to ResearchOS (pre-alpha version)","text":""},{"location":"Home/#introduction","title":"Introduction","text":"<p>Welcome to the documentation for ResearchOS, a Python package for scientific computing.</p>"},{"location":"Home/#project-description","title":"Project Description","text":"<p>Scientific computing is currently fractured, with many competing data standards (or lack thereof) and data processing tools that do not have a common way to communicate. ResearchOS provides a generalized framework to perform scientific computing of any kind, in a modular, easily shareable format.</p> <p>The primary innovation behind ResearchOS is to treat every single piece of the scientific data analysis workflow as an object, complete with ID and metadata. While this incurs some code overhead, the ability to have a standardized way to communicate between different parts of a pipeline and to share and integrate others' pipelines is invaluable, and sorely needed in the scientific computing community.</p>"}]}
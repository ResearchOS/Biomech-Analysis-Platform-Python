{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ResearchOS (pre-alpha version)","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the documentation for ResearchOS, a Python package for scientific computing.</p>"},{"location":"#project-description","title":"Project Description","text":"<p>Scientific computing is currently fractured, with many competing data standards (or lack thereof) and data processing tools that do not have a common way to communicate. ResearchOS provides a generalized framework to perform scientific computing of any kind, in a modular, easily shareable format.</p> <p>The primary innovation behind ResearchOS is to treat every single piece of the scientific data analysis workflow as an object, complete with ID and metadata. While this incurs some code overhead, the ability to have a standardized way to communicate between different parts of a pipeline and to share and integrate others' pipelines is invaluable, and sorely needed in the scientific computing community.</p>"},{"location":"DiGraph/digraph/","title":"DiGraph","text":""},{"location":"DiGraph/digraph/#introduction","title":"Introduction","text":"<p>The NetworkX MultiDiGraph (directional graph that can have multiple parallel edges) is the data structure that organizes the relationships between all of the different research objects. Just like the objects themselves, the Research Object DiGraph models the relationships between all research objects within the database.</p> <p>The Research Object DiGraph consists of both Data Objects and Pipeline Objects - therefore it can become quite large and cumbersome to work with. For example if there are 10 Trial objects (DataObject) each referencing 10 Variable objects (DataObject &amp; PipelineObject), this can quickly become quite large (100 connections in this small example). Often, it is not necessary to have both DataObjects and PipelineObjects in the same graph. Therefore, Data Object DiGraphs and Pipeline Object DiGraphs can be created separately by using <code>data_objects = True</code> and <code>pipeline_objects = True</code> keyword arguments.</p> <p>Subgraphs can also be created by specifying the top level node. For example, to work with just one project's DiGraph, use the <code>source_node = {research_object_id}</code> keyword argument in the constructor, where <code>{research_object_id}</code> is the Project object's ID.</p>"},{"location":"DiGraph/digraph/#note-about-adding-objects-to-the-digraph","title":"Note About Adding Objects to the DiGraph","text":"<p>When adding objects to the DiGraph, they must exist before being added to the DiGraph! In the future the ability to create objects by adding them to the DiGraph may be added, but for now object creation and addition to the DiGraph are two entirely separate steps.</p>"},{"location":"Getting%20Started/first_project/","title":"First Project","text":""},{"location":"Getting%20Started/first_project/#introduction","title":"Introduction","text":"<p>To familiarize you with the basics of how ResearchOS works, let's create a simple one step pipeline that reads a single number from a Logsheet, squares it, and stores that value. </p> <p>These instructions will begin with the assumption that you have already created a new project directory, activated a virtual environment within that folder, and installed ResearchOS. If you have not done so, please refer to the Installation section.</p> <p>Similar to the Installation section, I will be providing instructions for Visual Studio Code (VS Code), but the process is similar for other programs.</p>"},{"location":"Getting%20Started/first_project/#step-0-structuring-your-project","title":"Step 0: Structuring your Project.","text":"<p>I recommend that each project have a similar structure to the following: <pre><code>your_project_folder/\n| data/\n\u2502 research_objects/\n\u2502 \u2502 data_objects.py\n\u2502 \u2502 datasets.py\n\u2502 \u2502 logsheets.py\n\u2502 \u2502 processes.py\n\u2502 \u2502 subsets.py\n\u2502 \u2502 variables.py\n| | plots.py\n| | stats.py\n| paths.py\n| run_project.py\n</code></pre></p> <p>For larger projects, you may want to create additional folders for the runnable research objects. There should really only ever be one source (file) of Dataset, Logsheet, and Variable objects. However, for the runnable objects, given their more complex setup you may want to organize them into folders as follows: <pre><code>your_project_folder/\n| data/\n\u2502 research_objects/\n\u2502 \u2502 data_objects.py\n\u2502 \u2502 datasets.py\n\u2502 \u2502 logsheets.py\n| | subsets.py\n\u2502 \u2502 variables.py\n| step1.py/\n| | step1_processes.py\n| | step1_plots.py\n| | step1_stats.py\n| step2.py/\n| | step2_processes.py\n| | step2_plots.py\n| | step2_stats.py\n| paths.py\n| run_project.py\n</code></pre></p>"},{"location":"Getting%20Started/first_project/#step-1-initialize-the-database","title":"Step 1: Initialize the database.","text":"<ol> <li>In VS Code, create a new file called <code>run_project.py</code> in your project directory. In that file, type the following: <pre><code>import researchos as ros\n\nros.DBInitializer()\n</code></pre> Run that file and it will create two new files in the project folder: <code>researchos.db</code> and <code>researchos_data.db</code>. The <code>researchos.db</code> file stores all of the information about the structure and history of our project, while the <code>researchos_data.db</code> file stores the actual data that we compute.</li> </ol>"},{"location":"Getting%20Started/first_project/#step-2-create-some-dummy-data","title":"Step 2: Create some dummy data.","text":"<p>We'll need to create some dummy data so that our project has something to work with. We need to create a .csv file, which ResearchOS calls a Logsheet. In a typical project, this file would contain the records of an experiment, but for this example we'll just create a file called <code>dummy_logsheet.csv</code> with the following contents: <pre><code>\"Subject\",  \"Trial\",  \"Value\"\n\"Larry\",    1,        3\n</code></pre></p> <p>The first row is our headers, and the second row contains the value for the one trial we've recorded of our one subject, Larry. We'll use this data later.</p>"},{"location":"Getting%20Started/first_project/#step-3-define-the-dataset","title":"Step 3: Define the Dataset.","text":"<p>Next, we need to define some Research Objects that our pipeline will be built from. In your project folder, create a new folder called <code>research_objects</code>. We'll need a few types of research objects, so in this folder we will create a new file for each type. To define our Dataset, create a file called <code>dataset.py</code>.</p> <p>We need to define the attributes of this dataset. The minimum attributes that we need to define are its <code>schema</code> and <code>dataset_path</code>. We need to define a schema to tell ResearchOS how our data is structured. In this case, we have a Dataset that contains <code>Subject</code> Data Objects, and each <code>Subject</code> contains <code>Trial</code> Data Objects. These are two examples of custom Data Objects. In order for ResearchOS to cover any branch of science, Data Objects are custom defined each project. To define a <code>Subject</code> and <code>Trial</code> class, let's create a new .py file for our data objects <code>research_objects/data_objects.py</code>. Type the following contents: <pre><code>import ResearchOS as ros\n\nclass Subject(ros.DataObject):\n    prefix: str = \"SJ\" # Needs to start with \"SJ\" for \"Subject\", and be unique within the project.\n\nclass Trial(ros.DataObject):\n    prefix: str = \"TR\" # Needs to start with \"TR\" for \"Trial\", and be unique within the project.\n</code></pre> Now that the Data Objects are defined we can define the schema for the Dataset. We'll also define the <code>dataset_path</code> now too, which requires that we specify a folder in your project to put the data. Let's call it <code>data</code>. Type the following into <code>research_objects/datasets.py</code>: <pre><code>import ResearchOS as ros\nfrom research_objects.data_objects import Subject, Trial\n\ndataset = ros.Dataset(id = \"DS1\") # Needs to start with \"DS\" for \"Dataset\", and be unique within the project.\ndataset.schema = [\n    [ros.Dataset, Subject],\n    [Subject, Trial]\n]\ndataset.dataset_path = \"your_project_folder/data\"\n</code></pre></p>"},{"location":"Getting%20Started/first_project/#step-4-define-the-variables","title":"Step 4: Define the Variables.","text":"<p>Next, we need to define the Variables that we'll be using. Create a file called <code>research_objects/variables.py</code>. In <code>research_objects/variable.py</code>, type the following: <pre><code>import ResearchOS as ros\n\nsubject_name = ros.Variable(id = \"VR1\") # Needs to start with \"VR\" for \"Variable\", and be unique within the project.\ntrial_name = ros.Variable(id = \"VR2\")\nvalue = ros.Variable(id = \"VR3\")\n</code></pre></p>"},{"location":"Getting%20Started/first_project/#step-5-define-the-logsheet","title":"Step 5: Define the Logsheet","text":"<p>To define the Logsheet that we created in Step 2, create a file called <code>research_objects/logsheets.py</code>. We need to define the Logsheet attributes for the csv file that we created in step 2. The minimum attributes that we need to define are its <code>path</code>, <code>headers</code>, <code>num_header_rows</code>, and <code>class_column_names</code>.</p> <p>In <code>research_objects/logsheets.py</code>, type the following: <pre><code>import ResearchOS as ros\n\nlogsheet = ros.Logsheet(id = \"LG1\") # Needs to start with \"LG\" for \"Logsheet\", and be unique within the project.\nlogsheet.path = \"your_project_folder/dummy_logsheet.csv\"\nlogsheet.num_header_rows = 1\nlogsheet.class_column_names = {\n    \"Subject\": Subject,\n    \"Trial\": Trial\n}\nlogsheet.headers = [\n    (\"Subject\", str, Subject, vr.subject_name),\n    (\"Trial\", int, Trial, vr.trial_name),\n    (\"Value\", int, Trial, vr.value)\n]\n</code></pre></p>"},{"location":"Getting%20Started/first_project/#step-6-run-the-logsheet","title":"Step 6: Run the Logsheet.","text":"<p>Now that we have defined our Research Objects, we can run the Logsheet to initialize the dataset in our database. In <code>run_project.py</code>, type the following: <pre><code>from research_objects import logsheet as lg\nfrom research_objects import dataset as ds\n\nlg.logsheet.read_logsheet()\n</code></pre> After running this file, you should see a new dataset in your database with the data from the Logsheet.</p>"},{"location":"Getting%20Started/first_project/#step-7-define-the-process","title":"Step 7: Define the Process.","text":"<p>We need to define the Process that will square the value in our dataset. In <code>research_objects/processes.py</code>, type the following: <pre><code>import ResearchOS as ros\nfrom research_objects import variables as vr\nfrom square_value import square_value\n\nsquare_value = ros.Process(id = \"PR1\") # Needs to start with \"PR\" for \"Process\", and be unique within the project.\nsquare_value.set_input_vr(number = vr.value)\nsquare_value.set_output_vr(squared = vr.squared_value)\nsquare_value.subset_id = \"SS1\"\nsquare_value.method = square_value\n</code></pre> In another file called <code>square_value.py</code>, type the following: <pre><code>def square_value(number: int):\n    return number ** 2\n</code></pre></p>"},{"location":"Getting%20Started/first_project/#step-8-create-the-subset","title":"Step 8: Create the Subset.","text":"<p>We need to define the Subset to know which subset of our data the Process will operate on. In <code>research_objects/subsets.py</code>, type the following to define a subset that includes all of the data in our dataset: <pre><code>import ResearchOS as ros\n\nsubset = ros.Subset(id = \"SS1\") # Needs to start with \"SS\" for \"Subset\", and be unique within the project.\nsubset.conditions = {}\n</code></pre></p>"},{"location":"Getting%20Started/first_project/#step-9-run-the-process","title":"Step 9: Run the Process.","text":"<p>Finally, we will run the Process to square the value in our dataset. In <code>run_project.py</code>, type the following: <pre><code>from research_objects import processes as pr\n\npr.square_value.run()\n</code></pre></p>"},{"location":"Getting%20Started/first_project/#step-10-plot-the-results","title":"Step 10: Plot the results.","text":"<p>Create a <code>Plot</code> object and provide it with the Variables that you want to plot, the subset of data to plot, and the code to use. In <code>research_objects/plots.py</code>, type the following: <pre><code># research_objects/plots.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\nfrom research_objects import subsets as ss\nfrom paths import PLOT_CODE_PATH\nfrom research_objects.data_objects import Trial\n\nplot = ros.Plot(id = \"PL1\") # Needs to start with \"PL\" for \"Plot\", and be unique within the project.\nplot.set_input_vrs(data = vr.value, squared = vr.squared_value)\nplot.subset_id = ss.subset1.id\nplot.mfunc_name = \"plot_data\"\nplot.is_matlab = True\nplot.mfolder = PLOT_CODE_PATH\nplot.level = Trial\n</code></pre></p> <p>Then, in the <code>run_project.py</code> file: <pre><code>from research_objects import plots as pl\n\npl.plot.run()\n</code></pre></p> <p>This will save a .fig, .svg, and .png <code>Trial</code> in the project in the <code>your_project_folder/plots</code> folder, within the appropriate <code>Subject</code>'s folder, because that's how the Dataset <code>schema</code> is structured.</p> <p>Also, we can use a &lt;&gt; to print the value of the data for all <code>Data Object</code> instances in the Dataset."},{"location":"Getting%20Started/first_project/#step-11-run-the-statistics","title":"Step 11: Run the statistics","text":""},{"location":"Getting%20Started/installation/","title":"Installation","text":"<p>This page will guide you through the installation of Python, an Integrated Development Environment (IDE), and the ResearchOS package. If you want to use MATLAB with ResearchOS, you will also need to install the MATLAB Engine API for Python.</p>"},{"location":"Getting%20Started/installation/#installing-python-and-an-integrated-development-environment-ide","title":"Installing Python and an Integrated Development Environment (IDE)","text":"<ol> <li>Install Python 3.6 or later on your computer. If using MATLAB, check this Mathworks page for a list of compatible Python &amp; MATLAB versions and the table below for the MATLAB Engine API version.<ul> <li>Windows: While I recommend downloading Python from the official website, on Windows machines I have found downloading Python from the Windows store to be the most user-friendly for some reason. If downloading from the official website, select the \"x86 executable installer\" and be sure to add Python to your PATH during installation.</li> <li>Mac: You can download Python from the Python website or use Homebrew. If you want to use MATLAB with ResearchOS, be aware of Python version requirements for your version of MATLAB. </li> </ul> </li> <li>Make sure you have a program to run Python code installed, called an Integrated Development Environment (IDE). There are many options for Python, such as Spyder, PyCharm, Visual Studio Code, or Jupyter Notebook. I will be providing instructions for Visual Studio Code (VS Code), but the process is similar for other programs.</li> </ol> <p>NOTE: For Visual Studio Code, you will need to install the Python extension. You can do this by clicking the \"Extensions\" icon on the left-hand side of the window, searching for \"Python\", and clicking \"Install\" on the Python extension by Microsoft.</p>"},{"location":"Getting%20Started/installation/#installing-researchos","title":"Installing ResearchOS","text":"<ol> <li>Create a folder on your computer where you will store your code and data. This is where you will install the ResearchOS package.</li> <li>Set your current directory. Open a Command Prompt (Windows) or Terminal (Mac) and navigate to the folder you created in step 3. This sets that folder as the current directory. You can also do this in your IDE, which may be easier. In VS Code, you can do this by clicking \"File\" -&gt; \"Open Folder\" and selecting the folder you created. Then, click \"Terminal\" -&gt; \"New Terminal\" to open a terminal window in that folder.</li> <li>Create a virtual environment in your folder. This is a way to keep your Python packages separate from the rest of your computer, and should be done for every project folder. You can do this by typing <code>python -m venv .venv</code> into the Terminal while inside the folder we created in step 1. This will create a folder called <code>.venv</code> in your current directory that contains a separate Python installation. </li> <li>Note that you will need to activate this environment every time you open a new terminal window. VS Code frequently activates the venv for you when opening a Terminal using \"Terminal\" -&gt; \"New Terminal\". You can also do this by typing <code>source .venv/bin/activate</code> on Mac or <code>.\\.venv\\Scripts\\Activate</code> on Windows. You can deactivate the environment by typing <code>deactivate</code>. You will know that your virtual environment is activated if your terminal prompt has <code>(.venv)</code> at the beginning of the line.</li> <li>For Windows only: you may need to change your PowerShell settings to allow scripts to run. You can do this by opening PowerShell as an administrator and typing <code>Set-ExecutionPolicy RemoteSigned</code>. You can change this back to the default setting by typing <code>Set-ExecutionPolicy Restricted</code>. Alternatively, ensure that you are in a Command Prompt window, not PowerShell, by clicking the dropdown arrow next to the \"New Terminal\" button and selecting \"Command Prompt\". macOS and Linux users do not need to worry about this step.</li> <li>Verify that the correct Python interpreter is being used by typing <code>pip -V</code>. This should return the path to the Python interpreter in your virtual environment. If it does not, repeat step 4.</li> <li>Install the ResearchOS package and all of its dependencies by typing in your terminal: <pre><code>pip install researchos\n</code></pre></li> </ol> <p>You can now use ResearchOS in your Python code by importing it with: <pre><code>import researchos as ros\n</code></pre></p>"},{"location":"Getting%20Started/installation/#optional-installing-the-matlab-engine-api-for-python","title":"Optional: Installing the MATLAB Engine API for Python","text":"<p>If you want to use MATLAB with ResearchOS, you will need to install the MATLAB Engine API for Python. This installation is separate from the MATLAB software itself. Refer to the table below for the proper Matlab Engine API version for a subset of MATLAB releases. </p> <p>To install type <code>pip install matlabengine==##.##.##</code> in the terminal. You can find more instructions for installing the MATLAB Engine API for Python here.</p> MATLAB Version MATLAB Engine API Version R2023b 23.2.3 R2023a 9.14.6 R2021b 9.11.19 <p>If you have a MATLAB version that is not listed here and would like to contribute its corresponding MATLAB Engine API version to these docs, please open an issue or pull request on the ResearchOS GitHub page.</p>"},{"location":"Getting%20Started/installation/#optional-share-the-matlab-engine-for-interactivity-and-faster-researchos-startup","title":"Optional: Share the MATLAB Engine for Interactivity and Faster ResearchOS Startup","text":"<p>By default ResearchOS starts a new MATLAB Engine process each time a Process runs. This is slow, and does not allow the user to interact with the MATLAB session. To speed up ResearchOS startup and allow the user to interact with the MATLAB session, you can share the MATLAB Engine process from MATLAB. To do this, in MATLAB's Command Window type <code>matlab.engine.shareEngine('ResearchOS')</code>. This will share the MATLAB interactive window's session with ResearchOS. Then, breakpoints can be set in MATLAB and the MATLAB session can be interacted with while ResearchOS is running.</p> <p>To automatically share MATLAB's session with ResearchOS, add the <code>matlab.engine.shareEngine('ResearchOS')</code> to your <code>startup.m</code> file. This file is located in MATLAB's <code>userpath</code> directory, which can be found by typing <code>userpath</code> in MATLAB's Command Window. If the <code>startup.m</code> file does not exist, you can create it in the <code>userpath</code> directory. This will automatically share the MATLAB session with ResearchOS each time MATLAB is started.</p>"},{"location":"Getting%20Started/usage_orig/","title":"Usage","text":""},{"location":"Getting%20Started/usage_orig/#example-1","title":"Example 1","text":"<p>Let's use ResearchOS to create a simple one step pipeline that reads a single number from a text file, squares it, and stores that value.</p> <p>First, after creating a new project directory and activating a virtual environment in that directory, install ResearchOS: <pre><code>pip install researchos\n</code></pre></p> <p>Next, in the command line, run the following command: <pre><code>python -m researchos quick-start\n</code></pre></p> <p>This will perform the following actions:</p> <ol> <li> <p>Create a new directory called 'researchos_db' in the current directory.</p> </li> <li> <p>Create a .db file in the 'researchos_db' directory with the proper schema.</p> </li> <li> <p>Create a new Project object in the .db file, and sets it to be the current Project. </p> </li> </ol> <p>Then, create a file called <code>example1.py</code> with the following contents: <pre><code>from researchos.pipeline_objects.project import Project\n</code></pre></p> <p>This will create a new project.</p>"},{"location":"Home/","title":"Welcome to ResearchOS (pre-alpha version)","text":""},{"location":"Home/#introduction","title":"Introduction","text":"<p>Welcome to the documentation for ResearchOS, a Python package for scientific computing.</p>"},{"location":"Home/#project-description","title":"Project Description","text":"<p>Scientific computing is currently fractured, with many competing data standards (or lack thereof) and data processing tools that do not have a common way to communicate. ResearchOS provides a generalized framework to perform scientific computing of any kind, in a modular, easily shareable format.</p> <p>The primary innovation behind ResearchOS is to treat every single piece of the scientific data analysis workflow as an object, complete with ID and metadata. While this incurs some code overhead, the ability to have a standardized way to communicate between different parts of a pipeline and to share and integrate others' pipelines is invaluable, and sorely needed in the scientific computing community.</p>"},{"location":"Home/roadmap/","title":"Roadmap","text":""},{"location":"Home/roadmap/#version-01","title":"Version 0.1","text":"<ul> <li>[x] Do multiple things with one Action.</li> <li>[x] Create research objects, save and load them with attributes</li> <li>[x] Create edges between research objects and allow the edges to have their own attributes.</li> <li>[x] Load and save even complex attributes (e.g. list of dicts) with JSON. Right now I'm just using json.loads()/dumps() but I may need something more sophisticated.</li> <li>[x] Implement Logsheet<ul> <li>[x] Implement read logsheet.<ul> <li>[x] Populate the database with the logsheet data.</li> </ul> </li> </ul> </li> <li>[x] Implement saving participant data to disk/the database.<ul> <li>[x] Implement data schema for participant data</li> </ul> </li> <li>[x] Implement subsets.</li> <li>[~] Publish my proof of concept to JOSS.</li> </ul>"},{"location":"Home/roadmap/#version-02","title":"Version 0.2","text":"<ul> <li>[x] Implement Plots</li> <li>[ ] Implement Stats</li> <li>[ ] Create a graph of research objects and edges</li> <li>[ ] Implement rollback-able version history for research objects</li> <li>[ ] Enhance multi-user support on the same machine.</li> <li>[ ] Look into CI/CD best practices, improve test coverage.</li> <li>[ ] Import/export a ResearchObject for sharing with other users (using Parquet or something similar).</li> <li>[ ] Allow exporting stats results to LaTeX table templates (e.g. fill in ? with values).</li> <li>[ ] Export data to a CSV file in the common statistical format for processing in other software.</li> </ul>"},{"location":"Home/roadmap/#version-03-and-beyond","title":"Version 0.3 and beyond","text":"<ul> <li>[ ] Implement a MariaDB-based backend for ResearchOS so that it can be used in a multi-user environment.</li> <li>[ ] Implement password-based authentication for the MariaDB backend.</li> <li>[ ] Implement a web-based frontend for ResearchOS.</li> <li>[ ] Get journals on board with ResearchOS so that they can accept ResearchObjects with submissions.</li> <li>[ ] Integrate ResearchOS with participant management systems like RedHat so that participants &amp; data are linked.</li> </ul>"},{"location":"Research%20Objects/research_object/","title":"Research Objects","text":""},{"location":"Research%20Objects/research_object/#overview","title":"Overview","text":"<p>Everything within the ResearchOS framework is a Research Object. All Research Objects are stored in the database, and are accessible by using the methods provided in the ResearchOS API.</p> <p>All Research Objects are one of the following:</p> <ul> <li> <p>Data Objects - objects that are involved in storing data, such as a <code>Subject</code> or <code>Trial</code>. These objects are typically initialized by the Logsheet, and are used to store data that is generated by a Process. It is perhaps helpful to recognize that these objects' classes are analogous to the \"factors\" of a statistical analysis.</p> </li> <li> <p>Pipeline Objects - objects that comprise the steps of a data analysis pipeline, typically beginning with the Logsheet, processing data with Processes, plotting the data with a Plot, and summarizing it with Stats.</p> </li> <li> <p>Variable - Variable objects are used to store the values of variables that are used in the data analysis pipeline. Their values can be hard-coded or dynamic, and are used as inputs and outputs of the Pipeline Objects, and are associated with Data Objects.</p> </li> </ul> <p></p>"},{"location":"Research%20Objects/research_object/#implementation-details","title":"Implementation Details","text":"<p>Every Research Object needs to be created using a unique <code>id</code> as a keyword argument. If the <code>id</code> is not provided as a keyword argument, an error will be raised. This <code>id</code> must start with the two letter prefix that corresponds to the type of Research Object. Besides that, the <code>id</code> can consist of anything you want, as long as it is unique within the project.</p> <p>All Research Objects have two attributes that are inherited from the Research Object class: <code>name</code> and <code>notes</code>. The <code>name</code> attribute is a string that is used to more easily recognize the object in the database, and unlike the <code>id</code> it can be changed at any time. The <code>notes</code> attribute is a string that can be used to store any additional information about the object.</p> <p>No attributes besides the defaults can be added to a Research Object, and attempting to do so will raise an error. The attributes that are available for each Research Object are documented in the API documentation for that object.</p> <p>One research object. Parent class of Data Objects &amp; Pipeline Objects.</p> Source code in <code>src/ResearchOS/research_object.py</code> <pre><code>class ResearchObject():\n    \"\"\"One research object. Parent class of Data Objects &amp; Pipeline Objects.\"\"\"\n\n    def __deepcopy__(self, memo):\n        \"\"\"Raise an error if attempting to deepcopy this object.\"\"\"\n        raise ValueError(\"Research objects cannot be deepcopied.\")\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __eq__(self, other):\n        if isinstance(other, ResearchObject):\n            return self.id == other.id and self is other\n        return False\n\n    def __new__(cls, **kwargs):\n        \"\"\"Create a new research object in memory. If the object already exists in memory with this ID, return the existing object.\"\"\"\n        if \"id\" not in kwargs.keys():\n            raise ValueError(\"id is required as a kwarg\")  \n        id = kwargs[\"id\"]\n        if not IDCreator(None).is_ro_id(id):\n            raise ValueError(\"id is not a valid ID.\")\n        del kwargs[\"id\"]\n        if id in ResearchObjectHandler.instances:\n            ResearchObjectHandler.counts[id] += 1\n            ResearchObjectHandler.instances[id].__dict__[\"prev_loaded\"] = True\n            ResearchObjectHandler.instances[id].__dict__[\"_initialized\"] = True\n            return ResearchObjectHandler.instances[id]\n\n        ResearchObjectHandler.counts[id] = 1\n        instance = super(ResearchObject, cls).__new__(cls)\n        ResearchObjectHandler.instances[id] = instance\n        instance.__dict__[\"id\"] = id # Put the ID in the object.\n        instance.__dict__[\"prev_loaded\"] = False\n        instance.__dict__[\"_initialized\"] = False\n        return instance \n\n    def __setattr__(self, name: str = None, value: Any = None, action: Action = None, all_attrs: DefaultAttrs = None, kwargs_dict: dict = {}) -&gt; None:\n        \"\"\"Set the attribute value. If the attribute value is not valid, an error is thrown.\"\"\"        \n        if not self._initialized:\n            self.__dict__[name] = value\n            return\n        # Ensure that the criteria to set the attribute are met.\n        if not str(name).isidentifier():\n            raise ValueError(f\"{name} is not a valid attribute name.\") # Offers some protection for having to eval() the name to get custom function names.        \n        if name == \"id\":\n            raise ValueError(\"Cannot change the ID of a research object.\")\n        if name == \"prefix\":\n            raise ValueError(\"Cannot change the prefix of a research object.\")\n        if name == \"name\":\n            if not str(value).isidentifier():\n                raise ValueError(f\"name attribute, value: {value} is not a valid attribute name.\")\n        if name == \"slice\" and self.id.startswith(\"VR\"):\n            self.__dict__[name] = value\n            return # Store but don't save the changes to Variable slices. That is handled when saving the input VR's.\n\n        # Set the attribute. Create Action when __setattr__ is called as the top level.\n        if all_attrs is None:\n            all_attrs = DefaultAttrs(self)                \n        commit = False\n        if action is None:\n            commit = True\n            action = Action(name = \"attribute_changed\")\n\n        if not kwargs_dict:\n            kwargs_dict = {name: value}\n        self._setattrs(all_attrs.default_attrs, kwargs_dict, action, None)\n\n        action.commit = commit\n        action.exec = True\n        action.execute()     \n\n    def __init__(self, action: Action = None, **other_kwargs):\n        \"\"\"Initialize the research object.\n        On initialization, all attributes should be saved! \n        Unless the object is being loaded, in which case only *changed* attributes should be saved.\"\"\"\n        attrs = DefaultAttrs(self) # Get the default attributes for the class.\n        default_attrs_dict = attrs.default_attrs\n        if \"name\" in other_kwargs:\n            self.name = other_kwargs[\"name\"] # Set the name to the default name.\n        elif \"name\" not in self.__dict__:\n            self.name = default_attrs_dict[\"name\"] # Set the name to the default name.\n        if \"notes\" in other_kwargs:\n            self.notes = other_kwargs[\"notes\"]\n        elif \"notes\" not in self.__dict__:\n            self.notes = all_default_attrs[\"notes\"] # Set the notes to the default notes.\n\n        prev_loaded = self.__dict__[\"prev_loaded\"]\n        del self.__dict__[\"prev_loaded\"] # Remove prev_loaded attribute.      \n\n        self_dict = copy.copy(self.__dict__) # Get the current values of the object's attributes. These are mostly default but could have been overriden in the constructor.\n        del self_dict[\"_initialized\"] # Remove the _initialized attribute from the kwargs so that it is not set as an attribute.        \n        del self_dict[\"id\"] # Remove the ID from the kwargs so that it is not set as an attribute.                    \n\n        finish_action = False\n        if action is None:\n            action = Action(name = \"__init__\", exec = False) # One data object.\n            finish_action = True                \n\n        if prev_loaded:\n            prev_exists = True\n        else:\n            prev_exists = ResearchObjectHandler.object_exists(self.id, action)        \n        if not prev_exists:\n            # Create a new object.\n            query_name = \"robj_exists_insert\"\n            params = (self.id, action.id)\n            action.add_sql_query(id, query_name, params, group_name = \"robj_insert\")                                \n\n        loaded_attrs = {}\n        if prev_exists and not prev_loaded:\n            # Load the existing object's attributes from the database.\n            loaded_attrs = ResearchObjectHandler._load_ro(self, attrs, action)\n            self.__dict__.update(loaded_attrs) # Set the loaded attributes.            \n\n        # If creating a new object, save all attributes.\n        # If loaded an existing object: save only the changed attributes.        \n        save_dict = {}\n        for attr in self_dict:\n            try:\n                if not prev_exists or (self_dict[attr] != default_attrs_dict[attr] and self_dict[attr] != loaded_attrs[attr]):\n                    save_dict[attr] = self_dict[attr]\n            except:\n                pass\n\n        self._setattrs(default_attrs_dict, save_dict, action, None)\n        self._initialized = True\n\n        # Set the attributes.\n        if finish_action:\n            action.exec = True\n            action.commit = True\n            action.execute()\n\n    def _setattrs(self, default_attrs: dict, kwargs: dict, action: Action, pr_id: str) -&gt; None:\n        \"\"\"Set the attributes of the object.\n        default_attrs: The default attributes of the object.\n        orig_kwargs: The original kwargs passed to the object.\n        kwargs: The kwargs to be used to set the attributes. A combination of the default attributes and the original kwargs.\n        pr_id: Indicates that I am setting VR attributes.\"\"\"\n        del_keys = []\n        if self._initialized:\n            for key in kwargs:\n                try:\n                    if key in self.__dict__ and self.__dict__[key] == kwargs[key]:\n                        del_keys.append(key) # No change.\n                except ValueError:\n                    pass # Allow the Variable to not exist yet.\n\n        for key in del_keys:\n            del kwargs[key]\n        # 1. Set simple &amp; complex builtin attributes.\n        ResearchObjectHandler._set_builtin_attributes(self, default_attrs, kwargs, action)\n\n        # 2. Set VR attributes.\n        if pr_id is not None:\n            vr_attrs = {k: v for k, v in kwargs.items() if k not in default_attrs}\n            ResearchObjectHandler._set_vr_values(self, vr_attrs, action, pr_id)\n\n    def _get_dataset_id(self) -&gt; str:\n        \"\"\"Get the most recent dataset ID.\"\"\"        \n        sqlquery = f\"SELECT dataset_id FROM data_address_schemas\"\n        pool = SQLiteConnectionPool()\n        conn = pool.get_connection()\n        cursor = conn.cursor()\n        result = cursor.execute(sqlquery).fetchall()\n        pool.return_connection(conn)\n        if not result:\n            raise ValueError(\"Need to create a dataset and set up its schema first.\")\n        dataset_id = result[-1][0]        \n        return dataset_id\n\n    def get_current_schema_id(self, dataset_id: str) -&gt; str:\n        pool = SQLiteConnectionPool()\n        conn = pool.get_connection()\n        sqlquery = f\"SELECT action_id FROM data_address_schemas WHERE dataset_id = '{dataset_id}'\"\n        action_ids = conn.cursor().execute(sqlquery).fetchall()\n        action_ids = ResearchObjectHandler._get_time_ordered_result(action_ids, action_col_num=0)\n        action_id_schema = action_ids[0][0] if action_ids else None\n        if action_id_schema is None:\n            pool.return_connection(conn)\n            return # If the schema is empty and the addresses are empty, this is likely initialization so just return.\n\n        sqlquery = f\"SELECT schema_id FROM data_address_schemas WHERE dataset_id = '{dataset_id}' AND action_id = '{action_id_schema}'\"\n        schema_id = conn.execute(sqlquery).fetchone()\n        schema_id = schema_id[0] if schema_id else None\n        pool.return_connection(conn)\n        return schema_id\n</code></pre>"},{"location":"Research%20Objects/variable/","title":"Variable","text":""},{"location":"Research%20Objects/variable/#introduction","title":"Introduction","text":"<p>Variables are the links between all of the research objects in your project. They represent the data that is collected/computed in your project. Every Variable must have a unique <code>ID</code> within the project, and must start with \"VR\" to indicate that it is a Variable. Because Variables are used so frequently in the project, their definition is very lightweight. The only required attribute is the <code>ID</code>, but you can also specify a <code>name</code> attribute to give the Variable a more descriptive name.</p>"},{"location":"Research%20Objects/variable/#recommended-folder-structure","title":"Recommended Folder Structure","text":"<pre><code>project_folder/\n\u2502-- research_objects/\n\u2502   \u2502-- variables.py\n</code></pre> <p>In the <code>variables.py</code> file, Variables can be defined as follows: <pre><code>import ResearchOS as ros\n\ntest_vr1 = ros.Variable(id = \"VR1\") # ID needs to start with \"VR\" for \"Variable\", and be unique within the project.\ntest_vr2 = ros.Variable(id = \"VR2\", name = \"Test Variable 2\") # Can optionally specify a name attribute.\n</code></pre></p>"},{"location":"Research%20Objects/variable/#hard-coded-variables","title":"Hard-Coded Variables","text":"<p>Sometimes, a Variable's value is not computed, but is instead hard-coded when a Variable represents a constant value. This could be a scalar value, for example to control how much smoothing to apply to a signal, or a string value to represent a specific condition, or anything else. In this case, the Variable's <code>hard_coded_value</code> attribute can be provided in the <code>variables.py</code> file. For example: <pre><code>import ResearchOS as ros\n\ntest_vr1 = ros.Variable(id = \"VR1\", hard_coded_value = 5) # ID needs to start with \"VR\" for \"Variable\", and be unique within the project.\n\ntest_vr2 = ros.Variable(id = \"VR2\")\ntest_vr2.hard_coded_value = \"Condition A\" # Can also be specified on a separate line, useful for longer hard-coded values.\n</code></pre></p> <p>In some cases, the hard-coded value can be rather complex, such as a dictionary or a list. In this case, a common strategy is to create a .json file that contains the hard-coded value, and then load the .json file in the <code>variables.py</code> file.  <pre><code>import ResearchOS as ros\nimport json\n\ntest_vr1 = ros.Variable(id = \"VR1\", hard_coded_value = json.load(open(\"path/to/hard_coded_value.json\")))\n</code></pre></p>"},{"location":"Research%20Objects/variable/#removing-hard-coded-value","title":"Removing Hard-Coded Value","text":"<p>If a Variable has a hard-coded value but you want to remove it, you can set the <code>hard_coded_value</code> attribute to <code>None</code>. For example: <pre><code>import ResearchOS as ros\n\ntest_vr1 = ros.Variable(id = \"VR1\", hard_coded_value = 5) # ID needs to start with \"VR\" for \"Variable\", and be unique within the project.\n# Later in the code I decided that test_vr1 should not have a hard-coded value.\ntest_vr1.hard_coded_value = None # Now it does not have a hard-coded value, and is dynamically evaluated instead.\n</code></pre> </p> <p>             Bases: <code>ResearchObject</code></p> <p>Variable class.</p> Source code in <code>src/ResearchOS/variable.py</code> <pre><code>class Variable(ResearchObject):\n    \"\"\"Variable class.\"\"\"\n\n    prefix: str = \"VR\"\n    _initialized: bool = False\n\n    def __getitem__(self, slice: tuple) -&gt; Any:\n        \"\"\"Store the slice of the Variable.\"\"\"\n        self.slice = slice\n        return self\n\n    def __init__(self, hard_coded_value: Any = all_default_attrs[\"hard_coded_value\"], \n                **kwargs):\n        if self._initialized:\n            return\n        self.hard_coded_value = hard_coded_value\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"Research%20Objects/Data%20Objects/data_object/","title":"Data Objects","text":""},{"location":"Research%20Objects/Data%20Objects/data_object/#introduction","title":"Introduction","text":"<p>Data Objects are anything that can have a data value associated with it. They are analagous to factors in a statistical analysis. This definition is purposely vague as Data Objects are intended to be entirely user-defined, customizable, and extensible. This is the mechanism by which ResearchOS can be used to manage and analyze any type of data across any domain. For example, as a biomechanist my datasets consist of multiple Subjects who each have multiple Trials. Subjects and Trials are both Data Objects in this example. Another biomechanist may structure their dataset differently, with Subjects, Tasks, and Trials. Another scientist in a different domain, for example artificial intelligence, may have a dataset consisting of multiple Experiments which each have multiple Agents interacting with each other. In this case, Experiments and Agents are Data Objects. The point is that Data Objects are entirely user-defined and can be anything that can have a data value associated with it. The relationships between the Data Objects within one Dataset are defined by the Dataset's <code>schema</code> attribute as an edge list.</p> <p>             Bases: <code>ResearchObject</code></p> <p>The parent class for all data objects. Data objects represent some form of data storage, and approximately map to statistical factors.</p> Source code in <code>src/ResearchOS/DataObjects/data_object.py</code> <pre><code>class DataObject(ResearchObject):\n    \"\"\"The parent class for all data objects. Data objects represent some form of data storage, and approximately map to statistical factors.\"\"\"    \n\n    def __delattr__(self, name: str, action: Action = None) -&gt; None:\n        \"\"\"Delete an attribute. If it's a builtin attribute, don't delete it.\n        If it's a VR, make sure it's \"deleted\" from the database.\"\"\"\n        default_attrs = DefaultAttrs(self).default_attrs\n        if name in default_attrs:\n            raise AttributeError(\"Cannot delete a builtin attribute.\")\n        if name not in self.__dict__:\n            raise AttributeError(\"No such attribute.\")\n        if action is None:\n            action = Action(name = \"delete_attribute\")\n        vr_id = self.__dict__[name].id        \n        params = (action.id, self.id, vr_id)\n        if action is None:\n            action = Action(name = \"delete_attribute\")\n        action.add_sql_query(self.id, \"vr_to_dobj_insert_inactive\", params)\n        action.execute()\n        del self.__dict__[name]\n\n    def _load_vr_value(self, vr: \"Variable\", action: Action, process: \"Process\" = None, vr_name_in_code: str = None, node_lineage: list = []) -&gt; Any:\n        \"\"\"Load the value of a VR from the database for this data object.\n\n        Args:\n            vr (Variable): ResearchOS Variable object to load the value of.\n            process (Process): ResearchOS Process object that this data object is part of.\n            action (Action): The Action that this is part of.\n\n        Returns:\n            Any: The value of the VR for this data object.\n        \"\"\"\n        func_result = {}\n        func_result[\"input_vrs_names_dict\"] = None\n        from ResearchOS.variable import Variable\n        # 1. Check that the data object &amp; VR are currently associated. If not, throw an error.\n        cursor = action.conn.cursor()\n        self_idx = node_lineage.index(self)\n        for node in node_lineage[self_idx:]:\n            if isinstance(vr, Variable):\n                sqlquery_raw = \"SELECT action_id, is_active FROM vr_dataobjects WHERE dataobject_id = ? AND vr_id = ?\"\n                sqlquery = sql_order_result(action, sqlquery_raw, [\"dataobject_id\", \"vr_id\"], single = True, user = True, computer = False)\n                params = (node.id, vr.id)            \n                result = cursor.execute(sqlquery, params).fetchall()\n                if len(result) &gt; 0:\n                    break\n            else:\n                # TODO: Handle dict of {type: attr_name}\n                # If the value is a str, then it's a builtin attribute.\n                # Otherwise, if the value is a Variable, then it's a Variable and need to load its value. using self.load_vr_value()\n                pass\n        if len(result) == 0:\n            func_result[\"do_run\"] = False\n            func_result[\"exit_code\"] = 1\n            func_result[\"message\"] = f\"Failed to run {self.name} ({self.id}). {vr_name_in_code} ({vr.id}) not actively connected to {node.id}.\"\n            func_result[\"vr_values_in\"] = None\n            return func_result # If that variable does not exist for this dataobject, skip processing this dataobject.\n        is_active = result[0][1]\n        if is_active == 0:\n            raise ValueError(f\"The VR {vr.name} is not currently associated with the data object {node.id}.\")\n\n        # 2. Load the data hash from the database.\n        if hasattr(process, \"vrs_source_pr\"):\n            pr = process.vrs_source_pr[vr_name_in_code]\n        else:\n            pr = process\n        if not isinstance(pr, list):\n            pr = [pr]\n        sqlquery_raw = \"SELECT data_blob_hash, pr_id FROM data_values WHERE dataobject_id = ? AND vr_id = ? AND pr_id IN ({})\".format(\", \".join([\"?\" for _ in pr]))\n        params = (node.id, vr.id) + tuple([pr_elem.id for pr_elem in pr])\n        sqlquery = sql_order_result(action, sqlquery_raw, [\"dataobject_id\", \"vr_id\"], single = True, user = True, computer = False)        \n        result = cursor.execute(sqlquery, params).fetchall()\n        if len(result) == 0:\n            raise ValueError(f\"The VR {vr.name} does not have a value set for the data object {node.id} from Process {process.id}.\")\n        if len(result) &gt; 1:\n            raise ValueError(f\"The VR {vr.name} has multiple values set for the data object {node.id} from Process {process.id}.\")\n        pr_ids = [x[1] for x in result]\n        pr_idx = None\n        for pr_id in pr_ids:\n            pr_idx = None\n            try:\n                pr_idx = pr_ids.index(pr_id)\n            except:\n                pass\n            if pr_idx is not None:\n                break\n        if pr_idx is None:\n            raise ValueError(f\"The VR {vr.name} does not have a value set for the data object {node.id} from any process provided.\")\n        data_hash = result[pr_idx][0]\n\n        # 3. Get the value from the data_values table. \n        pool_data = SQLiteConnectionPool(name = \"data\")\n        conn_data = pool_data.get_connection()\n        cursor_data = conn_data.cursor()\n        sqlquery = \"SELECT data_blob FROM data_values_blob WHERE data_blob_hash = ?\"        \n        params = (data_hash,)\n        value = cursor_data.execute(sqlquery, params).fetchone()[0]\n        pool_data.return_connection(conn_data)\n        func_result[\"do_run\"] = True\n        func_result[\"exit_code\"] = 0\n        func_result[\"message\"] = f\"Success in {self.name} ({self.id}). Lookup VR found: {vr_name_in_code} ({vr.id})\"\n        func_result[\"vr_values_in\"] = pickle.loads(value)\n        return func_result\n</code></pre>"},{"location":"Research%20Objects/Data%20Objects/dataset/","title":"Dataset","text":"<p>Inherits from DataObject.</p>"},{"location":"Research%20Objects/Data%20Objects/dataset/#introduction","title":"Introduction","text":"<p><code>Dataset</code> is the only built-in Data Object, because all projects will have data contained within a Dataset. Dataset has two attributes that must be defined: <code>dataset_path</code> and <code>schema</code>. There is also one optional attribute <code>file_schema</code>.</p>"},{"location":"Research%20Objects/Data%20Objects/dataset/#dataset-path","title":"Dataset Path","text":"<p>This is the path to where the dataset is stored in the file system. For example, if the dataset is stored in a folder inside the project folder, the <code>dataset_path</code> can be specified as a relative path, e.g. <code>folder_name</code>. If it is stored in another folder outside of the project folder, then the <code>dataset_path</code> should be specified as an absolute path, e.g. <code>/path/to/folder</code>.</p>"},{"location":"Research%20Objects/Data%20Objects/dataset/#schema","title":"Schema","text":"<p>This defines how the <code>Dataset</code> is structured: which Data Objects are in this Dataset, and the hierarchical relationships between them. The <code>schema</code> is defined as an \"edge list\", which is a list of lists. Each sublist contains two elements: the parent Data Object and the child Data Object, always starting with the <code>Dataset</code> class. For example, if the <code>Dataset</code> contains <code>Subject</code> and <code>Trial</code> Data Objects, and each <code>Subject</code> has multiple <code>Trials</code>, the <code>schema</code> would be defined as  <pre><code>import ResearchOS as ros\nfrom research_objects.data_objects import Subject, Trial\nschema = [\n    [ros.Dataset, Subject],\n    [Subject, Trial]\n]\n</code></pre> In this case the <code>Dataset</code> is the parent of <code>Subject</code>, and <code>Subject</code> is the parent of <code>Trial</code>. <code>Subject</code> and <code>Trial</code> need to be custom defined as Data Objects.</p>"},{"location":"Research%20Objects/Data%20Objects/dataset/#file-schema","title":"File Schema","text":"<p>Sometimes, the dataset is structured in the file system in the exact same way as it is specified in the schema. In this case, the <code>file_schema</code> attribute does not need to be manually entered because it is filled in by the <code>schema</code> attribute. In our example <code>schema</code> of <code>Subject</code> Data Objects which each have multiple <code>Trial</code> Data Objects, we may have a directory structure like this containing our data: <pre><code>dataset_folder\n\u2502-- Subject1/\n\u2502   \u2502-- Trial1.ext\n\u2502   \u2502-- Trial2.ext\n\u2502-- Subject2/\n\u2502   \u2502-- Trial1.ext\n\u2502   \u2502-- Trial2.ext\n</code></pre></p> <p>However, for various reasons, sometimes the file system structure is different from the <code>schema</code>. In this case, the <code>file_schema</code> attribute can be used to specify the file system structure differently than the <code>Dataset</code> <code>schema</code>. For example, if my file structure were the same as above, but my <code>schema</code> consisted of a <code>Subject</code> Data Object which had multiple <code>Task</code> Data Objects, and each <code>Task</code> contained multiple <code>Trial</code> Data Objects, the schema would be defined as <pre><code>import ResearchOS as ros\nfrom research_objects.data_objects import Subject, Task, Trial\nschema = [\n    [ros.Dataset, Subject],\n    [Subject, Task],\n    [Task, Trial]\n]\n</code></pre> Because file system structure may still look like the above folder structure, the <code>file_schema</code> in this case would be defined as <pre><code>import ResearchOS as ros\nfrom research_objects.data_objects import Subject, Trial\nfile_schema = [\n    [ros.Dataset, Subject],\n    [Subject, Trial]\n]\n</code></pre></p> <p>             Bases: <code>DataObject</code></p> <p>A dataset is one set of data. Class-specific Attributes: 1. data path: The root folder location of the dataset. 2. data schema: The schema of the dataset (specified as a list of classes)</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>class Dataset(DataObject):\n    \"\"\"A dataset is one set of data.\n    Class-specific Attributes:\n    1. data path: The root folder location of the dataset.\n    2. data schema: The schema of the dataset (specified as a list of classes)\"\"\"\n\n    prefix: str = \"DS\"\n\n    def __init__(self, schema: list = all_default_attrs[\"schema\"], \n                 dataset_path: str = all_default_attrs[\"dataset_path\"], \n                 addresses: list = all_default_attrs[\"addresses\"], \n                 file_schema: list = all_default_attrs[\"file_schema\"],\n                 **kwargs):                 \n        \"\"\"Create a new dataset.\"\"\"\n        if self._initialized:\n            return\n        self.schema = schema\n        self.dataset_path = dataset_path\n        self.addresses = addresses\n        self.file_schema = file_schema\n        super().__init__(**kwargs)\n\n    ### Schema Methods\n\n    def validate_schema(self, schema: list, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate that the data schema follows the proper format.\n        Must be a dict of dicts, where all keys are Python types matching a DataObject subclass, and the lowest levels are empty.\n\n        Args:\n            self\n            schema (list) : dict of dicts, all keys are Python types matching a DataObject subclass, and the lowest levels are empty\n        Returns:\n            None\n        Raises:\n            ValueError: Incorrect schema type\"\"\"\n        from ResearchOS.research_object import ResearchObject\n        if schema == default:\n            return\n        subclasses = ResearchObject.__subclasses__()\n        dataobj_subclasses = DataObject.__subclasses__()\n        vr = [x for x in subclasses if hasattr(x,\"prefix\") and x.prefix == \"VR\"][0]                \n\n        graph = nx.MultiDiGraph()\n        try:\n            graph.add_edges_from(schema)\n        except nx.NetworkXError:\n            raise ValueError(\"The schema must be provided as an edge list!\")\n        if not nx.is_directed_acyclic_graph(graph):\n            raise ValueError(\"The schema must be a directed acyclic graph!\")\n\n        non_subclass = [node for node in graph if node not in dataobj_subclasses]\n        if non_subclass:\n            raise ValueError(\"The schema must only include DataObject subclasses!\")\n\n        if Dataset not in graph:\n            raise ValueError(\"The schema must include the Dataset class as a source node!\")\n\n        if vr in graph:\n            raise ValueError(\"The schema must not include the Variable class as a target node!\")\n\n        # nodes_with_no_targets = [node for node, out_degree in graph.out_degree() if out_degree == 0]\n        # nodes_with_a_source = [node for node, in_degree in graph.in_degree() if in_degree &gt; 0]\n        # if graph[Dataset] in nodes_with_no_targets or graph[Dataset] in nodes_with_a_source:\n        #     raise ValueError(\"The schema must include the Dataset class as a source node and not a target node!\")\n\n    def save_schema(self, schema: list, action: Action) -&gt; None:\n        \"\"\"Save the schema to the database. One cohesive action.\n        Args:\n            self\n            schema (list) : dict of dicts, all keys are Python types matching a DataObject subclass, and the lowest levels are empty\n            action (Action) : a set of sequal queries that perform multiple action with one Action object call\n        Returns:\n            None\"\"\"        \n        # 1. Convert the list of types to a list of str.\n        str_schema = []\n        for sch in schema:\n            classes = []\n            for cls in sch:\n                classes.append(cls.prefix)\n            str_schema.append(classes)\n        # 2. Convert the list of str to a json string.\n        json_schema = json.dumps(str_schema)\n\n        # 3. Save the schema to the database.        \n        schema_id = IDCreator(action.conn).create_action_id()\n        params = (schema_id, json_schema, self.id, action.id)\n        action.add_sql_query(self.id, \"data_address_schemas_insert\", params, group_name = \"robj_complex_attr_insert\")\n        # Set the file schema to the dataset schema if it is still the default value.\n        if self.file_schema == all_default_attrs[\"file_schema\"]:\n            self.__setattr__(\"file_schema\", schema, action)\n\n    def load_schema(self, action: Action) -&gt; list:\n        \"\"\"Load the schema from the database and convert it via json.\"\"\"\n\n        # 1. Get the dataset ID\n        id = self.id\n        # 2. Get the most recent action ID for the dataset in the data_address_schemas table.\n        schema_id = self.get_current_schema_id(id)\n        sqlquery = f\"SELECT levels_edge_list FROM data_address_schemas WHERE schema_id = '{schema_id}'\"\n        conn = action.conn\n        result = conn.execute(sqlquery).fetchone()\n\n        # 5. If the schema is not None, convert the string to a list of types.\n        str_schema = json.loads(result[0])\n        schema = []\n        for sch in str_schema:\n            for idx, prefix in enumerate(sch):\n                sch[idx] = ResearchObjectHandler._prefix_to_class(prefix)\n            schema.append(sch)  \n\n        return schema\n\n    ### Dataset path methods\n\n    def validate_dataset_path(self, path: str, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate the dataset path.\n\n        Args:\n            self\n            path (string): your dataset path\n        Returns:\n            None\n        Raises:\n            ValueError: given path does not exist\"\"\"        \n        if path == default:\n            return\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path is not a path or does not currently exist!\")\n\n    def load_dataset_path(self, action: Action) -&gt; str:\n        \"\"\"Load the dataset path from the database in a computer-specific way.\"\"\"\n        return ResearchObjectHandler.get_user_computer_path(self, \"dataset_path\", action)\n\n    ### File Schema Methods\n\n    def validate_file_schema(self, file_schema: list, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate that the file schema follows the proper format.\n        Must be a list of DataObjects.\n\n        Args:\n            self\n            file_schema (list) : list of strings\n        Returns:\n            None\n        Raises:\n            ValueError: Incorrect file schema type\"\"\"\n        if file_schema == default:\n            return\n        if not isinstance(file_schema, list):\n            raise ValueError(\"The file schema must be a list!\")\n        subclasses = DataObject.__subclasses__()\n        if not all([sch in subclasses for sch in file_schema]):\n            raise ValueError(\"The file schema must be a list of DataObject Types!\")\n        schema_graph = nx.MultiDiGraph()\n        schema_graph.add_edges_from(self.schema)\n        if len(schema_graph.nodes()) &gt; 0:\n            if not all([curr_type in schema_graph.nodes()]):\n                raise ValueError(\"The file schema must consist only of types found within the schema!\")\n\n    def to_json_file_schema(self, file_schema: list, action: Action) -&gt; str:\n        \"\"\"Convert the file schema to a json string.\"\"\"\n        return json.dumps([file_schema.prefix for file_schema in file_schema])\n\n    def from_json_file_schema(self, json_file_schema: str, action: Action) -&gt; list:\n        \"\"\"Convert the file schema from a json string to a list of DataObjects.\"\"\"\n        prefix_file_schema = json.loads(json_file_schema)\n        subclasses = DataObject.__subclasses__()\n        return [cls for cls in subclasses for prefix in prefix_file_schema if cls.prefix == prefix]\n\n    ### Address Methods\n\n    def validate_addresses(self, addresses: list, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate that the addresses are in the correct format.\n\n        Args:\n            self\n            addresses (list) : list of addresses IDK\n        Returns:\n            None\n        Raises:\n            ValueError: invalid address provided\"\"\"\n        if addresses == default:\n            return False # Used by the Process.run() method to ascertain whether the addresses have been properly instantiated or not.\n        self.validate_schema(self.schema, action, None)   \n\n        try:\n            graph = nx.MultiDiGraph()\n            graph.add_edges_from(addresses)\n        except nx.NetworkXError:\n            raise ValueError(\"The addresses must be provided as an edge list!\")\n\n        if not nx.is_directed_acyclic_graph(graph):\n            raise ValueError(\"The addresses must be a directed acyclic graph!\")\n\n        non_ro_id = [node for node in graph if not IDCreator(action.conn).is_ro_id(node)]\n        if non_ro_id:\n            raise ValueError(\"The addresses must only include ResearchObject ID's!\")\n\n        if not graph[self.id]:\n            raise ValueError(\"The addresses must include the dataset ID!\")\n\n        vrs = [node for node in graph if node.startswith(\"VR\")]\n        if vrs:\n            raise ValueError(\"The addresses must not include Variable ID's!\")\n\n        schema = self.schema\n        schema_graph = nx.MultiDiGraph()\n        schema_graph.add_edges_from(schema)\n        for address_edge in addresses:\n            cls0 = ResearchObjectHandler._prefix_to_class(address_edge[0])\n            cls1 = ResearchObjectHandler._prefix_to_class(address_edge[1])\n            if cls0 not in schema_graph.predecessors(cls1) or cls1 not in schema_graph.successors(cls0):\n                raise ValueError(\"The addresses must match the schema!\")\n\n    def save_addresses(self, addresses: list, action: Action) -&gt; list:\n        \"\"\"Save the addresses to the data_addresses table in the database.\n        Args:\n            self\n            addresses (list) : list of addresses IDK\n            action (Action) : IDK\n        Returns:\n            None\"\"\"        \n        # 1. Get the schema_id for the current dataset_id that has not been overwritten by an Action.       \n        dataset_id = self.id\n        schema_id = self.get_current_schema_id(dataset_id)                \n        for address_names in addresses:\n            params = (address_names[0], address_names[1], schema_id, action.id)\n            action.add_sql_query(self.id, \"addresses_insert\", params, group_name = \"robj_complex_attr_insert\")            \n        # self.__dict__[\"address_graph\"] = self.addresses_to_graph(addresses, action)        \n\n    def load_addresses(self, action: Action) -&gt; list:\n        \"\"\"Load the addresses from the database.\"\"\"\n        pool = SQLiteConnectionPool()        \n        schema_id = self.get_current_schema_id(self.id)\n        conn = pool.get_connection()\n\n        # 2. Get the addresses for the current schema_id.\n        sqlquery = f\"SELECT target_object_id, source_object_id FROM data_addresses WHERE schema_id = '{schema_id}'\"\n        addresses = conn.execute(sqlquery).fetchall()\n        pool.return_connection(conn)\n\n        # 3. Convert the addresses to a list of lists (from a list of tuples).\n        addresses = [list(address) for address in addresses]                \n        # self.__dict__[\"address_graph\"] = self.addresses_to_graph(addresses, action)\n\n        return addresses\n\n    def get_addresses_graph(self, objs: bool = False, action: Action = None) -&gt; nx.MultiDiGraph:\n        \"\"\"Convert the addresses edge list to a MultiDiGraph.\n        Args:\n            self\n            addresses (list) : list of addresses\n        Returns:\n            nx.MultiDiGraph of addresses\"\"\"\n        # action = Action(\"get_addresses_graph\")\n        addresses = self.addresses\n        G = nx.MultiDiGraph()\n        if not objs:\n            G.add_edges_from(addresses)            \n        else:\n            for address in addresses:\n                cls0 = ResearchObjectHandler._prefix_to_class(address[0])\n                cls1 = ResearchObjectHandler._prefix_to_class(address[1])\n                node0 = cls0(id = address[0], action = action)\n                node1 = cls1(id = address[1], action = action)\n                G.add_edge(node0, node1)  \n        return G\n</code></pre>"},{"location":"Research%20Objects/Data%20Objects/dataset/#src.ResearchOS.DataObjects.dataset.Dataset.get_addresses_graph","title":"<code>get_addresses_graph(objs=False, action=None)</code>","text":"<p>Convert the addresses edge list to a MultiDiGraph. Args:     self     addresses (list) : list of addresses Returns:     nx.MultiDiGraph of addresses</p> Source code in <code>src/ResearchOS/DataObjects/dataset.py</code> <pre><code>def get_addresses_graph(self, objs: bool = False, action: Action = None) -&gt; nx.MultiDiGraph:\n    \"\"\"Convert the addresses edge list to a MultiDiGraph.\n    Args:\n        self\n        addresses (list) : list of addresses\n    Returns:\n        nx.MultiDiGraph of addresses\"\"\"\n    # action = Action(\"get_addresses_graph\")\n    addresses = self.addresses\n    G = nx.MultiDiGraph()\n    if not objs:\n        G.add_edges_from(addresses)            \n    else:\n        for address in addresses:\n            cls0 = ResearchObjectHandler._prefix_to_class(address[0])\n            cls1 = ResearchObjectHandler._prefix_to_class(address[1])\n            node0 = cls0(id = address[0], action = action)\n            node1 = cls1(id = address[1], action = action)\n            G.add_edge(node0, node1)  \n    return G\n</code></pre>"},{"location":"Research%20Objects/Data%20Objects/defining_data_objects/","title":"Defining Data Objects","text":""},{"location":"Research%20Objects/Data%20Objects/defining_data_objects/#recommended-folder-structure","title":"Recommended Folder Structure","text":"<p><pre><code>project_folder/\n\u2502-- research_objects/\n\u2502   \u2502-- my_data_objects.py\n</code></pre> Data Objects can be defined by the user in the project folder. While the exact method is flexible as long as the user subclasses ResearchOS.data_objects.DataObject, I recommend to create a <code>research_objects</code> folder in the project folder and then create a <code>my_data_objects.py</code> file within that to define each Data Object. For each Data Object subclass, define a class that inherits from <code>DataObject</code>. For example, the following code defines a <code>Subject</code> and <code>Trial</code> Data Object:</p> <p><pre><code>from ResearchOS.DataObjects.data_object import DataObject\n\nclass Subject(DataObject):\n\n    prefix: str = \"SJ\" # Must be unique within the project.\n\nclass Trial(DataObject):\n\n    prefix: str = \"TR\" # Must be unique within the project.\n</code></pre> All of the above code must be present to define a custom Data Object. The class name and the two letter prefix should be changed to suit your needs. The <code>prefix</code> attribute is a two letter prefix that is used to generate the Data Object's ID.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/analysis/","title":"Analysis","text":"<p>             Bases: <code>PipelineObject</code></p> Source code in <code>src/ResearchOS/PipelineObjects/analysis.py</code> <pre><code>class Analysis(PipelineObject):\n\n    prefix = \"AN\"\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/logsheet/","title":"Logsheet","text":""},{"location":"Research%20Objects/Pipeline%20Objects/logsheet/#introduction","title":"Introduction","text":"<p>Most researchers take notes during their experiments. These notes come in many forms, from informal handwritten to very formalized forms or spreadsheets. The <code>Logsheet</code> object is designed to capture these notes in a structured way. The <code>Logsheet</code> object is a Pipeline Object, and must be the first object in the pipeline because it constructs the Data Objects in the <code>Dataset</code>. </p> <p>The format of the <code>Logsheet</code> spreadsheets will vary widely between labs and disciplines. However, there are broadly two types of columns in a <code>Logsheet</code>: <code>Data Object</code> columns and <code>Variable</code> columns. <code>Data Object</code> columns are used to define the <code>Dataset</code> <code>schema</code>, and <code>Variable</code> columns define attributes for each <code>Data Object</code>. </p>"},{"location":"Research%20Objects/Pipeline%20Objects/logsheet/#setting-attributes","title":"Setting Attributes","text":""},{"location":"Research%20Objects/Pipeline%20Objects/logsheet/#logsheet-path","title":"Logsheet Path","text":"<p>The path to the logsheet file is computer-specific. I recommend that you put the logsheet path in a <code>paths.py</code> file and import it into the research_objects/logsheets.py file. By putting the <code>paths.py</code> file into the <code>.gitignore</code> file (keeping it out of the project's GitHub repository) you can maintain different paths for different computers.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/logsheet/#headers","title":"Headers","text":"<p>The headers attribute is a list of tuples. Each tuple contains, in order: the name of the column (which must match the first row of the column exactly), the data type of the column, the custom <code>Data Object</code> class that the column belongs to, and the <code>Variable</code> object that the column is associated with. For example: <pre><code>headers = [\n    (\"Subject\", str, Subject, vr.subject_name), # This is a `Data Object` column\n    (\"Trial\", str, Trial, vr.trial_name), # This is a `Data Object` column\n    (\"Value\", int, Trial, vr.value) # This is a `Variable` column\n]\n</code></pre></p>"},{"location":"Research%20Objects/Pipeline%20Objects/logsheet/#number-of-header-rows","title":"Number of Header Rows","text":"<p>This attribute is an integer that specifies the number of rows at the top of the logsheet that are not part of the data. This is useful for logsheet files that have one or more header rows that describe the contents of the columns.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/logsheet/#class-column-names","title":"Class Column Names","text":"<p>This attribute is a dictionary that maps the names of the <code>Data Object</code> columns to the custom <code>Data Object</code> classes that they belong to. Note that the keys must match the first row of the column exactly. For example: <pre><code>class_column_names = {\n    \"Subject\": Subject,\n    \"Trial\": Trial\n}\n</code></pre></p> <p>             Bases: <code>PipelineObject</code></p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>class Logsheet(PipelineObject):\n\n    prefix = \"LG\"\n\n    def __init__(self, path: str = all_default_attrs[\"path\"], headers: list = all_default_attrs[\"headers\"],\n                num_header_rows: int = all_default_attrs[\"num_header_rows\"], class_column_names: dict = all_default_attrs[\"class_column_names\"], **kwargs):\n        if self._initialized:\n            return\n        self.path = path\n        self.headers = headers\n        self.num_header_rows = num_header_rows\n        self.class_column_names = class_column_names\n        super().__init__(**kwargs)\n\n    ### Logsheet path\n\n    def validate_path(self, path: str, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate the logsheet path.\n\n        Args:\n            self\n            path (string) : logsheet path as a string\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: invalid path format\"\"\"\n        if path == default:\n            return\n        # 1. Check that the path exists in the file system.\n        if not isinstance(path, str):\n            raise ValueError(\"Path must be a string!\")\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path does not exist!\")\n        # 2. Check that the path is a file.\n        if not os.path.isfile(path):\n            raise ValueError(\"Specified path is not a file!\")\n        # 3. Check that the file is a CSV.\n        if not path.endswith((\"csv\", \"xlsx\", \"xls\")):\n            raise ValueError(\"Specified file is not a CSV!\")\n\n    def load_path(self, action: Action) -&gt; None:\n        \"\"\"Load the logsheet path.\"\"\"\n        return ResearchObjectHandler.get_user_computer_path(self, \"path\", action)\n\n    ### Logsheet headers\n\n    def validate_headers(self, headers: list, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate the logsheet headers. These are the headers that are in the logsheet file.\n        The headers must be a list of tuples, where each tuple has 3 elements:\n        1. A string (the name of the header)\n        2. A type (the type of the header)\n        3. A valid variable ID (the ID of the Variable that the header corresponds to)\n\n        Args: \n            self\n            headers (list) : headers in the logsheet file as a list of tuples each with 3 elements\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: incorrect ''header'' format\"\"\"\n        if headers == default:\n            return\n        self.validate_path(self.path, action, None)\n\n        # 1. Check that the headers are a list.\n        if not isinstance(headers, list):\n            raise ValueError(\"Headers must be a list!\")\n\n        # 2. Check that the headers are a list of tuples and meet the other requirements.        \n        for header in headers:\n            if not isinstance(header, tuple):\n                raise ValueError(\"Headers must be a list of tuples!\")\n            # 3. Check that each header tuple has 3 elements.        \n            if len(header) != 4:\n                raise ValueError(\"Each header tuple must have 4 elements!\")\n            # 4. Check that the first element of each header tuple is a string.        \n            if not isinstance(header[0], str):\n                raise ValueError(\"First element of each header tuple must be a string!\")\n            if header[1] not in [str, int, float]:\n                raise ValueError(\"Second element of each header tuple must be a Python type!\")\n            if header[2] not in DataObject.__subclasses__():\n                raise ValueError(\"Third element of each header tuple must be a ResearchObject subclass!\")\n            # 6. Check that the third element of each header tuple is a valid variable ID.                \n            if not isinstance(header[3],(str, Variable)):\n                raise ValueError(\"Fourth element of each header tuple must be a valid pre-existing variable ID OR the variable object itself!\")\n            if isinstance(header[3], str) and not (header[3].startswith(Variable.prefix) and ResearchObjectHandler.object_exists(header[3], action)):\n                raise ValueError(\"Fourth element of each header tuple (if provided as a str) must be a valid pre-existing variable ID!\")\n\n        logsheet = self._read_and_clean_logsheet(nrows = 1)\n        headers_in_logsheet = logsheet[0]\n        header_names = [header[0] for header in headers]\n        missing = [header for header in headers_in_logsheet if header not in header_names]\n\n        if len(missing) &gt; 0:\n            raise ValueError(f\"The headers {missing} do not match between logsheet and code!\")\n\n    def to_json_headers(self, headers: list, action: Action) -&gt; str:\n        \"\"\"Convert the headers to a JSON string.\n        Also sets the VR's name and level.\n\n        Args:\n            self\n            headers (list) : headers in the logsheet file as a list of tuples\n\n        Returns:\n            ''headers'' as a JSON string using ''json.dumps''\"\"\"\n        str_headers = []        \n        for header in headers:\n            # Update the Variable object with the name if it is not already set, and the level.\n            if isinstance(header[3], Variable):\n                vr = header[3]                \n            else:\n                vr = Variable(id = header[3], action = action)\n            default_attrs = DefaultAttrs(vr).default_attrs\n            kwarg_dict = {\"name\": header[0]}\n            vr._setattrs(default_attrs, kwarg_dict, action, None)\n            str_headers.append((header[0], str(header[1])[8:-2], header[2].prefix, vr.id))\n        return json.dumps(str_headers)\n\n    def from_json_headers(self, json_var: str, action: Action) -&gt; list:\n        \"\"\"Convert the JSON string to a list of headers.\n\n\n        Args:\n            self\n            json_var (string) : JSON string returned by ''to_json_headers''\n\n        Returns:\n            formatted list of headers\"\"\"\n        subclasses = DataObject.__subclasses__()\n        str_var = json.loads(json_var)\n        headers = []\n        mapping = {\n            \"str\": str,\n            \"int\": int,\n            \"float\": float\n        }\n        for header in str_var:\n            cls_header = [cls for cls in subclasses if cls.prefix == header[2]][0]\n            headers.append((header[0], mapping[header[1]], cls_header, header[3]))                \n        return headers\n\n    ### Num header rows\n\n    def validate_num_header_rows(self, num_header_rows: int, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate the number of header rows. If it is not valid, the value is rejected.\n\n        Args:\n            self\n            num_header_rows (int) : number of header rows as an integer\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: invalid ''num_header_rows'' format\"\"\"\n        if num_header_rows == default:\n            return\n        if not isinstance(num_header_rows, (int, float)):\n            raise ValueError(\"Num header rows must be numeric!\")\n        if num_header_rows&lt;0:\n            raise ValueError(\"Num header rows must be positive!\")\n        if num_header_rows % 1 != 0:\n            raise ValueError(\"Num header rows must be an integer!\")  \n\n    ### Class column names\n\n    def validate_class_column_names(self, class_column_names: dict, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate the class column names. Must be a dict where the keys are the column names in the logsheet and the values are the DataObject subclasses.\n\n        Must be a dict where the keys are the column names in the logsheet and the values are the DataObject subclasses.\n\n        Args:\n            self\n            class_column_names (dict) : dictionary where keys are logsheet column names &amp; values are ''DataObject'' subcclasses\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: incorrect format of ''class_column_names'' \"\"\"\n        if class_column_names == default:\n            return\n        self.validate_path(self.path, action, None)\n        # 1. Check that the class column names are a dict.\n        if not isinstance(class_column_names, dict):\n            raise ValueError(\"Class column names must be a dict!\")\n        # 2. Check that the class column names are a dict of str to type.        \n        for key, value in class_column_names.items():\n            if not isinstance(key, str):\n                raise ValueError(\"Keys of class column names must be strings!\")\n            if not issubclass(value, DataObject):\n                raise ValueError(\"Values of class column names must be Python types that subclass DataObject!\")\n\n        headers = self._read_and_clean_logsheet(nrows = 1)[0]\n        if not all([header in headers for header in class_column_names.keys()]):\n            raise ValueError(\"The class column names must be in the logsheet headers!\")\n\n    def from_json_class_column_names(self, json_var: dict, action: Action) -&gt; dict:\n        \"\"\"Convert the dict from JSON string where values are class prefixes to a dict where keys are column names and values are DataObject subclasses.\n        QUESTION confused about data flow/the order that these functions are called\"\"\"     \n        prefix_var = json.loads(json_var)\n        class_column_names = {}\n        all_classes = ResearchObjectHandler._get_subclasses(ResearchObject)\n        for key, prefix in prefix_var.items():\n            for cls in all_classes:\n                if hasattr(cls, \"prefix\") and cls.prefix == prefix:\n                    class_column_names[key] = cls\n                    break\n        return class_column_names\n\n    def to_json_class_column_names(self, var: dict, action: Action) -&gt; dict:\n        \"\"\"Convert the dict from a dict where keys are column names and values are DataObject subclasses to a JSON string where values are class prefixes.\n\n        Convert the dict from a dict where keys are column names and values are DataObject subclasses to a JSON string where values are class prefixes.\n        UNCLEAR QUESTION does it return a JSON string or a dict? is the JSON string just the format of the new dict values?\n\n        Args:\n            self\n            var (dict) : same dict as ''class_column_name'' in ''validate_class_column_names''\n\n        Returns:\n            new dict where keys are are logsheet column names and values are a JSON sring of class prefixes IDK\"\"\"        \n        prefix_var = {}\n        for key in var:\n            prefix_var[key] = var[key].prefix\n        return json.dumps(prefix_var)\n\n    #################### Start class-specific methods ####################\n    def _read_and_clean_logsheet(self, nrows: int = None) -&gt; list:\n        \"\"\"Read the logsheet (CSV only) and clean it.\"\"\"\n        logsheet = []\n        if platform.system() == \"Windows\":\n            first_elem_prefix = \"\u00ef\u00bb\u00bf\"\n        else:\n            first_elem_prefix = '\\ufeff'\n        if self.path.endswith((\"xlsx\", \"xls\")):\n            raise ValueError(\"CSV files only!\")\n        with open(self.path, \"r\") as f:\n            reader = csv.reader(f, delimiter=',', quotechar='\"')            \n\n            for row_num, row in enumerate(reader):                                    \n                logsheet.append(row)\n                if nrows is not None and row_num == nrows-1:\n                    break\n\n        # 7. Check that the headers all match the logsheet.\n        logsheet[0][0] = logsheet[0][0][len(first_elem_prefix):]\n        return logsheet\n\n    def load_xlsx(self) -&gt; list:\n        \"\"\"Load the logsheet as a list of lists using Pandas.\n\n        Returns:\n            list of logsheet values from excel\"\"\"        \n        df = pd.read_excel(self.path, header = None)\n        return df.values.tolist()\n\n    # @profile(stream = read_logsheet_stream)\n    def read_logsheet(self) -&gt; None:\n        \"\"\"Run the logsheet import process.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: more header rows than logsheet rows or incorrect schema format?\"\"\"\n        action = Action(name = \"read logsheet\")\n        ds = Dataset(id = self._get_dataset_id(), action = action)\n        self.validate_class_column_names(self.class_column_names, action, None)\n        self.validate_headers(self.headers, action, None)\n        self.validate_num_header_rows(self.num_header_rows, action, None)\n        self.validate_path(self.path, action, None)\n\n        # 1. Load the logsheet (using built-in Python libraries)\n        if self.path.endswith((\"xlsx\", \"xls\")):\n            full_logsheet = self.load_xlsx()\n        else:\n            full_logsheet = self._read_and_clean_logsheet()\n\n        if len(full_logsheet) &lt; self.num_header_rows:\n            raise ValueError(\"The number of header rows is greater than the number of rows in the logsheet!\")\n\n        # Run the logsheet import.\n        # headers = full_logsheet[0:self.num_header_rows]\n        if len(full_logsheet) == self.num_header_rows:\n            logsheet = []\n        else:\n            logsheet = full_logsheet[self.num_header_rows:]\n\n        # logsheet = logsheet[0:50] # For testing purposes, only read the first 50 rows.\n\n        # For each row, connect instances of the appropriate DataObject subclass to all other instances of appropriate DataObject subclasses.\n        headers_in_logsheet = full_logsheet[0]\n        all_headers = self.headers\n        header_names = [header[0] for header in self.headers]\n        header_types = [header[1] for header in self.headers]\n        header_levels = [header[2] for header in self.headers]\n        header_vrids = [header[3] for header in self.headers]\n        # Load/create all of the Variables\n        vr_list = []\n        vr_obj_list = []\n        for idx, vr_id in enumerate(header_vrids):\n            if isinstance(vr_id, Variable):\n                vr = vr_id\n            else:\n                vr = Variable(id = vr_id, action = action)\n            vr_obj_list.append(vr)\n            vr_list.append(vr.id)\n\n        # Order the class column names by precedence in the schema so that higher level objects always exist before lower level.\n        schema = ds.schema\n        schema_graph = nx.DiGraph()\n        schema_graph.add_edges_from(schema)\n        order = list(nx.topological_sort(schema_graph))\n        if len(order) &lt;= 1:\n            raise ValueError(\"The schema must have at least 2 elements including the Dataset!\")\n        order = order[1:] # Remove the Dataset class from the order.\n        dobj_column_names = []\n        for cls in order:\n            for column_name, cls_item in self.class_column_names.items():\n                if cls is cls_item:\n                    dobj_column_names.append(column_name)\n\n        # Create the data objects.\n        # Get all of the names of the data objects, after they're cleaned for SQLite.\n        cols_idx = [headers_in_logsheet.index(header) for header in dobj_column_names] # Get the indices of the data objects columns.\n        dobj_names = [] # The matrix of data object names (values in the logsheet).\n        for row in logsheet:\n            dobj_names.append([])\n            for idx in cols_idx:\n                raw_value = row[idx]\n                type_class = header_types[idx]\n                value = self._clean_value(type_class, raw_value)\n                dobj_names[-1].append(value)\n        for row_num, row in enumerate(dobj_names):\n            if not all([str(cell).isidentifier() for cell in row]):\n                raise ValueError(f\"Row #{row_num+self.num_header_rows+1} (1-based): All data object names must be non-empty and valid variable names!\")\n        [row.insert(0, ds.id) for row in dobj_names] # Prepend the Dataset to the first column of each row.\n        name_ids_dict = {} # The dict that maps the values (names) to the IDs. Separate dict for each class, each class is a top-level key of the dict.\n        name_ids_dict[Dataset] = {ds.name: ds.id}\n        name_dobjs_dict = {}\n        name_dobjs_dict[Dataset] = {ds.name: ds}\n        for cls in order:\n            name_ids_dict[cls] = {} # Initialize the dict for this class.            \n            name_dobjs_dict[cls] = {}\n\n        # Create the DataObject instances in the dict.        \n        all_dobjs_ordered = [] # The list of lists of DataObject instances, ordered by the order of the schema.        \n        id_creator = IDCreator(action.conn)\n        for row_num, row in enumerate(dobj_names):\n            row = row[1:]\n            all_dobjs_ordered.append([ds]) # Add the Dataset to the beginning of each row.\n            for idx in range(len(row)):\n                cls = order[idx] # The class to create.\n                col_idx = cols_idx[idx] # The index of the column in the logsheet.\n                value = self._clean_value(header_types[col_idx], row[idx])\n                # NEED TO CHECK NOT ONLY IF THE VALUE MATCHES, BUT WHETHER THE ENTIRE LINEAGE MATCHES.\n                # For example, condition names can be reused between subjects (though not within the same subject) but a new ID should be created for each lineage.\n                row_to_now = [ds.id] + row[0:idx+1]\n                is_new = True\n                if row_num &gt; 0:\n                    for lst in dobj_names[0:row_num]:\n                        if set(row_to_now).issubset(set(lst)):\n                            is_new = False\n                            break\n                if is_new:\n                    name_ids_dict[cls][value] = id_creator.create_ro_id(cls) + \"_\" + value\n                    dobj = cls(id = name_ids_dict[cls][value], name = value, action = action) # Create the research object.\n                    name_dobjs_dict[cls][value] = dobj\n                    print(\"Creating DataObject, Row: \", row_num, \"Column: \", cls.prefix, \"Value: \", value, \"ID: \", dobj.id)\n                dobj = name_dobjs_dict[cls][value]\n                all_dobjs_ordered[-1].append(dobj) # Matrix of all research objects.                                        \n\n        # Assign the values to the DataObject instances.\n        # Validates that the logsheet is of valid format.\n        # i.e. Doesn't have conflicting values for one level (empty/None is OK)\n        attrs_cache_dict = {}\n        default_none_vals = {str: None, int: np.array(float('nan'))}\n        for row_num, row in enumerate(logsheet):\n            row_dobjs = all_dobjs_ordered[row_num][1:]\n            row_attrs = [{} for _ in range(len(order))] # The list of dicts of attributes for each DataObject instance.\n\n            # Assign all of the data to the appropriate DataObject instances.\n            # Includes the \"data object columns\" so that the DataObjects have an attribute with the name of the header name.\n            for header in all_headers:\n                name = header[0]\n                col_idx = headers_in_logsheet.index(name)                \n                type_class = header[1]\n                level = header[2]\n                level_idx = order.index(level)\n                vr_id = header[3]                \n                value = self._clean_value(type_class, row[headers_in_logsheet.index(name)])                                    \n                # Set up the cache dict for this data object.\n                if not row_dobjs[level_idx].id in attrs_cache_dict:\n                    attrs_cache_dict[row_dobjs[level_idx].id] = {}\n                # Set up the cache dict for this data object for this attribute.\n                if name not in attrs_cache_dict[row_dobjs[level_idx].id]:\n                    attrs_cache_dict[row_dobjs[level_idx].id][name] = default_none_vals[type_class]\n                print(\"Row: \", row_num+self.num_header_rows+1, \"Column: \", name, \"Value: \", value)\n                prev_value = attrs_cache_dict[row_dobjs[level_idx].id][name]\n                if prev_value is not default_none_vals[type_class] and (type(prev_value) == np.ndarray and not np.isnan(prev_value)):                    \n                    if prev_value == value or value == default_none_vals[type_class] or np.isnan(value):\n                        continue\n                    raise ValueError(f\"Row # (1-based): {row_num+self.num_header_rows+1} Column: {name} has conflicting values!\")\n                attrs_cache_dict[row_dobjs[level_idx].id][name] = value\n                row_attrs[level_idx][vr_id] = attrs_cache_dict[row_dobjs[level_idx].id][name]\n            for idx, attrs in enumerate(row_attrs):\n                row_dobjs[idx]._setattrs({}, attrs, action = action, pr_id = self.id)                           \n\n        # Arrange the address ID's that were generated into an edge list.\n        # Then assign that to the Dataset.\n        addresses = []\n        for row in all_dobjs_ordered:\n            for idx, dobj in enumerate(row):\n                if idx == 0:\n                    continue\n                ids = [row[idx-1].id, dobj.id]\n                if ids not in addresses:\n                    addresses.append(ids)\n        all_default_attrs = DefaultAttrs(ds)\n        ds._setattrs(all_default_attrs.default_attrs, {\"addresses\": addresses}, action = action, pr_id = self.id)\n        # ds.addresses = addresses # Store addresses, also creates address_graph.\n\n        action.exec = True\n        action.commit = True\n        action.execute() # Commit the action.\n\n    def _clean_value(self, type_class: type, raw_value: Any) -&gt; Any:\n        \"\"\"Convert to proper type and clean the value of the logsheet cell.\"\"\"\n        try:\n            value = type_class(raw_value)\n        except ValueError:\n            value = raw_value\n        if isinstance(value, str):\n            value = value.replace(\"'\", \"''\") # Handle single quotes.\n            value = value.strip()\n        if value == '': # Empty\n            value = None\n        if type_class is int:\n            if value is None:\n                value = np.array(float('nan'))\n            else:\n                value = np.array(value)\n        return value\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/logsheet/#src.ResearchOS.PipelineObjects.logsheet.Logsheet.read_logsheet","title":"<code>read_logsheet()</code>","text":"<p>Run the logsheet import process.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>more header rows than logsheet rows or incorrect schema format?</p> Source code in <code>src/ResearchOS/PipelineObjects/logsheet.py</code> <pre><code>def read_logsheet(self) -&gt; None:\n    \"\"\"Run the logsheet import process.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: more header rows than logsheet rows or incorrect schema format?\"\"\"\n    action = Action(name = \"read logsheet\")\n    ds = Dataset(id = self._get_dataset_id(), action = action)\n    self.validate_class_column_names(self.class_column_names, action, None)\n    self.validate_headers(self.headers, action, None)\n    self.validate_num_header_rows(self.num_header_rows, action, None)\n    self.validate_path(self.path, action, None)\n\n    # 1. Load the logsheet (using built-in Python libraries)\n    if self.path.endswith((\"xlsx\", \"xls\")):\n        full_logsheet = self.load_xlsx()\n    else:\n        full_logsheet = self._read_and_clean_logsheet()\n\n    if len(full_logsheet) &lt; self.num_header_rows:\n        raise ValueError(\"The number of header rows is greater than the number of rows in the logsheet!\")\n\n    # Run the logsheet import.\n    # headers = full_logsheet[0:self.num_header_rows]\n    if len(full_logsheet) == self.num_header_rows:\n        logsheet = []\n    else:\n        logsheet = full_logsheet[self.num_header_rows:]\n\n    # logsheet = logsheet[0:50] # For testing purposes, only read the first 50 rows.\n\n    # For each row, connect instances of the appropriate DataObject subclass to all other instances of appropriate DataObject subclasses.\n    headers_in_logsheet = full_logsheet[0]\n    all_headers = self.headers\n    header_names = [header[0] for header in self.headers]\n    header_types = [header[1] for header in self.headers]\n    header_levels = [header[2] for header in self.headers]\n    header_vrids = [header[3] for header in self.headers]\n    # Load/create all of the Variables\n    vr_list = []\n    vr_obj_list = []\n    for idx, vr_id in enumerate(header_vrids):\n        if isinstance(vr_id, Variable):\n            vr = vr_id\n        else:\n            vr = Variable(id = vr_id, action = action)\n        vr_obj_list.append(vr)\n        vr_list.append(vr.id)\n\n    # Order the class column names by precedence in the schema so that higher level objects always exist before lower level.\n    schema = ds.schema\n    schema_graph = nx.DiGraph()\n    schema_graph.add_edges_from(schema)\n    order = list(nx.topological_sort(schema_graph))\n    if len(order) &lt;= 1:\n        raise ValueError(\"The schema must have at least 2 elements including the Dataset!\")\n    order = order[1:] # Remove the Dataset class from the order.\n    dobj_column_names = []\n    for cls in order:\n        for column_name, cls_item in self.class_column_names.items():\n            if cls is cls_item:\n                dobj_column_names.append(column_name)\n\n    # Create the data objects.\n    # Get all of the names of the data objects, after they're cleaned for SQLite.\n    cols_idx = [headers_in_logsheet.index(header) for header in dobj_column_names] # Get the indices of the data objects columns.\n    dobj_names = [] # The matrix of data object names (values in the logsheet).\n    for row in logsheet:\n        dobj_names.append([])\n        for idx in cols_idx:\n            raw_value = row[idx]\n            type_class = header_types[idx]\n            value = self._clean_value(type_class, raw_value)\n            dobj_names[-1].append(value)\n    for row_num, row in enumerate(dobj_names):\n        if not all([str(cell).isidentifier() for cell in row]):\n            raise ValueError(f\"Row #{row_num+self.num_header_rows+1} (1-based): All data object names must be non-empty and valid variable names!\")\n    [row.insert(0, ds.id) for row in dobj_names] # Prepend the Dataset to the first column of each row.\n    name_ids_dict = {} # The dict that maps the values (names) to the IDs. Separate dict for each class, each class is a top-level key of the dict.\n    name_ids_dict[Dataset] = {ds.name: ds.id}\n    name_dobjs_dict = {}\n    name_dobjs_dict[Dataset] = {ds.name: ds}\n    for cls in order:\n        name_ids_dict[cls] = {} # Initialize the dict for this class.            \n        name_dobjs_dict[cls] = {}\n\n    # Create the DataObject instances in the dict.        \n    all_dobjs_ordered = [] # The list of lists of DataObject instances, ordered by the order of the schema.        \n    id_creator = IDCreator(action.conn)\n    for row_num, row in enumerate(dobj_names):\n        row = row[1:]\n        all_dobjs_ordered.append([ds]) # Add the Dataset to the beginning of each row.\n        for idx in range(len(row)):\n            cls = order[idx] # The class to create.\n            col_idx = cols_idx[idx] # The index of the column in the logsheet.\n            value = self._clean_value(header_types[col_idx], row[idx])\n            # NEED TO CHECK NOT ONLY IF THE VALUE MATCHES, BUT WHETHER THE ENTIRE LINEAGE MATCHES.\n            # For example, condition names can be reused between subjects (though not within the same subject) but a new ID should be created for each lineage.\n            row_to_now = [ds.id] + row[0:idx+1]\n            is_new = True\n            if row_num &gt; 0:\n                for lst in dobj_names[0:row_num]:\n                    if set(row_to_now).issubset(set(lst)):\n                        is_new = False\n                        break\n            if is_new:\n                name_ids_dict[cls][value] = id_creator.create_ro_id(cls) + \"_\" + value\n                dobj = cls(id = name_ids_dict[cls][value], name = value, action = action) # Create the research object.\n                name_dobjs_dict[cls][value] = dobj\n                print(\"Creating DataObject, Row: \", row_num, \"Column: \", cls.prefix, \"Value: \", value, \"ID: \", dobj.id)\n            dobj = name_dobjs_dict[cls][value]\n            all_dobjs_ordered[-1].append(dobj) # Matrix of all research objects.                                        \n\n    # Assign the values to the DataObject instances.\n    # Validates that the logsheet is of valid format.\n    # i.e. Doesn't have conflicting values for one level (empty/None is OK)\n    attrs_cache_dict = {}\n    default_none_vals = {str: None, int: np.array(float('nan'))}\n    for row_num, row in enumerate(logsheet):\n        row_dobjs = all_dobjs_ordered[row_num][1:]\n        row_attrs = [{} for _ in range(len(order))] # The list of dicts of attributes for each DataObject instance.\n\n        # Assign all of the data to the appropriate DataObject instances.\n        # Includes the \"data object columns\" so that the DataObjects have an attribute with the name of the header name.\n        for header in all_headers:\n            name = header[0]\n            col_idx = headers_in_logsheet.index(name)                \n            type_class = header[1]\n            level = header[2]\n            level_idx = order.index(level)\n            vr_id = header[3]                \n            value = self._clean_value(type_class, row[headers_in_logsheet.index(name)])                                    \n            # Set up the cache dict for this data object.\n            if not row_dobjs[level_idx].id in attrs_cache_dict:\n                attrs_cache_dict[row_dobjs[level_idx].id] = {}\n            # Set up the cache dict for this data object for this attribute.\n            if name not in attrs_cache_dict[row_dobjs[level_idx].id]:\n                attrs_cache_dict[row_dobjs[level_idx].id][name] = default_none_vals[type_class]\n            print(\"Row: \", row_num+self.num_header_rows+1, \"Column: \", name, \"Value: \", value)\n            prev_value = attrs_cache_dict[row_dobjs[level_idx].id][name]\n            if prev_value is not default_none_vals[type_class] and (type(prev_value) == np.ndarray and not np.isnan(prev_value)):                    \n                if prev_value == value or value == default_none_vals[type_class] or np.isnan(value):\n                    continue\n                raise ValueError(f\"Row # (1-based): {row_num+self.num_header_rows+1} Column: {name} has conflicting values!\")\n            attrs_cache_dict[row_dobjs[level_idx].id][name] = value\n            row_attrs[level_idx][vr_id] = attrs_cache_dict[row_dobjs[level_idx].id][name]\n        for idx, attrs in enumerate(row_attrs):\n            row_dobjs[idx]._setattrs({}, attrs, action = action, pr_id = self.id)                           \n\n    # Arrange the address ID's that were generated into an edge list.\n    # Then assign that to the Dataset.\n    addresses = []\n    for row in all_dobjs_ordered:\n        for idx, dobj in enumerate(row):\n            if idx == 0:\n                continue\n            ids = [row[idx-1].id, dobj.id]\n            if ids not in addresses:\n                addresses.append(ids)\n    all_default_attrs = DefaultAttrs(ds)\n    ds._setattrs(all_default_attrs.default_attrs, {\"addresses\": addresses}, action = action, pr_id = self.id)\n    # ds.addresses = addresses # Store addresses, also creates address_graph.\n\n    action.exec = True\n    action.commit = True\n    action.execute() # Commit the action.\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/pipeline_object/","title":"Pipeline Objects","text":"<p>Pipeline Objects are the building blocks of the data analysis pipeline in ResearchOS projects. While they are technically extensible, this is not needed for most users. The following Pipeline Objects are available:</p> <ul> <li> <p>Project</p> </li> <li> <p>Analysis</p> </li> <li> <p>Logsheet</p> </li> <li> <p>Process</p> </li> <li> <p>Subset</p> </li> <li> <p>Plot</p> </li> <li> <p>Stats</p> </li> </ul> <p>             Bases: <code>ResearchObject</code></p> <p>Parent class of all pipeline objects: Projects, Analyses, Logsheets, Process Groups, Processes, Variables, SpecifyTrials, Views</p> Source code in <code>src/ResearchOS/PipelineObjects/pipeline_object.py</code> <pre><code>class PipelineObject(ResearchObject):\n    \"\"\"Parent class of all pipeline objects: Projects, Analyses, Logsheets, Process Groups, Processes, Variables, SpecifyTrials, Views\"\"\"\n\n    def _add_source_object_id(self, source_object_id: str, action: Action = None) -&gt; None:\n        \"\"\"Add a source object ID to the current object.\"\"\"\n        target_object_id = self.id\n        is_active = 1\n        self._update_pipeline_edge(source_object_id, target_object_id, is_active, action)\n\n    def _remove_source_object_id(self, source_object_id: str, action: Action = None) -&gt; None:\n        \"\"\"Remove a source object ID from the current object.\"\"\"\n        target_object_id = self.id\n        is_active = 0\n        self._update_pipeline_edge(source_object_id, target_object_id, is_active, action)\n\n    def _add_target_object_id(self, target_object_id: str, action: Action = None) -&gt; None:\n        \"\"\"Add a target object ID to the current object.\"\"\"\n        source_object_id = self.id\n        is_active = 1\n        self._update_pipeline_edge(source_object_id, target_object_id, is_active, action)\n\n    def _remove_target_object_id(self, target_object_id: str, action: Action = None) -&gt; None:\n        \"\"\"Remove a target object ID from the current object.\"\"\"\n        source_object_id = self.id\n        is_active = 0\n        self._update_pipeline_edge(source_object_id, target_object_id, is_active, action)\n\n    def _update_pipeline_edge(self, source_object_id: str, target_object_id: str, is_active: bool, action: Action = None) -&gt; None:\n        \"\"\"Update one pipeline edge.\"\"\"\n        if action is None:\n            action = Action(name = \"update_pipeline_edges\")\n        sqlquery = f\"INSERT INTO pipelineobjects_graph (action_id, source_object_id, target_object_id, is_active) VALUES ('{action.id}', '{source_object_id}', '{target_object_id}', {is_active})\"        \n        action.add_sql_query(sqlquery)\n        action.execute()\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/plot/","title":"Plot","text":""},{"location":"Research%20Objects/Pipeline%20Objects/plot/#introduction","title":"Introduction","text":"<p>This is a runnable Pipeline Object that is used to generate plots from computed Variables.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/process/","title":"Process","text":""},{"location":"Research%20Objects/Pipeline%20Objects/process/#introduction","title":"Introduction","text":"<p>This is a runnable Pipeline Object that is used to process data. This object is used to perform any data processing that is needed before the data can be plotted or summarized.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/process/#import-attributes","title":"Import Attributes","text":"<p>One special case for <code>Process</code> objects is when first importing data from their native file formats into ResearchOS. To do that, a couple of custom attributes need to be defined for the <code>Process</code> object that performs this task.</p> <p>The code to be run to import the data from the native file format should be defined in the <code>method</code> (Python) or <code>mfunc_name</code> (MATLAB) of the <code>Process</code> object. The code should handle loading only one file format at a time. If multiple file formats need to be imported, multiple <code>Process</code> objects should be created with separate functions for each.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/process/#import_file_ext","title":"import_file_ext","text":"<p>A string that specifies the file extension of the native file format. For example, if the native file format is a CSV file, the <code>import_file_ext</code> attribute should be set to <code>csv</code>.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/process/#import_file_vr_name","title":"import_file_vr_name","text":"<p>A string that specifies the name of the variable that will provide the file path of the file to import the data from. With the Dataset <code>schema</code> (or <code>file_schema</code>) and the <code>Process</code> object's <code>import_file_ext</code> and <code>import_file_vr_name</code> attributes, the full file name of the native data file can be constructed and passed as an input argument to the <code>Process</code> object's code.</p> <p>             Bases: <code>PipelineObject</code></p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>class Process(PipelineObject):\n\n    prefix = \"PR\"\n    # __slots__ = tuple(all_default_attrs.keys())\n\n    def __init__(self, is_matlab: bool = all_default_attrs[\"is_matlab\"],\n                 mfolder: str = all_default_attrs[\"mfolder\"], \n                 mfunc_name: str = all_default_attrs[\"mfunc_name\"], \n                 method: Callable = all_default_attrs[\"method\"], \n                 level: type = all_default_attrs[\"level\"], \n                 input_vrs: dict = all_default_attrs[\"input_vrs\"], \n                 output_vrs: dict = all_default_attrs[\"output_vrs\"], \n                 subset_id: str = all_default_attrs[\"subset_id\"], \n                 import_file_ext: str = all_default_attrs[\"import_file_ext\"], \n                 import_file_vr_name: str = all_default_attrs[\"import_file_vr_name\"], \n                 vrs_source_pr: dict = all_default_attrs[\"vrs_source_pr\"],\n                 lookup_vrs: dict = all_default_attrs[\"lookup_vrs\"],\n                 batch: list = all_default_attrs[\"batch\"],\n                 **kwargs) -&gt; None:\n        if self._initialized:\n            return\n        self.is_matlab = is_matlab\n        self.mfolder = mfolder\n        self.mfunc_name = mfunc_name\n        self.method = method\n        self.level = level\n        self.input_vrs = input_vrs\n        self.output_vrs = output_vrs\n        self.subset_id = subset_id\n        self.import_file_ext = import_file_ext\n        self.import_file_vr_name = import_file_vr_name\n        self.vrs_source_pr = vrs_source_pr\n        self.lookup_vrs = lookup_vrs\n        self.batch = batch\n        super().__init__(**kwargs)                                                                        \n\n    ## import_file_ext\n\n    def validate_import_file_ext(self, file_ext: str, action: Action, default: Any) -&gt; None:\n        if file_ext == default:\n            return\n        if not self.import_file_vr_name and file_ext is None:\n            return\n        if self.import_file_vr_name and file_ext is None:\n            raise ValueError(\"File extension must be specified if import_file_vr_name is specified.\")\n        if not isinstance(file_ext, str):\n            raise ValueError(\"File extension must be a string.\")\n        if not file_ext.startswith(\".\"):\n            raise ValueError(\"File extension must start with a period.\")\n\n    ## import_file_vr_name\n\n    def validate_import_file_vr_name(self, vr_name: str, action: Action, default: Any) -&gt; None:\n        if vr_name == default:\n            return\n        if not isinstance(vr_name, str):\n            raise ValueError(\"Variable name must be a string.\")\n        if not str(vr_name).isidentifier():\n            raise ValueError(\"Variable name must be a valid variable name.\")\n        if vr_name not in self.input_vrs:\n            raise ValueError(\"Variable name must be a valid input variable name.\")                                                        \n\n    def set_input_vrs(self, **kwargs) -&gt; None:\n        \"\"\"Convenience function to set the input variables with named variables rather than a dict.\"\"\"\n        self.__setattr__(\"input_vrs\", VRHandler.add_slice_to_input_vrs(kwargs))\n\n    def set_output_vrs(self, **kwargs) -&gt; None:\n        \"\"\"Convenience function to set the output variables with named variables rather than a dict.\"\"\"\n        self.__setattr__(\"output_vrs\", kwargs)\n\n    def set_vrs_source_pr(self, **kwargs) -&gt; None:\n        \"\"\"Convenience function to set the source process for the input variables with named variables rather than a dict.\"\"\"\n        self.__setattr__(\"vrs_source_pr\", kwargs)\n\n    def set_lookup_vrs(self, **kwargs) -&gt; None:\n        \"\"\"Convenience function to set the lookup variables with named variables rather than a dict.\"\"\"\n        self.__setattr__(\"lookup_vrs\", kwargs)\n\n    def run(self, force_redo: bool = False) -&gt; None:\n        \"\"\"Execute the attached method.\n        kwargs are the input VR's.\"\"\"        \n        start_msg = f\"Running {self.mfunc_name} on {self.level.__name__}s.\"\n        print(start_msg)\n        action = Action(name = start_msg)\n        process_runner = ProcessRunner()        \n        batches_dict_to_run, all_batches_graph, G, pool = process_runner.prep_for_run(self, action, force_redo)\n        curr_batch_graph = nx.MultiDiGraph()\n        process_runner.add_matlab_to_path(__file__)\n        for batch_id, batch_value in batches_dict_to_run.items():\n            if self.batch is not None:\n                curr_batch_graph = nx.MultiDiGraph(all_batches_graph.subgraph([batch_id] + list(nx.descendants(all_batches_graph, batch_id))))\n            process_runner.run_batch(batch_id, batch_value, G, curr_batch_graph)\n\n        if process_runner.matlab_loaded and self.is_matlab:\n            ProcessRunner.matlab_eng.rmpath(self.mfolder)\n\n        for vr_name, vr in self.output_vrs.items():\n            print(f\"Saved VR {vr_name} (VR: {vr.id}).\")\n\n        if action.conn:\n            pool.return_connection(action.conn)\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/process/#src.ResearchOS.PipelineObjects.process.Process.run","title":"<code>run(force_redo=False)</code>","text":"<p>Execute the attached method. kwargs are the input VR's.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def run(self, force_redo: bool = False) -&gt; None:\n    \"\"\"Execute the attached method.\n    kwargs are the input VR's.\"\"\"        \n    start_msg = f\"Running {self.mfunc_name} on {self.level.__name__}s.\"\n    print(start_msg)\n    action = Action(name = start_msg)\n    process_runner = ProcessRunner()        \n    batches_dict_to_run, all_batches_graph, G, pool = process_runner.prep_for_run(self, action, force_redo)\n    curr_batch_graph = nx.MultiDiGraph()\n    process_runner.add_matlab_to_path(__file__)\n    for batch_id, batch_value in batches_dict_to_run.items():\n        if self.batch is not None:\n            curr_batch_graph = nx.MultiDiGraph(all_batches_graph.subgraph([batch_id] + list(nx.descendants(all_batches_graph, batch_id))))\n        process_runner.run_batch(batch_id, batch_value, G, curr_batch_graph)\n\n    if process_runner.matlab_loaded and self.is_matlab:\n        ProcessRunner.matlab_eng.rmpath(self.mfolder)\n\n    for vr_name, vr in self.output_vrs.items():\n        print(f\"Saved VR {vr_name} (VR: {vr.id}).\")\n\n    if action.conn:\n        pool.return_connection(action.conn)\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/process/#src.ResearchOS.PipelineObjects.process.Process.set_input_vrs","title":"<code>set_input_vrs(**kwargs)</code>","text":"<p>Convenience function to set the input variables with named variables rather than a dict.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def set_input_vrs(self, **kwargs) -&gt; None:\n    \"\"\"Convenience function to set the input variables with named variables rather than a dict.\"\"\"\n    self.__setattr__(\"input_vrs\", VRHandler.add_slice_to_input_vrs(kwargs))\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/process/#src.ResearchOS.PipelineObjects.process.Process.set_lookup_vrs","title":"<code>set_lookup_vrs(**kwargs)</code>","text":"<p>Convenience function to set the lookup variables with named variables rather than a dict.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def set_lookup_vrs(self, **kwargs) -&gt; None:\n    \"\"\"Convenience function to set the lookup variables with named variables rather than a dict.\"\"\"\n    self.__setattr__(\"lookup_vrs\", kwargs)\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/process/#src.ResearchOS.PipelineObjects.process.Process.set_output_vrs","title":"<code>set_output_vrs(**kwargs)</code>","text":"<p>Convenience function to set the output variables with named variables rather than a dict.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def set_output_vrs(self, **kwargs) -&gt; None:\n    \"\"\"Convenience function to set the output variables with named variables rather than a dict.\"\"\"\n    self.__setattr__(\"output_vrs\", kwargs)\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/process/#src.ResearchOS.PipelineObjects.process.Process.set_vrs_source_pr","title":"<code>set_vrs_source_pr(**kwargs)</code>","text":"<p>Convenience function to set the source process for the input variables with named variables rather than a dict.</p> Source code in <code>src/ResearchOS/PipelineObjects/process.py</code> <pre><code>def set_vrs_source_pr(self, **kwargs) -&gt; None:\n    \"\"\"Convenience function to set the source process for the input variables with named variables rather than a dict.\"\"\"\n    self.__setattr__(\"vrs_source_pr\", kwargs)\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/project/","title":"Project","text":""},{"location":"Research%20Objects/Pipeline%20Objects/project/#introduction","title":"Introduction","text":"<p>This is the top level Pipeline Object in ResearchOS. This object serves to identify and contain all of the <code>Project</code> files when sharing it publicly with others.</p> <p>Inherits from PipelineObject</p> <p>             Bases: <code>PipelineObject</code></p> <p>A project is a collection of analyses. Class-specific Attributes: 1. current_analysis_id: The ID of the current analysis for this project. 2. current_dataset_id: The ID of the current dataset for this project. 3. project path: The root folder location of the project.</p> Source code in <code>src/ResearchOS/PipelineObjects/project.py</code> <pre><code>class Project(PipelineObject):\n    \"\"\"A project is a collection of analyses.\n    Class-specific Attributes:\n    1. current_analysis_id: The ID of the current analysis for this project.\n    2. current_dataset_id: The ID of the current dataset for this project.\n    3. project path: The root folder location of the project.\"\"\"\n\n    prefix: str = \"PJ\"\n\n    ## current_analysis_id\n\n    def validate_current_analysis_id(self, id: str, default: Any) -&gt; None:\n        \"\"\"Validate the current analysis ID. If it is not valid, the value is rejected.\"\"\"\n        if id == default:\n            return\n        if not isinstance(id, str):\n            raise ValueError(\"Specified value is not a string!\")\n        if not self.is_id(id):\n            raise ValueError(\"Specified value is not an ID!\")\n        parsed_id = self.parse_id(id)\n        if parsed_id[0] != \"AN\":\n            raise ValueError(\"Specified ID is not an Analysis!\")\n        if not self.object_exists(id):\n            raise ValueError(\"Analysis does not exist!\")\n\n    ## current_dataset_id\n\n    def validate_current_dataset_id(self, id: str, default: Any) -&gt; None:\n        \"\"\"Validate the current dataset ID. If it is not valid, the value is rejected.\"\"\"\n        if id == default:\n            return\n        if not self.is_id(id):\n            raise ValueError(\"Specified value is not an ID!\")\n        parsed_id = self.parse_id(id)\n        if parsed_id[0] != \"DS\":\n            raise ValueError(\"Specified ID is not a Dataset!\")\n        if not self.object_exists(id):\n            raise ValueError(\"Dataset does not exist!\")\n\n    ## project_path\n\n    def validate_project_path(self, path: str, default: Any) -&gt; None:\n        \"\"\"Validate the project path. If it is not valid, the value is rejected.\"\"\"\n        if path == default:\n            return\n        # 1. Check that the path exists in the file system.        \n        if not isinstance(path, str):\n            raise ValueError(\"Specified path is not a string!\")\n        if not os.path.exists(path):\n            raise ValueError(\"Specified path is not a path or does not currently exist!\")        \n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/","title":"Runnables","text":""},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#introduction","title":"Introduction","text":"<p>Process, Plot, and Stats are all runnable Pipeline Objects. This means that they all share very similar attributes and behaviors. After the attributes listed below are all properly specified, these runnable Pipeline Objects all have a <code>run()</code> method which executes the code associated with that object, using the specified input and output Variables.</p> <p>This page will list the behaviors and attributes that are common to all runnable Pipeline Objects. Check their individual pages for more information on their specific attributes and behaviors.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#general-attributes","title":"General Attributes","text":""},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#level-required","title":"level (Required)","text":"<p>Specify which set of Data Objects this object operates on. For example, a Process that operates on Trials would have <code>level = Trial</code>. This is a required attribute. Often, the <code>level</code> is the lowest level of Data Object in the <code>Dataset</code>'s <code>schema</code> attribute. For example, if the <code>schema</code> is <pre><code># research_objects/dataset.py\nimport ResearchOS as ros\n\nschema = [\n    [ros.Dataset, Subject],\n    [Subject, Trial]\n]\n</code></pre> Generally, most of my analysis will probably focus on the <code>Trial</code> level.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#input_vrs-required","title":"input_vrs (Required)","text":"<p>The runnable Pipeline Objects expect <code>input_vrs</code> to be a dictionary, where the keys are strings representing the variable's name in the runnable Pipeline object's code to be run, and the value is the variable's value. Most commonly, that looks something like this: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\n\ntest_pr = ros.Process(id = \"test_pr\")\ninput_vrs = {\n    \"input_variable_name_in_code1\": input_variable_object1,\n    \"input_variable_name_in_code2\": input_variable_object2,\n    ...\n}\n</code></pre> where <code>input_variable_name_in_code</code> is the name of the variable as it appears in the code associated with this object, and <code>input_variable_object</code> is the Variable object that represents the data value for that variable.</p> <p>A helper function <code>set_input_vrs()</code> is provided so that you can set the <code>input_vrs</code> attribute using keyword arguments. For example: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\n\ntest_pr = ros.Process(id = \"test_pr\")\ntest_pr.set_input_vrs(input_variable_name_in_code = vr.input_variable_object)\n</code></pre></p> <p>This syntax can be used to specify the input Variables that have either dynamic or hard-coded values. However, there are some rare cases that require the inputs to be specified in a different way.</p> <p>NOTE: For keyword arguments to a function, the order of the input_vrs does not matter. But for positional arguments (including all MATLAB functions), the order must match the order of the input arguments.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#data-object-attributes","title":"Data Object attributes","text":"<p>Sometimes, the code being run requires an attribute of the Data Object to be passed as an input. For example, if the code being run on a particular <code>Trial</code> requires the <code>Subject</code> name, or requires the <code>Task</code> type of a particular <code>Trial</code>. In this case, the <code>input_vr</code> dictionary can be specified as follows, using the helper function <code>set_input_vrs()</code>: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\nfrom research_objects.my_data_objects import Subject, Trial\n\ntest_pr = ros.Process(id = \"test_pr\", level = Trial)\ntest_pr.set_input_vr(subject_name = {Subject: \"name\"})\n</code></pre> This syntax tells ResearchOS that for each <code>Trial</code> object, the <code>Subject</code> object associated with that <code>Trial</code> should have its <code>name</code> attribute passed to the code being run for the <code>subject_name</code> input variable.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#vrs_source_pr-recommended","title":"vrs_source_pr (Recommended)","text":"<p>This attribute tells the runnable Pipeline Object where to look for the input Variables that are specified in the <code>input_vrs</code> attribute. The source of the input Variables can be either a Process or Logsheet object. This helps to manage the flow of data through the pipeline, especially when a Variable is overwritten by one or more runnable Pipeline Objects. If no source_pr is specified for a Variable, then it is assumed that the Process that most recently outputted that Variable is the source of that Variable. Hard-coded Variables will not be included here, because they don't have a \"source\" in the same way that Variables that are passed from another object do.</p> <p>Similar to the <code>input_vrs</code> this attribute is specified as a dictionary, where the keys are the names of the input Variables in the code and the values are the Process or Logsheet object that is the source of that Variable. For example: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\nfrom research_objects.logsheets import logsheet\n\nsource_pr1 = ros.Process(id = \"PR1\")\nsource_pr2 = ros.Process(id = \"PR2\")\n\ntest_pr = ros.Process(id = \"PR3\")\nvrs_source_pr = {\n    \"input_variable_name_in_code1\": source_pr1,\n    \"input_variable_name_in_code2\": source_pr2,\n    \"input_variable_name_in_code3\": logsheet,\n    ...\n}\ntest_pr.vrs_source_pr = vrs_source_pr\n</code></pre> There is also a helper function <code>set_vrs_source_pr()</code> that can be used to set the <code>vrs_source_pr</code> attribute using keyword arguments. For example: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\nfrom research_objects.logsheets import logsheet\n\nsource_pr1 = ros.Process(id = \"PR1\")\nsource_pr2 = ros.Process(id = \"PR2\")\n\ntest_pr = ros.Process(id = \"PR3\")\ntest_pr.set_vrs_source_pr(input_variable_name_in_code1 = source_pr1, input_variable_name_in_code2 = source_pr2, input_variable_name_in_code3 = logsheet, ...)\n</code></pre></p> <p>There are a few instances where this syntax needs to be modified to accommodate different workflows.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#multiple-sources","title":"Multiple sources","text":"<p>If a Variable is derived from multiple sources, then the <code>vrs_source_pr</code> attribute should be a list of the sources. For example, a <code>Subject</code>'s mass could be calculated from a <code>Process</code> or it could be listed in the <code>Logsheet</code>: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\nfrom research_objects.logsheets import logsheet\n\n# outputs the mass of the subject\ncompute_subject_mass = ros.Process(id = \"PR1\")\n\n# a Process that uses the mass of the subject. If for a Data Object instance, the mass is not calculated, then it will use the mass from the logsheet\nuses_subject_mass = ros.Process(id = \"PR2\")\nuses_subject_mass.set_input_vrs(subject_mass = vr.mass)\nuses_subject_mass.set_vrs_source_pr(subject_mass = [compute_subject_mass, logsheet])\n</code></pre></p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#output_vrs-pr-st-required","title":"output_vrs (PR, ST Required)","text":"<p>Similar to input_vrs, the _output_vrs are specified as a dictionary. The keys are the names of the output Variables in the code and the values are the Variable objects that represent the data value for that variable. For example: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\n\ntest_pr = ros.Process(id = \"PR0\")\noutput_vrs = {\n    \"output_variable_name_in_code1\": vr.output_variable_object1,\n    \"output_variable_name_in_code2\": vr.output_variable_object2,\n    ...\n}\ntest_pr.output_vrs = output_vrs\n</code></pre></p> <p>Using the helper function <code>set_output_vrs()</code> is also an option. For example: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\n\ntest_pr = ros.Process(id = \"PR0\")\ntest_pr.set_output_vrs(output_variable_name_in_code1 = vr.output_variable_object1, output_variable_name_in_code2 = vr.output_variable_object2, ...)\n</code></pre></p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#lookup_vrs-optional","title":"lookup_vrs (Optional)","text":"<p>For some workflows, it is necessary to look up the value of a Variable from a different Data Object. For example, if a <code>Trial</code> object needs to retrieve some calibrated value from a calibration <code>Trial</code>, then the <code>lookup_vrs</code> attribute can be used. This attribute is specified as a dictionary, where the keys are the names of the Variables in the code and the values are dictionaries themselves. The inner dictionaries have the Variable object as the key, and a list of strings as the value. The strings are the variable names in code that are being looked up in another Data Object. In the below example, the <code>lookup_input_variable_name_in_code</code> is the name of the Variable that specifies which Data Object to reference, and <code>other_input_vr</code> is the name of the Variable to retrieve from that Data Object. <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects import variables as vr\n\ntest_pr = ros.Process(id = \"PR0\")\ntest_pr.set_input_vrs(lookup_input_variable_name_in_code = vr.input_variable_object, other_input_vr = vr.other_input_variable_object)\nlookup_vrs = {\n    \"lookup_input_variable_name_in_code\": {vr.lookup_variable_object: [\"other_input_vr\", ...]}\n}\n</code></pre></p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#batch-optional","title":"batch (Optional)","text":"<p>Specify whether this object should be run in batch mode. The default is <code>None</code>, indicating that only one Data Object's values are provided at a time to the runnable Pipeline Object, as would be expected. If <code>batch = []</code>, then all Data Objects at the specified level are provided to the runnable Pipeline Object at once. If <code>batch</code> is a list of Data Object classes, then all Data Objects that are of the specified classes are provided to the runnable Pipeline Object at once.</p> <p>For example, if the Dataset <code>schema</code> is <pre><code># research_objects/dataset.py\nimport ResearchOS as ros\nfrom research_objects.my_data_objects import Subject, Trial\nschema = [\n    [ros.Dataset, Subject],\n    [Subject, Trial]\n]\n</code></pre> and for a Process object the <code>batch</code> is: <pre><code># research_objects/processes.py\nimport ResearchOS as ros\nfrom research_objects.my_data_objects import Subject, Trial\ntest_pr = ros.Process(id = \"test_pr\")\ntest_pr.batch = [Subject, Trial]\n</code></pre> Then all <code>Trials</code> for one <code>Subject</code> are provided to the runnable Pipeline Object at once in the form of a dictionary with each <code>Trial</code>'s <code>id</code> as the key and that <code>Variable</code>'s value for that <code>Trial</code> object as the value.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#python-specific-attributes","title":"Python-specific Attributes","text":"<p>Be sure that <code>is_matlab</code> is set to <code>False</code> for this object to run Python code.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#method-required","title":"method (Required)","text":"<p>A <code>Callable</code> attribute that is <code>None</code> by default. This is the handle to the function that will be called when the <code>run()</code> method is executed. This method's code should take the input Variables as arguments and return the output Variables.</p> <p>Note that when loading this object from the database, the method needs to be in the global namespace. This is because the method is stored as a string in the database and is then loaded from sys.modules when the object is loaded.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#matlab-specific-attributes","title":"MATLAB-specific Attributes","text":""},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#is_matlab-required","title":"is_matlab (Required)","text":"<p>A boolean attribute that is <code>False</code> by default. Set this to <code>True</code> if the code associated with this object is written in MATLAB. Note that this requires that MATLAB is installed on your system and that the MATLAB Engine API is installed in your Python environment.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#mfolder-required","title":"mfolder (Required)","text":"<p>A string attribute that is <code>None</code> by default. Set this to the path to the folder containing the MATLAB code associated with this object. This is used to set the MATLAB working directory to the correct location before running the MATLAB code. It is computer-specific.</p> <p>For maximum portability between computers and users, I recommend defining a <code>paths.py</code> file in the project folder that contains the path to the folder containing the MATLAB code. You can import this path into the .py file that defines your runnable Pipeline Objects.</p> <pre><code># paths.py\nmfolder = \"path/to/matlab/folder\"\n\n# processes.py\nfrom paths import mfolder\nimport ResearchOS as ros\n\ntest_pr = ros.Process(id = \"PR0\")\ntest_pr.mfolder = mfolder\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/runnables/#mfunc_name-required","title":"mfunc_name (Required)","text":"<p>Specifies the name of the MATLAB function to be run. This is a string attribute that is <code>None</code> by default. This is the name of the MATLAB function that will be called when the <code>run()</code> method is executed.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/stats/","title":"Stats","text":""},{"location":"Research%20Objects/Pipeline%20Objects/stats/#introduction","title":"Introduction","text":"<p>This is a runnable Pipeline Object that is used to generate summary statistics from computed Variables.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/subset/","title":"Subset","text":""},{"location":"Research%20Objects/Pipeline%20Objects/subset/#introduction","title":"Introduction","text":"<p>This Pipeline Object is used to specify a subset of data to operate on for any runnable Pipeline Object.</p>"},{"location":"Research%20Objects/Pipeline%20Objects/subset/#conditions","title":"Conditions","text":"<p><code>Subset</code> objects have a <code>conditions</code> attribute that is used to specify the criteria to select a subset of data.The <code>conditions</code> attribute is a list of dictionaries, where each dictionary can also contain sub-dictionaries. The keys of the dictionaries are either <code>\"and\"</code> or <code>\"or\"</code>. <code>\"and\"</code> is used to specify that all conditions in the sub-dictionary must be met, while <code>\"or\"</code> is used to specify that at least one condition in the sub-dictionary must be met. The values of the dictionaries are lists. Each element of the list is either a sub-dictionary, or a tuple of three elements: the Variable object, the logical operator, and the value to compare the variable to. The operators that can be used are <code>\"==\"</code>, <code>\"!=\"</code>, <code>\"&lt;\"</code>, <code>\"&lt;=\"</code>, <code>\"&gt;\"</code>, <code>\"&gt;=\"</code>, <code>\"in\"</code>, and <code>\"not in\"</code>. The value can be any hard-coded value that can be JSON-serialized using <code>json.dumps()</code>.</p> <p>Here is an example <code>Subset</code> object <code>condition</code> attribute. This example selects <code>\"Subject1\"</code> and <code>\"Trial1\"</code> where the <code>vr.value</code> is greater than 5 or less than 0: <pre><code>conditions = [\n    {\n        \"and\": [\n            (vr.subject_name, \"==\", \"Subject1\"),\n            (vr.trial_name, \"==\", \"Trial1\"),\n            {\n                \"or\": [\n                    (vr.value, \"&gt;\", 5),\n                    (vr.value, \"&lt;\", 0)\n                ]\n            }\n        ]\n    }\n]\n</code></pre></p> <p>             Bases: <code>PipelineObject</code></p> <p>Provides rules to select a subset of data from a dataset.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>class Subset(PipelineObject):\n    \"\"\"Provides rules to select a subset of data from a dataset.\"\"\"\n\n    prefix = \"SS\"\n\n    def __init__(self, conditions: dict = all_default_attrs[\"conditions\"], **kwargs):\n        if self._initialized:\n            return\n        self.conditions = conditions\n        super().__init__(**kwargs)\n\n    ## conditions\n\n    def validate_conditions(self, conditions: dict, action: Action, default: Any) -&gt; None:\n        \"\"\"Validate the condition recursively.\n        Example usage:\n        conditions = {\n            \"and\": [\n                [vr1.id, \"&lt;\", 4],\n                {\n                    \"or\": [\n                        [vr1.id, \"&gt;\", 2],\n                        [vr1.id, \"=\", 7]\n                    ]\n                }\n            ]\n        }\n        \"\"\"\n        if conditions == default:\n            return\n        # Validate a single condition.\n        if isinstance(conditions, list):\n            if len(conditions) != 3:\n                raise ValueError(\"Condition must be a list of length 3.\")\n            if not IDCreator(action.conn).is_ro_id(conditions[0]):\n                raise ValueError(\"Variable ID must be a valid Variable ID.\")\n            if not ResearchObjectHandler.object_exists(conditions[0], action):\n                raise ValueError(\"Variable must be pre-existing.\")\n            if conditions[1] not in logic_options:\n                raise ValueError(\"Invalid logic.\")\n            if conditions[1] in numeric_logic_options and not isinstance(conditions[2], int):\n                raise ValueError(\"Numeric logical symbols must have an int value.\")\n            try:\n                a = json.dumps(conditions[2])\n            except:\n                raise ValueError(\"Value must be JSON serializable.\")\n            return\n\n        # Validate the \"and\"/\"or\" keys.\n        if not isinstance(conditions, dict):\n            raise ValueError(\"Condition must be a dict.\")\n        if \"and\" not in conditions and \"or\" not in conditions:\n            raise ValueError(\"Condition must contain an 'and' or 'or' key.\")\n        if \"and\" in conditions and \"or\" in conditions:\n            raise ValueError(\"Condition cannot contain both 'and' and 'or' keys.\")\n\n        for key, value in conditions.items():\n            if key not in (\"and\", \"or\"):\n                raise ValueError(\"Invalid key in condition.\")\n            if not isinstance(value, list):\n                raise ValueError(\"Value must be a list.\")\n            if not isinstance(value, (list, dict)):\n                raise ValueError(\"Value must be a list of lists or dicts.\")\n            a = [self.validate_conditions(cond, action, default = default) for cond in value] # Assigned to a just to make interpreter happy.\n\n    def get_subset(self, action: Action) -&gt; nx.MultiDiGraph:\n        \"\"\"Resolve the conditions to the actual subset of data.\"\"\"\n        from ResearchOS.DataObjects.data_object import DataObject\n        print(f'Getting subset of DataObjects: {self.name} ({self.id})')\n        # 1. Get the dataset.\n        dataset_id = self._get_dataset_id()\n        ds = Dataset(id = dataset_id)\n        schema_id = self.get_current_schema_id(ds.id)\n\n        # 2. For each node_id in the address_graph, check if it meets the conditions.\n        nodes_for_subgraph = [ds.id]\n        G = ds.get_addresses_graph()\n        sorted_nodes = list(nx.topological_sort(G))\n        subclasses = DataObject.__subclasses__()\n\n        # Loop through all conditions in the conditions dict. Handle when the condition is a list or a dict.\n        conditions_list = []\n        self._extract_and_replace_lists(self.conditions, conditions_list)\n        vr_ids = [cond[0] for cond in conditions_list]\n\n        # Get the hashes\n        sqlquery_raw = \"SELECT data_blob_hash, dataobject_id, vr_id FROM data_values WHERE vr_id IN ({}) AND schema_id = ?\".format(\", \".join([\"?\" for _ in vr_ids]))\n        sqlquery = sql_order_result(action, sqlquery_raw, [\"dataobject_id\", \"vr_id\"], single = False, user = True, computer = False)\n        params = tuple(vr_ids) + (schema_id,)\n        cursor = action.conn.cursor()\n        result = cursor.execute(sqlquery, params).fetchall()\n\n        # Get the values\n        pool_data = SQLiteConnectionPool(name = \"data\")\n        conn_data = pool_data.get_connection()\n        cursor_data = conn_data.cursor()\n        sqlquery = \"SELECT data_blob, data_blob_hash FROM data_values_blob WHERE data_blob_hash IN ({})\".format(\", \".join([\"?\" for _ in result]))\n        params = tuple([x[0] for x in result])\n        values = cursor_data.execute(sqlquery, params).fetchall()\n        pool_data.return_connection(conn_data)\n        values = [list(item) for item in values]\n\n        for value in values:\n            value[0] = pickle.loads(value[0])\n\n        # Put the values into a dict.\n        vr_values = {}\n        for row in result:\n            data_blob_hash = row[0]\n            dataobject_id = row[1]\n            vr_id = row[2]\n            if vr_id not in vr_values:\n                vr_values[vr_id] = {}\n            if dataobject_id not in vr_values[vr_id]:\n                vr_values[vr_id][dataobject_id] = None\n            blob_hash_idx = [x[1] for x in values].index(data_blob_hash)\n            vr_values[vr_id][dataobject_id] = values[blob_hash_idx][0]\n\n        for node_id in sorted_nodes:\n            if not self._meets_conditions(node_id, self.conditions, G, vr_values, action):\n                continue\n            curr_nodes = [node_id]\n            curr_nodes.extend(nx.ancestors(G, node_id))\n            nodes_for_subgraph.extend([node_id for node_id in curr_nodes if node_id not in nodes_for_subgraph])\n\n        if len(nodes_for_subgraph) == 0:\n            print(f\"No nodes meet the conditions of {self.name} ({self.id}).\")\n        return G.subgraph(nodes_for_subgraph) # Maintains the relationships between all of the nodes in the subgraph.\n\n    def _extract_and_replace_lists(self, data, extracted_lists: list, counter=[0]):\n        \"\"\" Recursively traverses the data structure, replaces each list with a unique number, and extracts the lists. \"\"\"\n        if isinstance(data, list):\n            # Append the current list to the extracted lists\n            extracted_lists.append(data)\n            # Replace the list with the current counter value\n            number = counter[0]\n            counter[0] += 1\n            return number\n        elif isinstance(data, dict):\n            # Traverse dictionary and process each value\n            return {key: [self._extract_and_replace_lists(item, extracted_lists, counter) if isinstance(item, list) else item for item in value] if isinstance(value, list) else self._extract_and_replace_lists(value, extracted_lists, counter) for key, value in data.items()}\n        else:\n            # For other data types, return as is\n            return data\n\n\n    def _meets_conditions(self, node_id: str, conditions: dict, G: nx.MultiDiGraph, vr_values: dict, action: Action) -&gt; bool:\n        \"\"\"Check if the node_id meets the conditions.\"\"\"\n        if isinstance(conditions, dict):\n            if \"and\" in conditions:\n                for cond in conditions[\"and\"]:\n                    if not self._meets_conditions(node_id, cond, G, vr_values, action):\n                        return False\n                return True\n            if \"or\" in conditions:\n                return any([self._meets_conditions(node_id, cond, G, vr_values, action) for cond in conditions[\"or\"]])\n\n        # Check the condition.\n        vr_id = conditions[0]\n        logic = conditions[1]\n        value = conditions[2]\n        try:\n            vr_value = vr_values[vr_id][node_id]\n            found_attr = True\n        except:\n            anc_nodes = nx.ancestors(G, node_id)\n            found_attr = False\n            for anc_node_id in anc_nodes:\n                try:\n                    vr_value = vr_values[vr_id][anc_node_id]\n                except:\n                    continue\n                found_attr = True\n                break\n        if not found_attr:\n            return False\n\n        if isinstance(vr_value, str):\n            vr_value = vr_value.lower()\n        if isinstance(value, str):\n            value = value.lower()\n        if isinstance(value, list):\n            value = [x.lower() if isinstance(x, str) else x for x in value]\n\n        # This is probably shoddy logic, but it'll serve as a first pass to handle None types.\n        if logic in plural_logic:\n            if logic == \"contains\" and vr_value is None:\n                return False\n            elif logic == \"not contains\" and vr_value is None and value is not None:                \n                return True\n            elif logic == \"in\" and value is None:\n                return False\n            elif logic == \"not in\" and value is None:\n                return True\n\n        # Numeric\n        bool_val = False\n        if logic == \"&gt;\" and vr_value &gt; value:\n            bool_val = True\n        elif logic == \"&lt;\" and vr_value &lt; value:\n            bool_val = True\n        elif logic == \"&gt;=\" and vr_value &gt;= value:\n            bool_val = True\n        elif logic == \"&lt;=\" and vr_value &lt;= value:\n            bool_val = True\n        # Any type\n        elif logic in [\"==\",\"=\"] and vr_value == value:\n            bool_val = True\n        elif logic == \"!=\" and vr_value != value:\n            bool_val = True\n        elif logic == \"in\" and vr_value in value:\n            bool_val = True\n        elif logic == \"not in\" and vr_value not in value:\n            bool_val = True\n        elif logic == \"is\" and vr_value is value:\n            bool_val = True\n        elif logic == \"is not\" and vr_value is not value:\n            bool_val = True\n        elif logic == \"contains\" and value in vr_value:\n            bool_val = True\n        elif logic == \"not contains\" and not value in vr_value:\n            bool_val = True\n\n        return bool_val\n</code></pre>"},{"location":"Research%20Objects/Pipeline%20Objects/subset/#src.ResearchOS.PipelineObjects.subset.Subset.get_subset","title":"<code>get_subset(action)</code>","text":"<p>Resolve the conditions to the actual subset of data.</p> Source code in <code>src/ResearchOS/PipelineObjects/subset.py</code> <pre><code>def get_subset(self, action: Action) -&gt; nx.MultiDiGraph:\n    \"\"\"Resolve the conditions to the actual subset of data.\"\"\"\n    from ResearchOS.DataObjects.data_object import DataObject\n    print(f'Getting subset of DataObjects: {self.name} ({self.id})')\n    # 1. Get the dataset.\n    dataset_id = self._get_dataset_id()\n    ds = Dataset(id = dataset_id)\n    schema_id = self.get_current_schema_id(ds.id)\n\n    # 2. For each node_id in the address_graph, check if it meets the conditions.\n    nodes_for_subgraph = [ds.id]\n    G = ds.get_addresses_graph()\n    sorted_nodes = list(nx.topological_sort(G))\n    subclasses = DataObject.__subclasses__()\n\n    # Loop through all conditions in the conditions dict. Handle when the condition is a list or a dict.\n    conditions_list = []\n    self._extract_and_replace_lists(self.conditions, conditions_list)\n    vr_ids = [cond[0] for cond in conditions_list]\n\n    # Get the hashes\n    sqlquery_raw = \"SELECT data_blob_hash, dataobject_id, vr_id FROM data_values WHERE vr_id IN ({}) AND schema_id = ?\".format(\", \".join([\"?\" for _ in vr_ids]))\n    sqlquery = sql_order_result(action, sqlquery_raw, [\"dataobject_id\", \"vr_id\"], single = False, user = True, computer = False)\n    params = tuple(vr_ids) + (schema_id,)\n    cursor = action.conn.cursor()\n    result = cursor.execute(sqlquery, params).fetchall()\n\n    # Get the values\n    pool_data = SQLiteConnectionPool(name = \"data\")\n    conn_data = pool_data.get_connection()\n    cursor_data = conn_data.cursor()\n    sqlquery = \"SELECT data_blob, data_blob_hash FROM data_values_blob WHERE data_blob_hash IN ({})\".format(\", \".join([\"?\" for _ in result]))\n    params = tuple([x[0] for x in result])\n    values = cursor_data.execute(sqlquery, params).fetchall()\n    pool_data.return_connection(conn_data)\n    values = [list(item) for item in values]\n\n    for value in values:\n        value[0] = pickle.loads(value[0])\n\n    # Put the values into a dict.\n    vr_values = {}\n    for row in result:\n        data_blob_hash = row[0]\n        dataobject_id = row[1]\n        vr_id = row[2]\n        if vr_id not in vr_values:\n            vr_values[vr_id] = {}\n        if dataobject_id not in vr_values[vr_id]:\n            vr_values[vr_id][dataobject_id] = None\n        blob_hash_idx = [x[1] for x in values].index(data_blob_hash)\n        vr_values[vr_id][dataobject_id] = values[blob_hash_idx][0]\n\n    for node_id in sorted_nodes:\n        if not self._meets_conditions(node_id, self.conditions, G, vr_values, action):\n            continue\n        curr_nodes = [node_id]\n        curr_nodes.extend(nx.ancestors(G, node_id))\n        nodes_for_subgraph.extend([node_id for node_id in curr_nodes if node_id not in nodes_for_subgraph])\n\n    if len(nodes_for_subgraph) == 0:\n        print(f\"No nodes meet the conditions of {self.name} ({self.id}).\")\n    return G.subgraph(nodes_for_subgraph) # Maintains the relationships between all of the nodes in the subgraph.\n</code></pre>"},{"location":"SQL%20Database/","title":"SQLite Database","text":""},{"location":"SQL%20Database/#introduction","title":"Introduction","text":"<p>ResearchOS uses two SQLite databases to store data. The first database is used to store the Research Objects that define the structure and maintain the history of your project. The second database is used to store the data itself. These databases are stored in the root of the project folder and are named <code>researchos.db</code> and <code>researchos_data.db</code>, respectively. Because <code>researchos_data.db</code> contains the project's data, and can therefore become quite large, I recommend that it be added to the <code>.gitignore</code> file to prevent it from being uploaded to a repository.</p>"},{"location":"SQL%20Database/#database-structure","title":"Database Structure","text":"<p>The <code>researchos.db</code> database contains the following tables:</p> <ul> <li> <p>research_objects: The reference list of Research Objects. If the Research Object doesn't exist in this table, then it doesn't exist for the project.</p> </li> <li> <p>actions: The reference list of Actions that have been performed on this project.</p> </li> </ul>"}]}